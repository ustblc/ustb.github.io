<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>PyTorch_CNN</title>
    <url>/ck3bmvcwv002cbsg4hwny81ln.html</url>
    <content><![CDATA[<h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><ul>
<li>MNIST举例</li>
</ul>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设备的配置（这里使用GPU来跑训练）</span></span><br><span class="line">device=torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>

<pre><code>cuda</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设定超参数</span></span><br><span class="line">num_classes=<span class="number">10</span></span><br><span class="line">num_epochs=<span class="number">30</span></span><br><span class="line">batch_size=<span class="number">100</span></span><br><span class="line">learning_rate=<span class="number">0.001</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载MNIST数据集</span></span><br><span class="line">train_dataset=torchvision.datasets.MNIST(root=<span class="string">'./data'</span>,train=<span class="literal">True</span>,download=<span class="literal">True</span>,transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">test_dataset=torchvision.datasets.MNIST(root=<span class="string">'./data'</span>,train=<span class="literal">False</span>,transform=transforms.ToTensor())</span><br></pre></td></tr></table></figure>

<pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz


100.1%

Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz


113.5%

Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz


100.4%

Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz


180.4%

Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw
Processing...
Done!</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义数据加载器（数据输入流水线）</span></span><br><span class="line">train_dataloader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义一个卷积神经网络（两个卷积层）</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super(ConvNet,self).__init__()</span><br><span class="line">        self.layer1=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>,<span class="number">16</span>,kernel_size=<span class="number">5</span>,stride=<span class="number">1</span>,padding=<span class="number">2</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">        self.layer2=nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">16</span>,<span class="number">32</span>,kernel_size=<span class="number">5</span>,stride=<span class="number">1</span>,padding=<span class="number">2</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">        self.fc=nn.Linear(<span class="number">7</span>*<span class="number">7</span>*<span class="number">32</span>,num_classes)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        out=self.layer1(x)</span><br><span class="line">        out=self.layer2(out)</span><br><span class="line">        out=out.reshape(out.size(<span class="number">0</span>),<span class="number">-1</span>)</span><br><span class="line">        out=self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">model=ConvNet(num_classes).to(device)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义损失函数和优化器</span></span><br><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">total_step=len(train_dataloader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i,(images,labels) <span class="keyword">in</span> enumerate(train_dataloader):</span><br><span class="line">        images=images.to(device)</span><br><span class="line">        labels=labels.to(device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#前向传播</span></span><br><span class="line">        outputs=model(images)</span><br><span class="line">        loss=criterion(outputs,labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#梯度置0+反向传播+更新权重</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(i+<span class="number">1</span>)%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;'</span> .format(epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, total_step, loss.item()))</span><br></pre></td></tr></table></figure>

<pre><code>Epoch [1/30], Step [100/600], Loss: 0.1577
Epoch [1/30], Step [200/600], Loss: 0.1438
Epoch [1/30], Step [300/600], Loss: 0.1884
Epoch [1/30], Step [400/600], Loss: 0.2141
Epoch [1/30], Step [500/600], Loss: 0.1059
Epoch [1/30], Step [600/600], Loss: 0.2351
Epoch [2/30], Step [100/600], Loss: 0.1536
Epoch [2/30], Step [200/600], Loss: 0.2595
Epoch [2/30], Step [300/600], Loss: 0.1568
Epoch [2/30], Step [400/600], Loss: 0.1574
Epoch [2/30], Step [500/600], Loss: 0.1640
Epoch [2/30], Step [600/600], Loss: 0.1491
Epoch [3/30], Step [100/600], Loss: 0.1316
Epoch [3/30], Step [200/600], Loss: 0.1286
Epoch [3/30], Step [300/600], Loss: 0.1090
Epoch [3/30], Step [400/600], Loss: 0.1225
Epoch [3/30], Step [500/600], Loss: 0.1855
Epoch [3/30], Step [600/600], Loss: 0.2051
Epoch [4/30], Step [100/600], Loss: 0.1919
Epoch [4/30], Step [200/600], Loss: 0.0978
Epoch [4/30], Step [300/600], Loss: 0.1332
Epoch [4/30], Step [400/600], Loss: 0.0695
Epoch [4/30], Step [500/600], Loss: 0.1611
Epoch [4/30], Step [600/600], Loss: 0.1686
Epoch [5/30], Step [100/600], Loss: 0.0865
Epoch [5/30], Step [200/600], Loss: 0.0943
Epoch [5/30], Step [300/600], Loss: 0.0437
Epoch [5/30], Step [400/600], Loss: 0.1014
Epoch [5/30], Step [500/600], Loss: 0.1270
Epoch [5/30], Step [600/600], Loss: 0.0938
Epoch [6/30], Step [100/600], Loss: 0.1868
Epoch [6/30], Step [200/600], Loss: 0.1830
Epoch [6/30], Step [300/600], Loss: 0.1203
Epoch [6/30], Step [400/600], Loss: 0.1165
Epoch [6/30], Step [500/600], Loss: 0.1225
Epoch [6/30], Step [600/600], Loss: 0.0663
Epoch [7/30], Step [100/600], Loss: 0.1232
Epoch [7/30], Step [200/600], Loss: 0.0919
Epoch [7/30], Step [300/600], Loss: 0.0830
Epoch [7/30], Step [400/600], Loss: 0.1185
Epoch [7/30], Step [500/600], Loss: 0.1438
Epoch [7/30], Step [600/600], Loss: 0.1936
Epoch [8/30], Step [100/600], Loss: 0.0454
Epoch [8/30], Step [200/600], Loss: 0.0440
Epoch [8/30], Step [300/600], Loss: 0.1723
Epoch [8/30], Step [400/600], Loss: 0.1465
Epoch [8/30], Step [500/600], Loss: 0.1301
Epoch [8/30], Step [600/600], Loss: 0.1120
Epoch [9/30], Step [100/600], Loss: 0.1239
Epoch [9/30], Step [200/600], Loss: 0.0668
Epoch [9/30], Step [300/600], Loss: 0.1200
Epoch [9/30], Step [400/600], Loss: 0.0720
Epoch [9/30], Step [500/600], Loss: 0.1159
Epoch [9/30], Step [600/600], Loss: 0.0869
Epoch [10/30], Step [100/600], Loss: 0.0502
Epoch [10/30], Step [200/600], Loss: 0.1090
Epoch [10/30], Step [300/600], Loss: 0.0604
Epoch [10/30], Step [400/600], Loss: 0.0823
Epoch [10/30], Step [500/600], Loss: 0.1274
Epoch [10/30], Step [600/600], Loss: 0.0921
Epoch [11/30], Step [100/600], Loss: 0.0766
Epoch [11/30], Step [200/600], Loss: 0.0915
Epoch [11/30], Step [300/600], Loss: 0.0680
Epoch [11/30], Step [400/600], Loss: 0.0537
Epoch [11/30], Step [500/600], Loss: 0.0872
Epoch [11/30], Step [600/600], Loss: 0.0501
Epoch [12/30], Step [100/600], Loss: 0.0640
Epoch [12/30], Step [200/600], Loss: 0.1074
Epoch [12/30], Step [300/600], Loss: 0.0648
Epoch [12/30], Step [400/600], Loss: 0.1053
Epoch [12/30], Step [500/600], Loss: 0.1114
Epoch [12/30], Step [600/600], Loss: 0.0853
Epoch [13/30], Step [100/600], Loss: 0.0316
Epoch [13/30], Step [200/600], Loss: 0.0837
Epoch [13/30], Step [300/600], Loss: 0.1915
Epoch [13/30], Step [400/600], Loss: 0.0859
Epoch [13/30], Step [500/600], Loss: 0.0947
Epoch [13/30], Step [600/600], Loss: 0.0501
Epoch [14/30], Step [100/600], Loss: 0.0765
Epoch [14/30], Step [200/600], Loss: 0.0526
Epoch [14/30], Step [300/600], Loss: 0.0773
Epoch [14/30], Step [400/600], Loss: 0.0578
Epoch [14/30], Step [500/600], Loss: 0.0845
Epoch [14/30], Step [600/600], Loss: 0.0521
Epoch [15/30], Step [100/600], Loss: 0.1169
Epoch [15/30], Step [200/600], Loss: 0.0391
Epoch [15/30], Step [300/600], Loss: 0.0686
Epoch [15/30], Step [400/600], Loss: 0.0966
Epoch [15/30], Step [500/600], Loss: 0.0331
Epoch [15/30], Step [600/600], Loss: 0.0582
Epoch [16/30], Step [100/600], Loss: 0.0694
Epoch [16/30], Step [200/600], Loss: 0.0841
Epoch [16/30], Step [300/600], Loss: 0.1188
Epoch [16/30], Step [400/600], Loss: 0.0712
Epoch [16/30], Step [500/600], Loss: 0.0655
Epoch [16/30], Step [600/600], Loss: 0.0883
Epoch [17/30], Step [100/600], Loss: 0.1239
Epoch [17/30], Step [200/600], Loss: 0.0856
Epoch [17/30], Step [300/600], Loss: 0.0598
Epoch [17/30], Step [400/600], Loss: 0.0667
Epoch [17/30], Step [500/600], Loss: 0.0577
Epoch [17/30], Step [600/600], Loss: 0.0958
Epoch [18/30], Step [100/600], Loss: 0.0657
Epoch [18/30], Step [200/600], Loss: 0.0419
Epoch [18/30], Step [300/600], Loss: 0.0927
Epoch [18/30], Step [400/600], Loss: 0.1247
Epoch [18/30], Step [500/600], Loss: 0.1200
Epoch [18/30], Step [600/600], Loss: 0.1246
Epoch [19/30], Step [100/600], Loss: 0.0615
Epoch [19/30], Step [200/600], Loss: 0.0164
Epoch [19/30], Step [300/600], Loss: 0.0816
Epoch [19/30], Step [400/600], Loss: 0.1274
Epoch [19/30], Step [500/600], Loss: 0.0988
Epoch [19/30], Step [600/600], Loss: 0.0349
Epoch [20/30], Step [100/600], Loss: 0.1025
Epoch [20/30], Step [200/600], Loss: 0.1068
Epoch [20/30], Step [300/600], Loss: 0.0400
Epoch [20/30], Step [400/600], Loss: 0.0398
Epoch [20/30], Step [500/600], Loss: 0.0671
Epoch [20/30], Step [600/600], Loss: 0.0874
Epoch [21/30], Step [100/600], Loss: 0.0413
Epoch [21/30], Step [200/600], Loss: 0.1078
Epoch [21/30], Step [300/600], Loss: 0.0597
Epoch [21/30], Step [400/600], Loss: 0.0410
Epoch [21/30], Step [500/600], Loss: 0.0644
Epoch [21/30], Step [600/600], Loss: 0.1118
Epoch [22/30], Step [100/600], Loss: 0.0907
Epoch [22/30], Step [200/600], Loss: 0.0542
Epoch [22/30], Step [300/600], Loss: 0.2506
Epoch [22/30], Step [400/600], Loss: 0.0521
Epoch [22/30], Step [500/600], Loss: 0.0435
Epoch [22/30], Step [600/600], Loss: 0.0573
Epoch [23/30], Step [100/600], Loss: 0.0587
Epoch [23/30], Step [200/600], Loss: 0.0452
Epoch [23/30], Step [300/600], Loss: 0.0409
Epoch [23/30], Step [400/600], Loss: 0.0517
Epoch [23/30], Step [500/600], Loss: 0.0404
Epoch [23/30], Step [600/600], Loss: 0.0853
Epoch [24/30], Step [100/600], Loss: 0.0939
Epoch [24/30], Step [200/600], Loss: 0.0728
Epoch [24/30], Step [300/600], Loss: 0.0334
Epoch [24/30], Step [400/600], Loss: 0.0385
Epoch [24/30], Step [500/600], Loss: 0.0339
Epoch [24/30], Step [600/600], Loss: 0.1187
Epoch [25/30], Step [100/600], Loss: 0.0291
Epoch [25/30], Step [200/600], Loss: 0.0209
Epoch [25/30], Step [300/600], Loss: 0.0638
Epoch [25/30], Step [400/600], Loss: 0.1146
Epoch [25/30], Step [500/600], Loss: 0.0283
Epoch [25/30], Step [600/600], Loss: 0.0954
Epoch [26/30], Step [100/600], Loss: 0.0835
Epoch [26/30], Step [200/600], Loss: 0.0453
Epoch [26/30], Step [300/600], Loss: 0.0255
Epoch [26/30], Step [400/600], Loss: 0.0836
Epoch [26/30], Step [500/600], Loss: 0.0456
Epoch [26/30], Step [600/600], Loss: 0.0207
Epoch [27/30], Step [100/600], Loss: 0.1213
Epoch [27/30], Step [200/600], Loss: 0.0258
Epoch [27/30], Step [300/600], Loss: 0.0545
Epoch [27/30], Step [400/600], Loss: 0.0296
Epoch [27/30], Step [500/600], Loss: 0.0507
Epoch [27/30], Step [600/600], Loss: 0.0386
Epoch [28/30], Step [100/600], Loss: 0.0545
Epoch [28/30], Step [200/600], Loss: 0.0468
Epoch [28/30], Step [300/600], Loss: 0.2291
Epoch [28/30], Step [400/600], Loss: 0.1048
Epoch [28/30], Step [500/600], Loss: 0.0407
Epoch [28/30], Step [600/600], Loss: 0.0895
Epoch [29/30], Step [100/600], Loss: 0.0504
Epoch [29/30], Step [200/600], Loss: 0.0448
Epoch [29/30], Step [300/600], Loss: 0.0254
Epoch [29/30], Step [400/600], Loss: 0.0468
Epoch [29/30], Step [500/600], Loss: 0.0783
Epoch [29/30], Step [600/600], Loss: 0.0542
Epoch [30/30], Step [100/600], Loss: 0.0531
Epoch [30/30], Step [200/600], Loss: 0.0461
Epoch [30/30], Step [300/600], Loss: 0.0339
Epoch [30/30], Step [400/600], Loss: 0.0592
Epoch [30/30], Step [500/600], Loss: 0.0287
Epoch [30/30], Step [600/600], Loss: 0.0403</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试模型</span></span><br><span class="line">model.eval()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    correct=<span class="number">0</span></span><br><span class="line">    total=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images,labels <span class="keyword">in</span> test_dataloader:</span><br><span class="line">        images=images.to(device)</span><br><span class="line">        labels=labels.to(device)</span><br><span class="line">        outputs=model(images)</span><br><span class="line">        _,predicted=torch.max(outputs.data,<span class="number">1</span>)</span><br><span class="line">        total+=labels.size(<span class="number">0</span>)</span><br><span class="line">        correct+=(predicted==labels).sum().item()</span><br><span class="line">        </span><br><span class="line">    print(<span class="string">'Test Accuracy of the model on the 10000 test images:&#123;&#125;%'</span>.format(<span class="number">100</span>*correct/total))</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存模型的权重</span></span><br><span class="line">torch.save(model.state_dict(),<span class="string">'cnn.ckpt'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Test Accuracy of the model on the 10000 test images:98.37%</code></pre>]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-Day5</title>
    <url>/ck3bmvcxa002qbsg4b0tydr3h.html</url>
    <content><![CDATA[<h3 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h3><ul>
<li>这里仍然使用MNIST数据集</li>
</ul>
<a id="more"></a>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设备的配置（这里使用GPU来跑训练）</span></span><br><span class="line">device=torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>

<pre><code>cuda</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设定超参数</span></span><br><span class="line">input_size=<span class="number">784</span></span><br><span class="line">hidden_size=<span class="number">500</span></span><br><span class="line">num_classes=<span class="number">10</span></span><br><span class="line">num_epochs=<span class="number">30</span></span><br><span class="line">batch_size=<span class="number">100</span></span><br><span class="line">learning_rate=<span class="number">0.001</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加载MNIST数据集</span></span><br><span class="line">train_dataset=torchvision.datasets.MNIST(root=<span class="string">'./data'</span>,train=<span class="literal">True</span>,download=<span class="literal">True</span>,transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">test_dataset=torchvision.datasets.MNIST(root=<span class="string">'./data'</span>,train=<span class="literal">False</span>,transform=transforms.ToTensor())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义数据加载器（数据输入流水线）</span></span><br><span class="line">train_dataloader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_dataloader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#一个隐含层的全连接神经网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size,hidden_size,num_classes)</span>:</span></span><br><span class="line">        super(NeuralNet,self).__init__()</span><br><span class="line">        self.fc1=nn.Linear(input_size,hidden_size)</span><br><span class="line">        self.relu=nn.ReLU()</span><br><span class="line">        self.fc2=nn.Linear(hidden_size,num_classes)</span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        out=self.fc1(x)</span><br><span class="line">        out=self.relu(out)</span><br><span class="line">        out=self.fc2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">model=NeuralNet(input_size,hidden_size,num_classes).to(device)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义损失函数和优化器</span></span><br><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">total_step=len(train_dataloader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i,(images,labels) <span class="keyword">in</span> enumerate(train_dataloader):</span><br><span class="line">        <span class="comment">#将tensor张量移动到GPU上</span></span><br><span class="line">        images=images.reshape(<span class="number">-1</span>,<span class="number">28</span>*<span class="number">28</span>).to(device)</span><br><span class="line">        labels=labels.to(device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#前向传播</span></span><br><span class="line">        outputs=model(images)</span><br><span class="line">        loss=criterion(outputs,labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#梯度置0+反向传播+更新权重</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>)%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;'</span> .format(epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, total_step, loss.item()))</span><br></pre></td></tr></table></figure>

<pre><code>Epoch [1/30], Step [100/600], Loss: 2.2577
Epoch [1/30], Step [200/600], Loss: 2.2602
Epoch [1/30], Step [300/600], Loss: 2.2123
Epoch [1/30], Step [400/600], Loss: 2.2026
Epoch [1/30], Step [500/600], Loss: 2.1547
Epoch [1/30], Step [600/600], Loss: 2.1598
Epoch [2/30], Step [100/600], Loss: 2.0920
Epoch [2/30], Step [200/600], Loss: 2.0759
Epoch [2/30], Step [300/600], Loss: 2.0557
Epoch [2/30], Step [400/600], Loss: 2.0411
Epoch [2/30], Step [500/600], Loss: 1.9963
Epoch [2/30], Step [600/600], Loss: 1.9562
Epoch [3/30], Step [100/600], Loss: 1.8933
Epoch [3/30], Step [200/600], Loss: 1.8954
Epoch [3/30], Step [300/600], Loss: 1.8854
Epoch [3/30], Step [400/600], Loss: 1.9073
Epoch [3/30], Step [500/600], Loss: 1.8073
Epoch [3/30], Step [600/600], Loss: 1.7214
Epoch [4/30], Step [100/600], Loss: 1.7209
Epoch [4/30], Step [200/600], Loss: 1.6359
Epoch [4/30], Step [300/600], Loss: 1.6842
Epoch [4/30], Step [400/600], Loss: 1.6216
Epoch [4/30], Step [500/600], Loss: 1.6705
Epoch [4/30], Step [600/600], Loss: 1.5234
Epoch [5/30], Step [100/600], Loss: 1.4869
Epoch [5/30], Step [200/600], Loss: 1.4968
Epoch [5/30], Step [300/600], Loss: 1.4577
Epoch [5/30], Step [400/600], Loss: 1.5792
Epoch [5/30], Step [500/600], Loss: 1.4062
Epoch [5/30], Step [600/600], Loss: 1.3446
Epoch [6/30], Step [100/600], Loss: 1.3835
Epoch [6/30], Step [200/600], Loss: 1.2209
Epoch [6/30], Step [300/600], Loss: 1.3533
Epoch [6/30], Step [400/600], Loss: 1.2524
Epoch [6/30], Step [500/600], Loss: 1.1529
Epoch [6/30], Step [600/600], Loss: 1.1282
Epoch [7/30], Step [100/600], Loss: 1.1611
Epoch [7/30], Step [200/600], Loss: 1.1645
Epoch [7/30], Step [300/600], Loss: 1.0757
Epoch [7/30], Step [400/600], Loss: 1.1130
Epoch [7/30], Step [500/600], Loss: 1.1147
Epoch [7/30], Step [600/600], Loss: 0.9933
Epoch [8/30], Step [100/600], Loss: 0.9965
Epoch [8/30], Step [200/600], Loss: 1.0421
Epoch [8/30], Step [300/600], Loss: 0.9947
Epoch [8/30], Step [400/600], Loss: 0.9643
Epoch [8/30], Step [500/600], Loss: 0.8699
Epoch [8/30], Step [600/600], Loss: 0.9051
Epoch [9/30], Step [100/600], Loss: 0.9915
Epoch [9/30], Step [200/600], Loss: 0.8200
Epoch [9/30], Step [300/600], Loss: 0.8870
Epoch [9/30], Step [400/600], Loss: 0.7617
Epoch [9/30], Step [500/600], Loss: 0.8657
Epoch [9/30], Step [600/600], Loss: 0.8298
Epoch [10/30], Step [100/600], Loss: 0.8165
Epoch [10/30], Step [200/600], Loss: 0.8207
Epoch [10/30], Step [300/600], Loss: 0.8202
Epoch [10/30], Step [400/600], Loss: 0.8262
Epoch [10/30], Step [500/600], Loss: 0.8131
Epoch [10/30], Step [600/600], Loss: 0.8367
Epoch [11/30], Step [100/600], Loss: 0.8376
Epoch [11/30], Step [200/600], Loss: 0.8952
Epoch [11/30], Step [300/600], Loss: 0.8049
Epoch [11/30], Step [400/600], Loss: 0.8018
Epoch [11/30], Step [500/600], Loss: 0.6761
Epoch [11/30], Step [600/600], Loss: 0.7581
Epoch [12/30], Step [100/600], Loss: 0.6733
Epoch [12/30], Step [200/600], Loss: 0.7217
Epoch [12/30], Step [300/600], Loss: 0.7343
Epoch [12/30], Step [400/600], Loss: 0.6188
Epoch [12/30], Step [500/600], Loss: 0.6395
Epoch [12/30], Step [600/600], Loss: 0.6714
Epoch [13/30], Step [100/600], Loss: 0.7165
Epoch [13/30], Step [200/600], Loss: 0.6684
Epoch [13/30], Step [300/600], Loss: 0.6127
Epoch [13/30], Step [400/600], Loss: 0.6626
Epoch [13/30], Step [500/600], Loss: 0.6130
Epoch [13/30], Step [600/600], Loss: 0.6483
Epoch [14/30], Step [100/600], Loss: 0.5773
Epoch [14/30], Step [200/600], Loss: 0.6234
Epoch [14/30], Step [300/600], Loss: 0.5617
Epoch [14/30], Step [400/600], Loss: 0.6304
Epoch [14/30], Step [500/600], Loss: 0.5647
Epoch [14/30], Step [600/600], Loss: 0.6992
Epoch [15/30], Step [100/600], Loss: 0.7693
Epoch [15/30], Step [200/600], Loss: 0.5023
Epoch [15/30], Step [300/600], Loss: 0.6256
Epoch [15/30], Step [400/600], Loss: 0.5553
Epoch [15/30], Step [500/600], Loss: 0.5332
Epoch [15/30], Step [600/600], Loss: 0.6705
Epoch [16/30], Step [100/600], Loss: 0.5651
Epoch [16/30], Step [200/600], Loss: 0.6031
Epoch [16/30], Step [300/600], Loss: 0.6298
Epoch [16/30], Step [400/600], Loss: 0.6218
Epoch [16/30], Step [500/600], Loss: 0.5065
Epoch [16/30], Step [600/600], Loss: 0.5554
Epoch [17/30], Step [100/600], Loss: 0.5246
Epoch [17/30], Step [200/600], Loss: 0.6190
Epoch [17/30], Step [300/600], Loss: 0.5547
Epoch [17/30], Step [400/600], Loss: 0.5002
Epoch [17/30], Step [500/600], Loss: 0.6297
Epoch [17/30], Step [600/600], Loss: 0.6091
Epoch [18/30], Step [100/600], Loss: 0.5265
Epoch [18/30], Step [200/600], Loss: 0.4593
Epoch [18/30], Step [300/600], Loss: 0.5204
Epoch [18/30], Step [400/600], Loss: 0.5156
Epoch [18/30], Step [500/600], Loss: 0.5276
Epoch [18/30], Step [600/600], Loss: 0.5380
Epoch [19/30], Step [100/600], Loss: 0.4246
Epoch [19/30], Step [200/600], Loss: 0.4761
Epoch [19/30], Step [300/600], Loss: 0.5177
Epoch [19/30], Step [400/600], Loss: 0.5698
Epoch [19/30], Step [500/600], Loss: 0.5143
Epoch [19/30], Step [600/600], Loss: 0.4552
Epoch [20/30], Step [100/600], Loss: 0.4989
Epoch [20/30], Step [200/600], Loss: 0.4384
Epoch [20/30], Step [300/600], Loss: 0.5648
Epoch [20/30], Step [400/600], Loss: 0.4323
Epoch [20/30], Step [500/600], Loss: 0.4153
Epoch [20/30], Step [600/600], Loss: 0.5401
Epoch [21/30], Step [100/600], Loss: 0.4298
Epoch [21/30], Step [200/600], Loss: 0.4795
Epoch [21/30], Step [300/600], Loss: 0.4865
Epoch [21/30], Step [400/600], Loss: 0.6770
Epoch [21/30], Step [500/600], Loss: 0.4689
Epoch [21/30], Step [600/600], Loss: 0.5034
Epoch [22/30], Step [100/600], Loss: 0.4461
Epoch [22/30], Step [200/600], Loss: 0.3873
Epoch [22/30], Step [300/600], Loss: 0.4634
Epoch [22/30], Step [400/600], Loss: 0.4536
Epoch [22/30], Step [500/600], Loss: 0.5801
Epoch [22/30], Step [600/600], Loss: 0.3840
Epoch [23/30], Step [100/600], Loss: 0.4354
Epoch [23/30], Step [200/600], Loss: 0.3644
Epoch [23/30], Step [300/600], Loss: 0.5900
Epoch [23/30], Step [400/600], Loss: 0.5235
Epoch [23/30], Step [500/600], Loss: 0.4935
Epoch [23/30], Step [600/600], Loss: 0.4815
Epoch [24/30], Step [100/600], Loss: 0.3946
Epoch [24/30], Step [200/600], Loss: 0.5413
Epoch [24/30], Step [300/600], Loss: 0.4906
Epoch [24/30], Step [400/600], Loss: 0.4895
Epoch [24/30], Step [500/600], Loss: 0.5062
Epoch [24/30], Step [600/600], Loss: 0.4927
Epoch [25/30], Step [100/600], Loss: 0.5096
Epoch [25/30], Step [200/600], Loss: 0.5281
Epoch [25/30], Step [300/600], Loss: 0.5560
Epoch [25/30], Step [400/600], Loss: 0.3325
Epoch [25/30], Step [500/600], Loss: 0.5069
Epoch [25/30], Step [600/600], Loss: 0.4295
Epoch [26/30], Step [100/600], Loss: 0.3942
Epoch [26/30], Step [200/600], Loss: 0.5095
Epoch [26/30], Step [300/600], Loss: 0.4046
Epoch [26/30], Step [400/600], Loss: 0.4315
Epoch [26/30], Step [500/600], Loss: 0.4742
Epoch [26/30], Step [600/600], Loss: 0.4902
Epoch [27/30], Step [100/600], Loss: 0.5214
Epoch [27/30], Step [200/600], Loss: 0.5570
Epoch [27/30], Step [300/600], Loss: 0.4271
Epoch [27/30], Step [400/600], Loss: 0.4110
Epoch [27/30], Step [500/600], Loss: 0.3797
Epoch [27/30], Step [600/600], Loss: 0.4555
Epoch [28/30], Step [100/600], Loss: 0.4667
Epoch [28/30], Step [200/600], Loss: 0.4648
Epoch [28/30], Step [300/600], Loss: 0.4887
Epoch [28/30], Step [400/600], Loss: 0.3500
Epoch [28/30], Step [500/600], Loss: 0.4439
Epoch [28/30], Step [600/600], Loss: 0.4054
Epoch [29/30], Step [100/600], Loss: 0.5129
Epoch [29/30], Step [200/600], Loss: 0.4508
Epoch [29/30], Step [300/600], Loss: 0.4131
Epoch [29/30], Step [400/600], Loss: 0.4463
Epoch [29/30], Step [500/600], Loss: 0.5682
Epoch [29/30], Step [600/600], Loss: 0.3458
Epoch [30/30], Step [100/600], Loss: 0.3850
Epoch [30/30], Step [200/600], Loss: 0.4949
Epoch [30/30], Step [300/600], Loss: 0.3873
Epoch [30/30], Step [400/600], Loss: 0.4957
Epoch [30/30], Step [500/600], Loss: 0.3352
Epoch [30/30], Step [600/600], Loss: 0.4667</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试模型（在测试阶段不需要再计算梯度）</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    correct=<span class="number">0</span></span><br><span class="line">    total=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images,labels <span class="keyword">in</span> test_dataloader:</span><br><span class="line">        images=images.reshape(<span class="number">-1</span>,<span class="number">28</span>*<span class="number">28</span>).to(device)</span><br><span class="line">        labels=labels.to(device)</span><br><span class="line">        outputs=model(images)</span><br><span class="line">        _,predicted=torch.max(outputs.data,<span class="number">1</span>)</span><br><span class="line">        total+=labels.size(<span class="number">0</span>)</span><br><span class="line">        correct+=(predicted==labels).sum().item()</span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images:&#123;&#125;%'</span>.format(<span class="number">100</span>*correct/total))</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">torch.save(model.state_dict(),<span class="string">'NeuralNet.ckpt'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy of the network on the 10000 test images:89.47%</code></pre>]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>安装anaconda虚拟环境、pytorch和jupyter notebook（远程访问）</title>
    <url>/ck3bmvcy3003jbsg4h0wv1wbc.html</url>
    <content><![CDATA[<p>最近在学习Pytorch，然后实验室有一台双卡的服务器，显卡是特斯拉（貌似很nb），所以就想在服务器上跑代码，炼丹。今天花了点时间配置了一下环境，主要安装了anaconda和pytorch(gpu版本)，为此记录一下= =。</p>
<a id="more"></a>

<h3 id="Anaconda简介"><a href="#Anaconda简介" class="headerlink" title="Anaconda简介"></a>Anaconda简介</h3><p>anaconda 是一个python的发行版，包括了python和很多常见的软件库, 和一个包管理器conda。<br>常见的科学计算类的库都包含在里面了，使得安装比常规python安装要容易。主要是！装了anaconda就不需要单独装python了。<br>因为Anaconda就是用来管理我们不同版本的python环境的。</p>
<p>对整个python环境, 最关键的是需要有一个解释器, 和一个包集合，所有的第三方包都放在site-packages文件夹里面。<br>比如说一个爬虫脚本用到了第三方的requests包,而另一台计算机是刚刚是装好原始python的, 也就是说根本没有任何第三方包,<br>那么这个爬虫脚本是无法在另一台机器上运行的。（因为需要requests包的支持。）</p>
<h3 id="Anaconda安装"><a href="#Anaconda安装" class="headerlink" title="Anaconda安装"></a>Anaconda安装</h3><p>Anaconda和Python版本是对应的，所以需要选择安装对应Python2.7版本的还是Python3.7版本或其他版本的，根据自己的需要下载合适的安装包。</p>
<p>下载链接：<a href="https://www.anaconda.com/download/#linux" target="_blank" rel="noopener">https://www.anaconda.com/download/#linux</a></p>
<p> 点击下面的64-Bit (x86) Installer ,下载64位的版本，下载完成是：Anaconda3-日期-Linux-x86_64.sh，这是一个shell脚本文件。 </p>
<h4 id="安装步骤："><a href="#安装步骤：" class="headerlink" title="安装步骤："></a>安装步骤：</h4><ul>
<li><p>进入安装包Anaconda3-日期-Linux-x86_64.sh所在目录，打开终端</p>
</li>
<li><p>执行：bash Anaconda3-日期-Linux-x86_64.sh，然后一路回车，知道出现提示，输入yes</p>
</li>
<li><p>回车选择默认路径，然后开始安装，最后会询问你是否添加到环境变量，输入yes</p>
</li>
<li><p>这样就安装OK了，不需要重启</p>
</li>
</ul>
<h4 id="检查是否成功："><a href="#检查是否成功：" class="headerlink" title="检查是否成功："></a>检查是否成功：</h4><ul>
<li>打开新终端，输入python，里边含有Anaconda信息即可</li>
<li>或者输入<code>conda --version</code>，如果有版本号，也是OK的</li>
</ul>
<h3 id="管理虚拟环境"><a href="#管理虚拟环境" class="headerlink" title="管理虚拟环境"></a>管理虚拟环境</h3><p> 安装了anaconda以后，我们可以用它来创建我们一个个独立的python环境 </p>
<h4 id="创建独立的虚拟环境"><a href="#创建独立的虚拟环境" class="headerlink" title="创建独立的虚拟环境"></a>创建独立的虚拟环境</h4><p><code>conda create -n pytorch_gpu python=3.7</code></p>
<h4 id="source-activate命令进入到虚拟环境"><a href="#source-activate命令进入到虚拟环境" class="headerlink" title="source activate命令进入到虚拟环境"></a>source activate命令进入到虚拟环境</h4><p><code>source activate pytorch_gpu</code></p>
<h4 id="activate命令进入到虚拟环境"><a href="#activate命令进入到虚拟环境" class="headerlink" title="activate命令进入到虚拟环境"></a>activate命令进入到虚拟环境</h4><p><code>source activate pytorch_gpu</code></p>
<h4 id="显示当前虚拟环境所安装的包"><a href="#显示当前虚拟环境所安装的包" class="headerlink" title="显示当前虚拟环境所安装的包"></a>显示当前虚拟环境所安装的包</h4><p><code>conda list</code></p>
<h4 id="安装需要的包"><a href="#安装需要的包" class="headerlink" title="安装需要的包"></a>安装需要的包</h4><p><code>conda install 包的名字</code></p>
<h3 id="安装pytorch-GPU版本"><a href="#安装pytorch-GPU版本" class="headerlink" title="安装pytorch(GPU版本)"></a>安装pytorch(GPU版本)</h3><p>准备工作（ 用conda安装Pytorch过程中会连接失败，这是因为Anaconda.org的服务器在国外，需要切换到国内镜像源 ）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ </span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br></pre></td></tr></table></figure>

<p>官网一步到位： <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">https://pytorch.org/get-started/locally/</a> </p>
<p>选择适合自己的版本即可，我看了Ubuntu的显卡信息<code>nvidia-smi</code>,发现cuda是10.0版本的，所以cudatoolkit改成10.0</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit=10.0 -c pytorch</span><br></pre></td></tr></table></figure>

<p>最后安装完成，打开python，<code>import torch</code>不出错即可 </p>
<p>查看GPU是否可用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device=torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>

<h3 id="Jupyter-notebook简介"><a href="#Jupyter-notebook简介" class="headerlink" title="Jupyter notebook简介"></a>Jupyter notebook简介</h3><blockquote>
<p>Jupyter Notebook是基于网页的用于交互计算的应用程序。其可被应用于全过程计算：开发、文档编写、运行代码和展示结果。——<a href="https://link.jianshu.com?t=https%3A%2F%2Fjupyter-notebook.readthedocs.io%2Fen%2Fstable%2Fnotebook.html" target="_blank" rel="noopener">Jupyter Notebook官方介绍</a></p>
</blockquote>
<p>简而言之，Jupyter Notebook是以网页的形式打开，可以在网页页面中<strong>直接</strong>编写代码和运行代码，代码的运行结果也会直接在代码块下显示。如在编程过程中需要编写说明文档，可在同一个页面中直接编写，便于作及时的说明和解释。</p>
<h3 id="安装Jupyter-Notebook"><a href="#安装Jupyter-Notebook" class="headerlink" title="安装Jupyter Notebook"></a>安装Jupyter Notebook</h3><p>之前我已经安装过了anaconda，所以直接使用<code>conda install jupyter</code>即可</p>
<p>安装完成后，因为我是远程连接服务器，所以就要设置jupyter notebook远程访问，这样写代码比较方便</p>
<h3 id="Jupyter-nootbook远程访问设置"><a href="#Jupyter-nootbook远程访问设置" class="headerlink" title="Jupyter nootbook远程访问设置"></a>Jupyter nootbook远程访问设置</h3><h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><p>首先，我们需要修改一个名为“upyter_notebook_config.py”的文件，从其命名规则可以看出，这是有关Notebook的配置文件。</p>
<p>其实，在本质上，它是一个由Python编写的脚本文档。</p>
<p>通常，这个文档位于我们的家目录（home directory）之下，不同的操作系统，它所处的位置稍有不同，大致如下：</p>
<p><code>Linux: /home/USERNAME/.jupyter/jupyter_notebook_config.py</code></p>
<p>上面的「USERNAME」就是前面我们提到的诸如‘jpnb’这样的用户名，我们根据实际情况替换为实际路径即可。</p>
<p>如果你在上述路径下没有找到这个配置文件，那么就需要在终端运行如下命令：</p>
<p><code>jupyter notebook --generate-config</code>即可， 这个命令的功能，就是创建Jupyter文件夹和配置文件「jupyter_notebook_config.py」。 </p>
<h4 id="设置Jupyter远程访问密码"><a href="#设置Jupyter远程访问密码" class="headerlink" title="设置Jupyter远程访问密码"></a>设置Jupyter远程访问密码</h4><h4 id="设置访问密码"><a href="#设置访问密码" class="headerlink" title="设置访问密码"></a>设置访问密码</h4><p>打开终端输入：<code>jupyter notebook password</code></p>
<h4 id="下面我们还需要手动生成一个hash密码"><a href="#下面我们还需要手动生成一个hash密码" class="headerlink" title="下面我们还需要手动生成一个hash密码,"></a>下面我们还需要手动生成一个hash密码,</h4><p>如果你没有生成这么一个hash密码的话，那么每次通过浏览器远程访问Jupyter时，你都需要输入一次密码，这很繁琐！  但如果我们启用了这个hash密码，只需要首次远程访问Jupyter文档时，输入一次密码，在下次访问时，这个hash密码就好比一个钥匙（token），替我们打开密码之门，也就是免密码登录。 </p>
<p> 为了生成这个hash密码，我们需要在终端输入“ipython”（全部小写）命令，以进入IPython的交互shell。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> notebook.auth <span class="keyword">import</span> passwd</span><br><span class="line">In [<span class="number">2</span>]: passwd()</span><br><span class="line">Enter password:</span><br><span class="line">Verify password:</span><br><span class="line">Out[<span class="number">2</span>]: <span class="string">'sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed'</span></span><br></pre></td></tr></table></figure>

<p>然后exit()退出IPython</p>
<h4 id="将hash密码添加到配置文件中"><a href="#将hash密码添加到配置文件中" class="headerlink" title="将hash密码添加到配置文件中"></a>将hash密码添加到配置文件中</h4><p> 下面，我们把前面生成的hash密码，添加到前面生成的配置文件：jupyter_notebook_config.py， </p>
<p>找到c.NotebookApp.password 所在行，将如下代码：</p>
<p><code>#c.NotebookApp.password = &#39; &#39;</code>将#注释去掉，然后改成<code>c.NotebookApp.password = u&#39;sha1:67c9e60bb8b6:9ffede0825894254b2e042ea597d771089e11aed&#39;</code></p>
<p> 这里，特别需要注意的是，hash密码字符串前面的那个u不可省略。 </p>
<h4 id="编辑配置文件"><a href="#编辑配置文件" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h4><p>将默认值False修改为True，表示允许外部访问，这个设置和下面IP设置，双重保障外部用户访问的可达性：</p>
<blockquote>
<p>c.NotebookApp.allow_remote_access = True</p>
</blockquote>
<p>等号右边的‘localhost’（仅仅运行本地访问），修改为‘*’，表示允许所有IP皆可访问</p>
<blockquote>
<p>c.NotebookApp.ip=’*’</p>
</blockquote>
<p>禁止自动打开浏览器</p>
<blockquote>
<p>c.NotebookApp.open_browser = False</p>
</blockquote>
<p>设置一个固定的notebook服务会监听的IP端口（这里设置为9999），这个值可以任意，只要保证不和其他已经启用的端口号冲突即可。</p>
<blockquote>
<p>c.NotebookApp.port = 9999</p>
</blockquote>
<p>做完上述配置之后，保存文件并退出。</p>
<p>但此时修改的配置并没有生效，我们还需要在终端输入“jupyter notebook”命令，这样确保Jupyter重新加载jupyter_notebook_config.py，进而使得新配置起效。</p>
<p>最后打开浏览器，输入相应IP:端口号即可。</p>
<h3 id="修改jupyter-notebook的默认工作路径"><a href="#修改jupyter-notebook的默认工作路径" class="headerlink" title="修改jupyter notebook的默认工作路径"></a>修改jupyter notebook的默认工作路径</h3><p>找到刚才的 <strong>jupyter_notebook_config.py</strong> 配置文件，然后定位到 <code>#c.NotebookApp.notebook_dir = &#39;&#39;</code></p>
<p>然后将其改成你的工作目录即可</p>
]]></content>
      <categories>
        <category>环境配置</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title>139单词拆分</title>
    <url>/ck3bmvcsm0006bsg48gan49uw.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p> 给定一个<strong>非空</strong>字符串 <em>s</em> 和一个包含<strong>非空</strong>单词列表的字典 wordDict，判定 s 是否可以被空格拆分为一个或多个在字典中出现的单词。 </p>
<p><strong>说明：</strong></p>
<ul>
<li>拆分时可以重复使用字典中的单词。</li>
<li>你可以假设字典中没有重复的单词。</li>
</ul>
<a id="more"></a>

<blockquote>
<p><strong>示例 1：</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: s = <span class="string">"leetcode"</span>, wordDict = [<span class="string">"leet"</span>, <span class="string">"code"</span>]</span><br><span class="line">输出: <span class="literal">true</span></span><br><span class="line">解释: 返回 <span class="literal">true</span> 因为 <span class="string">"leetcode"</span> 可以被拆分成 <span class="string">"leet code"</span>。</span><br></pre></td></tr></table></figure>

<p><strong>示例 2：</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: s = <span class="string">"applepenapple"</span>, wordDict = [<span class="string">"apple"</span>, <span class="string">"pen"</span>]</span><br><span class="line">输出: <span class="literal">true</span></span><br><span class="line">解释: 返回 <span class="literal">true</span> 因为 <span class="string">"applepenapple"</span> 可以被拆分成 <span class="string">"apple pen apple"</span>。</span><br><span class="line">     注意你可以重复使用字典中的单词。</span><br></pre></td></tr></table></figure>

<p><strong>示例 3：</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: s = <span class="string">"catsandog"</span>, wordDict = [<span class="string">"cats"</span>, <span class="string">"dog"</span>, <span class="string">"sand"</span>, <span class="string">"and"</span>, <span class="string">"cat"</span>]</span><br><span class="line">输出: <span class="literal">false</span></span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="解题思路：本题最开始的思路是，遍历字符串，str初试为空，一直加，加到能在字典数组里查到有一个字符串和其相等，然后str再为空，最后能遍历完数组，在最后一位结束也能在字典中查到即可，但是对于aaaaaaa-“aaaa”-“aaa”-，这样的测试数据就没法通过了-，比如aaa，aaa，只剩下一个a，返回false（字典中没有一个a），但是他其实是可以的，先aaaa，再aaa-，所以这种方法宣告失败！后来借鉴了一下动态规划的思想，比较巧妙，dp-i-表示i之前都能够分割完成，相当于将子结构分为两部分，从j的位置开始到i，如果dp-j-1并且-j-i-1-这一部分又能在字典中找到，就意味着i之前这个子结构就是可分的，那么一直遍历到最后，如果字符串长度最后结果还是1的话，代表整个字符串都能够成功分割！"><a href="#解题思路：本题最开始的思路是，遍历字符串，str初试为空，一直加，加到能在字典数组里查到有一个字符串和其相等，然后str再为空，最后能遍历完数组，在最后一位结束也能在字典中查到即可，但是对于aaaaaaa-“aaaa”-“aaa”-，这样的测试数据就没法通过了-，比如aaa，aaa，只剩下一个a，返回false（字典中没有一个a），但是他其实是可以的，先aaaa，再aaa-，所以这种方法宣告失败！后来借鉴了一下动态规划的思想，比较巧妙，dp-i-表示i之前都能够分割完成，相当于将子结构分为两部分，从j的位置开始到i，如果dp-j-1并且-j-i-1-这一部分又能在字典中找到，就意味着i之前这个子结构就是可分的，那么一直遍历到最后，如果字符串长度最后结果还是1的话，代表整个字符串都能够成功分割！" class="headerlink" title="解题思路：本题最开始的思路是，遍历字符串，str初试为空，一直加，加到能在字典数组里查到有一个字符串和其相等，然后str再为空，最后能遍历完数组，在最后一位结束也能在字典中查到即可，但是对于aaaaaaa     “aaaa”   “aaa”   ，这样的测试数据就没法通过了= =，比如aaa，aaa，只剩下一个a，返回false（字典中没有一个a），但是他其实是可以的，先aaaa，再aaa~，所以这种方法宣告失败！后来借鉴了一下动态规划的思想，比较巧妙，dp[i]表示i之前都能够分割完成，相当于将子结构分为两部分，从j的位置开始到i，如果dp[j]=1并且[j,i-1]这一部分又能在字典中找到，就意味着i之前这个子结构就是可分的，那么一直遍历到最后，如果字符串长度最后结果还是1的话，代表整个字符串都能够成功分割！"></a>解题思路：本题最开始的思路是，遍历字符串，str初试为空，一直加，加到能在字典数组里查到有一个字符串和其相等，然后str再为空，最后能遍历完数组，在最后一位结束也能在字典中查到即可，但是对于aaaaaaa     “aaaa”   “aaa”   ，这样的测试数据就没法通过了= =，比如aaa，aaa，只剩下一个a，返回false（字典中没有一个a），但是他其实是可以的，先aaaa，再aaa~，所以这种方法宣告失败！后来借鉴了一下动态规划的思想，比较巧妙，dp[i]表示i之前都能够分割完成，相当于将子结构分为两部分，从j的位置开始到i，如果dp[j]=1并且[j,i-1]这一部分又能在字典中找到，就意味着i之前这个子结构就是可分的，那么一直遍历到最后，如果字符串长度最后结果还是1的话，代表整个字符串都能够成功分割！</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">wordBreak</span><span class="params">(<span class="built_in">string</span> s, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; wordDict)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> size=s.size()+<span class="number">1</span>;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;dp(size);</span><br><span class="line">        dp[<span class="number">0</span>]=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;size;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="built_in">string</span> word : wordDict)&#123;</span><br><span class="line">                <span class="keyword">int</span> wsize=word.size();</span><br><span class="line">                <span class="keyword">if</span>(i&gt;=wsize)&#123;</span><br><span class="line">                    <span class="keyword">int</span> is_exist=s.compare(i-wsize,wsize,word);</span><br><span class="line">                    <span class="keyword">if</span>(is_exist==<span class="number">0</span>&amp;&amp;dp[i-wsize]==<span class="number">1</span>)dp[i]=<span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(dp[size<span class="number">-1</span>]==<span class="number">1</span>)<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>DFS深优先度搜索</title>
    <url>/ck3bmvcwe0024bsg4h8vxcyx0.html</url>
    <content><![CDATA[<p> 讲搜索当然不能撇开图，搜索思想在图问题中能以最直观的方式展现。 </p>
<p><strong>深度优先搜索的步骤分为 1.递归下去 2.回溯上来。顾名思义，深度优先，则是以深度为准则，先一条路走到底，直到达到目标。这里称之为递归下去。</strong></p>
<p><strong>如果既没有达到目标又无路可走了，那么则退回到上一步的状态，走其他路。这便是回溯上来。</strong></p>
<p>下面结合具体例子来理解。</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch-Day4</title>
    <url>/ck3bmvcx8002nbsg4db4ghefw.html</url>
    <content><![CDATA[<p>看完官方的入门文档之后，又在github上找了一个教程，这是一个韩国人写的教程，感觉还不错。</p>
<ul>
<li>这个资源为深度学习研究人员提供了学习PyTorch的教程</li>
<li>代码大多数模型都使用少于30行代码实现</li>
</ul>
<a id="more"></a>

<h3 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h3><ul>
<li>基本的自动求导 例子1</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建张量tensors(这里举得的例子是张量)</span></span><br><span class="line">x = torch.tensor(<span class="number">1.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">w = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个可以计算的公式 y=2x+3</span></span><br><span class="line">y = w * x + b</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.)
tensor(1.)
tensor(1.)</code></pre><h4 id="来解释一样2，1，1这三个数是怎么来的，首先y-2x-3，那么对于x求梯度（也就是求导）结果为2，以此类推。"><a href="#来解释一样2，1，1这三个数是怎么来的，首先y-2x-3，那么对于x求梯度（也就是求导）结果为2，以此类推。" class="headerlink" title="来解释一样2，1，1这三个数是怎么来的，首先y=2x+3，那么对于x求梯度（也就是求导）结果为2，以此类推。"></a>来解释一样2，1，1这三个数是怎么来的，首先y=2x+3，那么对于x求梯度（也就是求导）结果为2，以此类推。</h4><ul>
<li>基本的自动求导 例子2</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#创建一个 shape(10,3)和(10,2)的张量tensors</span></span><br><span class="line">x=torch.randn(<span class="number">10</span>,<span class="number">3</span>)</span><br><span class="line">y=torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个全连接层</span></span><br><span class="line">linear=nn.Linear(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">print(<span class="string">'w:'</span>,linear.weight)</span><br><span class="line">print(<span class="string">'b:'</span>,linear.bias)</span><br></pre></td></tr></table></figure>

<pre><code>w: Parameter containing:
tensor([[-0.1955,  0.2712,  0.5710],
        [-0.3143, -0.5540,  0.3058]], requires_grad=True)
b: Parameter containing:
tensor([-0.2155, -0.1076], requires_grad=True)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#建立一个损失函数和优化器</span></span><br><span class="line">criterion=nn.MSELoss()</span><br><span class="line">optimizer=torch.optim.SGD(linear.parameters(),lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">pred=linear(x)</span><br><span class="line"><span class="comment">#计算损失</span></span><br><span class="line">loss=criterion(pre,y)</span><br><span class="line"><span class="comment">#加上item()，就可以转换python中的float型数值</span></span><br><span class="line">print(<span class="string">'loss:'</span>,loss.item())</span><br></pre></td></tr></table></figure>

<pre><code>loss: 0.9739418625831604</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment">#打印输出梯度</span></span><br><span class="line">print(<span class="string">'dl/dw:'</span>,linear.weight.grad)</span><br><span class="line">print(<span class="string">'dl/db:'</span>,linear.bias.grad)</span><br></pre></td></tr></table></figure>

<pre><code>dl/dw: tensor([[ 0.0576,  0.1626,  0.4199],
        [-0.1823, -0.3512,  0.5736]])
dl/db: tensor([0.1189, 0.1488])</code></pre><ul>
<li>optimizer.step()这个方法会更新所有的参数。一旦梯度被如backward()之类的函数计算好后，我们就可以调用这个函数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#接着进行梯度下降</span></span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="comment">#打印输出在一次梯度下降优化后的损失</span></span><br><span class="line">pred=linear(x)</span><br><span class="line">loss=criterion(pred,y)</span><br><span class="line">print(<span class="string">'loss after 1 step optimization:'</span>,loss.item())</span><br></pre></td></tr></table></figure>

<pre><code>loss after 1 step optimization: 0.9595106840133667</code></pre><h4 id="从numpy中加载数据"><a href="#从numpy中加载数据" class="headerlink" title="从numpy中加载数据"></a>从numpy中加载数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#先创建一个numpy数组</span></span><br><span class="line">x=np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#将numpy数组转换成torch tensor（torch中的张量）</span></span><br><span class="line">y=torch.from_numpy(x)</span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#再将torch tensor转换成numpy数组</span></span><br><span class="line">z=y.numpy()</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1, 2],
        [3, 4]], dtype=torch.int32)
[[1 2]
 [3 4]]</code></pre><h4 id="定义数据输入流水线（类似于keras中加载数据集的意味）"><a href="#定义数据输入流水线（类似于keras中加载数据集的意味）" class="headerlink" title="定义数据输入流水线（类似于keras中加载数据集的意味）"></a>定义数据输入流水线（类似于keras中加载数据集的意味）</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#下载和创建CIFAR10数据集</span></span><br><span class="line">train_dataset=torchvision.datasets.CIFAR10(root=<span class="string">'./data/'</span>,train=<span class="literal">True</span>,transform=transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取一个数据对(图像和标签)（从磁盘中读取数据）</span></span><br><span class="line">image,label=train_dataset[<span class="number">0</span>]</span><br><span class="line">print(image.size())</span><br><span class="line">print(label)</span><br></pre></td></tr></table></figure>

<pre><code>Files already downloaded and verified
torch.Size([3, 32, 32])
6</code></pre><img src="/ck3bmvcx8002nbsg4db4ghefw/1.jpg" class="">

<p> 我们可以看出，图片是彩色图像（3通道），尺寸大小是32x32，标签的索引为6，我们知道cifar10：它有如下10个类别:(’airplane’,’automobile’,’bird’,’cat’,’deer’,’dog’,’frog’,’horse’,’ship’,’truck’)，猜测这个6就应该是frog青蛙。嘿，果然他就是！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个显示图片的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img=img/<span class="number">2</span>+<span class="number">0.5</span></span><br><span class="line">    npimg=img.numpy()</span><br><span class="line">    <span class="comment">#transpose对换数组的维度</span></span><br><span class="line">    plt.imshow(np.transpose(npimg,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line">    print(np.transpose(npimg,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)).shape)</span><br><span class="line">    plt.show();</span><br><span class="line"></span><br><span class="line">imshow(image)</span><br></pre></td></tr></table></figure>

<pre><code>(32, 32, 3)</code></pre><p>这个<code>img=img/2+0.5</code>就是对图像灰度做了点修正，然后transpose对换数组的维度，我们看啊，从数据集中读取的image，它的格式是(3,32,32)，而plt中要显示的图片，它的格式是(32,32,3)所以，要使用这个函数，将图片转换成我们能识别的格式，终于找到原因了= =！~</p>
<h4 id="数据加载到数据加载器中（它以非常简单的方式提供了队列和线程）"><a href="#数据加载到数据加载器中（它以非常简单的方式提供了队列和线程）" class="headerlink" title="数据加载到数据加载器中（它以非常简单的方式提供了队列和线程）"></a>数据加载到数据加载器中（它以非常简单的方式提供了队列和线程）</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#当迭代器开始的时候，队列和线程就会从文件中加载数据</span></span><br><span class="line">data_iter=iter(train_loader)</span><br><span class="line"></span><br><span class="line"><span class="comment">#mini-batch的images和labels</span></span><br><span class="line">images,labels=data_iter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据加载器的实际使用方法如下</span></span><br><span class="line"><span class="keyword">for</span> images,labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="comment">#训练代码写在这里</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h4 id="为自定义的数据集设计数据输入流水线"><a href="#为自定义的数据集设计数据输入流水线" class="headerlink" title="为自定义的数据集设计数据输入流水线"></a>为自定义的数据集设计数据输入流水线</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#首先创建自定义的数据集</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#To do</span></span><br><span class="line">        <span class="comment">#初始化你的文件路径和文件名列表</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self,index)</span>:</span></span><br><span class="line">        <span class="comment">#To do</span></span><br><span class="line">        <span class="comment">#首先从文件中取一个数据（举个例子：using numpy.fromfile, PIL.Image.open（打开一个图像文件））</span></span><br><span class="line">        <span class="comment">#接着对数据进行预处理（举个例子：torchvision.Transform）</span></span><br><span class="line">        <span class="comment">#PIL：Python Imaging Library，已经是Python平台事实上的图像处理标准库了。</span></span><br><span class="line">        <span class="comment">#torchvision.Transform对PIL.Image进行变换</span></span><br><span class="line">        <span class="comment">#最后返回一个数据对（image和label）</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#应该将数据集的总大小更改为0，意思就是全部训练完成</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#然后你就可以使用提前创建好的数据加载器</span></span><br><span class="line">custom_dataset=CustomDataset()</span><br><span class="line">train_loader=torch.utils.data.DataLoader(dataset=custom_dataset,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#下载和加载提前训练好的ResNet-18</span></span><br><span class="line">resnet=torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果你只想微调模型的顶层，可以按照下面设置</span></span><br><span class="line"><span class="keyword">for</span> parm <span class="keyword">in</span> resnet.parameters():</span><br><span class="line">    parm.requires_grad=<span class="literal">False</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#更换顶层进行微调(将原先resnet的全连接层的输入样本大小改为100)</span></span><br><span class="line">resnet.fc=nn.Linear(resnet.fc.in_features,<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#forward省略</span></span><br><span class="line"></span><br><span class="line">images=torch.randn(<span class="number">64</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">outputs=resnet(images)</span><br><span class="line">print(outputs.size())</span><br></pre></td></tr></table></figure>

<pre><code>Downloading: &quot;https://download.pytorch.org/models/resnet18-5c106cde.pth&quot; to C:\Users\user/.cache\torch\checkpoints\resnet18-5c106cde.pth
100%|█████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:29&lt;00:00, 1.59MB/s]


torch.Size([64, 100])</code></pre><h4 id="保存和加载模型"><a href="#保存和加载模型" class="headerlink" title="保存和加载模型"></a>保存和加载模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存和加载完整的模型</span></span><br><span class="line">torch.save(resnet,<span class="string">'model.ckpt'</span>)</span><br><span class="line">model=torch.load(<span class="string">'model.ckpt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#只保存和加载模型的权重参数（推荐！）</span></span><br><span class="line">torch.save(resnet.state_dict(),<span class="string">'params.ckpt'</span>)</span><br><span class="line">resnet.load_state_dict(torch.load(<span class="string">'params.ckpt'</span>))</span><br></pre></td></tr></table></figure>




<pre><code>&lt;All keys matched successfully&gt;</code></pre><ul>
<li>state_dict() 以dict返回optimizer的状态。它包含两项。<ul>
<li>state 一个保存了当前优化状态的dict。optimizer的类别不同，state的内容也会不同。</li>
<li>param_groups  一个包含了全部参数组的dict。</li>
</ul>
</li>
</ul>
<h3 id="线性回归练习"><a href="#线性回归练习" class="headerlink" title="线性回归练习"></a>线性回归练习</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h4 id="设置超参数Hyper-Parameters-就是人工设定的参数，不是从网络中学到的"><a href="#设置超参数Hyper-Parameters-就是人工设定的参数，不是从网络中学到的" class="headerlink" title="设置超参数Hyper-Parameters(就是人工设定的参数，不是从网络中学到的)"></a>设置超参数Hyper-Parameters(就是人工设定的参数，不是从网络中学到的)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_size=<span class="number">1</span></span><br><span class="line">output_size=<span class="number">1</span></span><br><span class="line">num_epochs=<span class="number">60</span></span><br><span class="line">learning_rate=<span class="number">0.001</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#准备数据集Toy dataset（玩具数据集，小数据集，缺乏实验研究，先这样理解，自己玩的数据集（自定义）= =）</span></span><br><span class="line"><span class="comment"># Toy dataset</span></span><br><span class="line">x_train = np.array([[<span class="number">3.3</span>], [<span class="number">4.4</span>], [<span class="number">5.5</span>], [<span class="number">6.71</span>], [<span class="number">6.93</span>], [<span class="number">4.168</span>], </span><br><span class="line">                    [<span class="number">9.779</span>], [<span class="number">6.182</span>], [<span class="number">7.59</span>], [<span class="number">2.167</span>], [<span class="number">7.042</span>], </span><br><span class="line">                    [<span class="number">10.791</span>], [<span class="number">5.313</span>], [<span class="number">7.997</span>], [<span class="number">3.1</span>]], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">y_train = np.array([[<span class="number">1.7</span>], [<span class="number">2.76</span>], [<span class="number">2.09</span>], [<span class="number">3.19</span>], [<span class="number">1.694</span>], [<span class="number">1.573</span>], </span><br><span class="line">                    [<span class="number">3.366</span>], [<span class="number">2.596</span>], [<span class="number">2.53</span>], [<span class="number">1.221</span>], [<span class="number">2.827</span>], </span><br><span class="line">                    [<span class="number">3.465</span>], [<span class="number">1.65</span>], [<span class="number">2.904</span>], [<span class="number">1.3</span>]], dtype=np.float32)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#线性回归的模型,这里输入输出都是一维的，标量，也就是全连接层</span></span><br><span class="line">model=nn.Linear(input_size,output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数和优化器</span></span><br><span class="line">criterion=nn.MSELoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment">#将numpy数组转换成torch tensors</span></span><br><span class="line">    inputs=torch.from_numpy(x_train)</span><br><span class="line">    targets=torch.from_numpy(y_train)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#前向传播</span></span><br><span class="line">    outputs=model(inputs)</span><br><span class="line">    loss=criterion(outputs,targets)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#反向传播和优化</span></span><br><span class="line">    <span class="comment">#zero_grad()将module中的所有模型参数的梯度设置为0.</span></span><br><span class="line">    <span class="comment">#将所有参数的梯度缓存清零,然后进行反向传播</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(epoch+<span class="number">1</span>)%<span class="number">5</span>==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Epoch [&#123;&#125;/&#123;&#125;],Loss:&#123;:.4f&#125;'</span>.format(epoch+<span class="number">1</span>,num_epochs,loss.item()))</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制图表</span></span><br><span class="line"><span class="comment">#这里如果不加detach()的话，会报错：</span></span><br><span class="line"><span class="comment">#Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.</span></span><br><span class="line"><span class="comment">#无法在需要grad的Variable上调用numpy()。 请改用var.detach().numpy()</span></span><br><span class="line"><span class="comment">#detach()的作用就是不带梯度，返回一个从当前图中分离下来的新的Variable，返回的Variable的requires_grad=False。</span></span><br><span class="line">predicted=model(torch.from_numpy(x_train)).detach().numpy()</span><br><span class="line">plt.plot(x_train,y_train,<span class="string">'ro'</span>,label=<span class="string">'Original data'</span>)</span><br><span class="line">plt.plot(x_train,predicted,label=<span class="string">'Fitted line'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存模型的权重（推荐，不推荐直接保存网络）</span></span><br><span class="line">torch.save(model.state_dict(),<span class="string">'linear.ckpt'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch [5/60],Loss:10.8415
Epoch [10/60],Loss:4.5517
Epoch [15/60],Loss:2.0035
Epoch [20/60],Loss:0.9710
Epoch [25/60],Loss:0.5526
Epoch [30/60],Loss:0.3829
Epoch [35/60],Loss:0.3140
Epoch [40/60],Loss:0.2860
Epoch [45/60],Loss:0.2745
Epoch [50/60],Loss:0.2696
Epoch [55/60],Loss:0.2675
Epoch [60/60],Loss:0.2665</code></pre><img src="/ck3bmvcx8002nbsg4db4ghefw/2.jpg" class="">


<h3 id="逻辑回归练习"><a href="#逻辑回归练习" class="headerlink" title="逻辑回归练习"></a>逻辑回归练习</h3><h4 id="设置超参数Hyper-Parameters-就是人工设定的参数，不是从网络中学到的-1"><a href="#设置超参数Hyper-Parameters-就是人工设定的参数，不是从网络中学到的-1" class="headerlink" title="设置超参数Hyper-Parameters(就是人工设定的参数，不是从网络中学到的)"></a>设置超参数Hyper-Parameters(就是人工设定的参数，不是从网络中学到的)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_size=<span class="number">784</span></span><br><span class="line">num_classes=<span class="number">10</span></span><br><span class="line">num_epochs=<span class="number">5</span></span><br><span class="line">batch_size=<span class="number">100</span></span><br><span class="line">learning_rate=<span class="number">0.001</span></span><br></pre></td></tr></table></figure>

<h4 id="经典的手写数字识别，MNIST数据集（images，labels）"><a href="#经典的手写数字识别，MNIST数据集（images，labels）" class="headerlink" title="经典的手写数字识别，MNIST数据集（images，labels）"></a>经典的手写数字识别，MNIST数据集（images，labels）</h4><ul>
<li>这里测试集就不需要<code>download=True</code>了，因为经查看文件夹，发现MNIST训练集和测试集在一个包里= =~</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dateset=torchvision.datasets.MNIST(root=<span class="string">'./data'</span>,train=<span class="literal">True</span>,transform=transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">test_dataset=torchvision.datasets.MNIST(root=<span class="string">'./data'</span>,train=<span class="literal">False</span>,transform=transforms.ToTensor())</span><br></pre></td></tr></table></figure>

<h4 id="数据加载-设计数据输入流水线"><a href="#数据加载-设计数据输入流水线" class="headerlink" title="数据加载(设计数据输入流水线)"></a>数据加载(设计数据输入流水线)</h4><ul>
<li>shuffle设置为True时会在每个epoch重新打乱数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_loader=torch.utils.data.DataLoader(dataset=train_dateset,batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="逻辑回归的模型"><a href="#逻辑回归的模型" class="headerlink" title="逻辑回归的模型"></a>逻辑回归的模型</h4><ul>
<li><p>这里有必要再介绍一下MNIST数据集，因为你要搞懂输入大小和输出大小的设定是怎么来的</p>
<ul>
<li><p>MNIST共有7万张图片。其中6万张用于训练神经网络，1万张用于测试神经网络。</p>
</li>
<li><p>每张图片是一个28*28像素点的0~9的手写数字图片。</p>
</li>
<li><p>黑底白字。黑底用0表示，白字用0~1之间的浮点数表示，越接近1，颜色越白。</p>
</li>
<li><p>我们把784个像素点组成一个长度为784的一维数组，这个一维数据就是我们要喂入神经网络的输入特征。MNIST数据集还提供了每张图片对应的标签，以一个长度为10的一维数组给出。</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model=nn.Linear(input_size,num_classes)</span><br></pre></td></tr></table></figure>

<h4 id="定义损失函数和优化器"><a href="#定义损失函数和优化器" class="headerlink" title="定义损失函数和优化器"></a>定义损失函数和优化器</h4><ul>
<li>这里我们使用交叉熵来作为损失函数比较好，因为这是一个分类问题，具体原因需要机器学习基础，可能看过，暂时忘了= =！</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)</span><br></pre></td></tr></table></figure>

<h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#因为我们是按照批量进行训练的，之前设置的batch_size=100,总共有60000张，那么就应该需要600个批次</span></span><br><span class="line">total_step=len(train_loader)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i,(images,labels) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        <span class="comment">#将图片重新设定形状大小，（batch_size,input_size）,多了一维是批量数大小</span></span><br><span class="line">        <span class="comment">#当前的images的大小是28*28*batch_size的，所以再将它们重新变一下形状即可</span></span><br><span class="line">        images=images.reshape(<span class="number">-1</span>,<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#前向传播</span></span><br><span class="line">        outputs=model(images)</span><br><span class="line">        loss=criterion(outputs,labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#梯度置0+反向传播+更新参数</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(i+<span class="number">1</span>)%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;'</span>.format(epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, total_step, loss.item()))</span><br></pre></td></tr></table></figure>

<pre><code>Epoch [1/5], Step [100/600], Loss: 2.2148
Epoch [1/5], Step [200/600], Loss: 2.1090
Epoch [1/5], Step [300/600], Loss: 2.0445
Epoch [1/5], Step [400/600], Loss: 1.9044
Epoch [1/5], Step [500/600], Loss: 1.8223
Epoch [1/5], Step [600/600], Loss: 1.7955
Epoch [2/5], Step [100/600], Loss: 1.7728
Epoch [2/5], Step [200/600], Loss: 1.7020
Epoch [2/5], Step [300/600], Loss: 1.6512
Epoch [2/5], Step [400/600], Loss: 1.4894
Epoch [2/5], Step [500/600], Loss: 1.5539
Epoch [2/5], Step [600/600], Loss: 1.4890
Epoch [3/5], Step [100/600], Loss: 1.4254
Epoch [3/5], Step [200/600], Loss: 1.4015
Epoch [3/5], Step [300/600], Loss: 1.4230
Epoch [3/5], Step [400/600], Loss: 1.3121
Epoch [3/5], Step [500/600], Loss: 1.3245
Epoch [3/5], Step [600/600], Loss: 1.3191
Epoch [4/5], Step [100/600], Loss: 1.2653
Epoch [4/5], Step [200/600], Loss: 1.1637
Epoch [4/5], Step [300/600], Loss: 1.2509
Epoch [4/5], Step [400/600], Loss: 1.1195
Epoch [4/5], Step [500/600], Loss: 1.1106
Epoch [4/5], Step [600/600], Loss: 1.1059
Epoch [5/5], Step [100/600], Loss: 1.1150
Epoch [5/5], Step [200/600], Loss: 1.0129
Epoch [5/5], Step [300/600], Loss: 1.0519
Epoch [5/5], Step [400/600], Loss: 1.0661
Epoch [5/5], Step [500/600], Loss: 0.9755
Epoch [5/5], Step [600/600], Loss: 1.0962</code></pre><h4 id="测试模型并保存模型"><a href="#测试模型并保存模型" class="headerlink" title="测试模型并保存模型"></a>测试模型并保存模型</h4><ul>
<li>这里需要注意的是，在测试阶段，我们不需要再次计算梯度了（为了提高内存的效率= =！）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试模型</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    correct=<span class="number">0</span></span><br><span class="line">    total=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images,labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        <span class="comment">#print(labels.size(0)),其值就是100，也就是我们设置的批量大小</span></span><br><span class="line">        images=images.reshape(<span class="number">-1</span>,<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        outputs=model(images)</span><br><span class="line">        _,predicted=torch.max(outputs.data,<span class="number">1</span>)</span><br><span class="line">        total+=labels.size(<span class="number">0</span>)</span><br><span class="line">        correct+=(predicted==labels).sum()</span><br><span class="line">        </span><br><span class="line">    print(<span class="string">'Accuracy of the model on the 10000 test images: &#123;&#125; %'</span>.format(<span class="number">100</span> * correct / total))</span><br><span class="line">    </span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">torch.save(model.state_dict(),<span class="string">'logistic.ckpt'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy of the model on the 10000 test images: 82 %</code></pre><ul>
<li>没有用CNN，得到的这个效果其实还可以</li>
</ul>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>PyTorch使用的流程：</p>
<ul>
<li>第一步：通常是设置一些超参数（例如输入大小、如果是分类问题：可能有类别数目、训练轮次，批量大小，学习率）</li>
<li>第二步：加载数据集（有的是PyTorch中封装好的，也有是自定义的，具体问题具体分析）<ul>
<li>训练数据集</li>
<li>测试训练集</li>
</ul>
</li>
<li>第三步：定义数据加载器（设计数据输入流水线）</li>
<li>第四步：建立模型，复杂一点的就是创建自定义网络模型</li>
<li>第五步：定义损失函数和优化器<ul>
<li>回归一般用MSE（均方误差）</li>
<li>分类问题一般用CrossEntropyLoss（交叉熵）</li>
</ul>
</li>
<li>第六步：训练模型<ul>
<li>外部循环是训练轮次</li>
<li>内部循环如果有，一般是数据集很大，我们分成了很多批次，每次按照批次大小训练</li>
<li>训练的步骤一般是：如果是批量输入，考虑是否要给数据改变形状-&gt;前向传播-&gt;计算损失-&gt;优化器梯度置0-&gt;反向传播-&gt;更新所有参数</li>
</ul>
</li>
<li>第七步：测试模型（在测试阶段不需要计算梯度，使用<code>with torch.no_grad()</code>）</li>
<li>第八步：保存模型，推荐只保存模型的权重</li>
</ul>
<h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><p>在这个教程里,我们将学习如何使用数据并行(DataParallel)来使用多GPU。</p>
<p>PyTorch非常容易的就可以使用GPU,你可以用如下方式把一个模型放到GPU上:</p>
<p><code>device = torch.device(&quot;cuda:0&quot;)</code></p>
<p><code>model.to(device)</code></p>
<p>然后你可以复制所有的张量到GPU上:</p>
<p><code>mytensor = my_tensor.to(device)</code></p>
<p>请注意,只调用<code>mytensor.gpu()</code>并没有复制张量到GPU上。你需要把它赋值给一个新的张量并在GPU上使用这个张量。</p>
<p>在多GPU上执行前向和反向传播是自然而然的事。然而，<strong>PyTorch默认将只是用一个GPU</strong>。你可以使用DataParallel让模型并行运行来轻易的让你的操作在多个GPU上运行。</p>
<p><code>model = nn.DataParallel(model)</code></p>
<p>这是这篇教程背后的核心，我们接下来将更详细的介绍它</p>
<h4 id="导入PyTorch模块和定义参数"><a href="#导入PyTorch模块和定义参数" class="headerlink" title="导入PyTorch模块和定义参数"></a>导入PyTorch模块和定义参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义参数</span></span><br><span class="line">input_size=<span class="number">5</span></span><br><span class="line">output_size=<span class="number">2</span></span><br><span class="line"></span><br><span class="line">batch_size=<span class="number">30</span></span><br><span class="line">data_size=<span class="number">100</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设备</span></span><br><span class="line">device=torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">print(device)</span><br><span class="line">print(torch.cuda.device_count())</span><br><span class="line">print(torch.cuda.current_device())</span><br><span class="line">print(torch.cuda.get_device_name(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<pre><code>cuda:0
1
0
GeForce GTX 950M</code></pre><h4 id="虚拟数据集"><a href="#虚拟数据集" class="headerlink" title="虚拟数据集"></a>虚拟数据集</h4><p>制作一个虚拟（随机）数据集，你只需实现<strong>getitem</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,size,length)</span>:</span></span><br><span class="line">        self.len=length</span><br><span class="line">        <span class="comment">#randn返回一个张量，从标准正态分布（均值为0，方差为1）中抽取的一组随机数。</span></span><br><span class="line">        <span class="comment">#length--整数序列，定义了输出张量的形状</span></span><br><span class="line">        <span class="comment">#size--输出张量的形状</span></span><br><span class="line">        self.data=torch.randn(length,size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self,index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.len</span><br><span class="line">    </span><br><span class="line">rand_loader=DataLoader(dataset=RandomDataset(input_size,data_size),batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="简单的模型"><a href="#简单的模型" class="headerlink" title="简单的模型"></a>简单的模型</h4><p>作为演示，我们的模型只接受一个输入，执行一个线性操作，然后得到结果。然而，你能在任何模型（CNN，RNN，Capsule Net等）上使用DataParallel。</p>
<p>我们在模型内部放置了一条打印语句来检测输入和输出向量的大小。请注意批等级为0时打印的内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size,output_size)</span>:</span></span><br><span class="line">        <span class="comment">#这句话官方就是这么写，规定好了,继承Module的初始化函数</span></span><br><span class="line">        super(Model,self).__init__()</span><br><span class="line">        self.fc=nn.Linear(input_size,output_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input)</span>:</span></span><br><span class="line">        output=self.fc(input)</span><br><span class="line">        print(<span class="string">"\tIn Model: input size"</span>, input.size(),<span class="string">"output size"</span>, output.size())</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<h4 id="创建一个模型和数据并行"><a href="#创建一个模型和数据并行" class="headerlink" title="创建一个模型和数据并行"></a>创建一个模型和数据并行</h4><p>这是本教程的核心部分。首先，我们需要创建一个模型实例和检测我们是否有多个GPU。如果我们有多个GPU，我们使用nn.DataParallel来包装我们的模型。然后通过model.to(device)把模型放到GPU上。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model=Model(input_size,output_size)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count()&gt;<span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"let's use"</span>,torch.cuda.device_count(),<span class="string">"GPUs!"</span>)</span><br><span class="line">    </span><br><span class="line">    model=nn.DataParallel(model)</span><br><span class="line">    </span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>




<pre><code>Model(
  (fc): Linear(in_features=5, out_features=2, bias=True)
)</code></pre><h4 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h4><p>现在我们可以看看输入和输出张量的大小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input=data.to(device)</span><br><span class="line">    output=model(input)</span><br><span class="line">    print(<span class="string">"outside: input size"</span>,input_size,<span class="string">"output size"</span>,output_size)</span><br></pre></td></tr></table></figure>

<pre><code>    In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])
outside: input size 5 output size 2
    In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])
outside: input size 5 output size 2
    In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])
outside: input size 5 output size 2
    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])
outside: input size 5 output size 2</code></pre><h4 id="但是，我只有一个GPU-！"><a href="#但是，我只有一个GPU-！" class="headerlink" title="但是，我只有一个GPU = =！"></a>但是，我只有一个GPU = =！</h4>]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Python之进程和线程_2</title>
    <url>/ck3bmvcxh002ybsg46e5o99cl.html</url>
    <content><![CDATA[<h3 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h3><p>多任务可以由多进程完成，也可以由一个进程内的多线程完成。</p>
<p>我们前面提到了进程是由若干线程组成的，一个进程至少有一个线程。</p>
<p>由于线程是操作系统直接支持的执行单元，因此，高级语言通常都内置多线程的支持，Python也不例外，并且，Python的线程是真正的Posix Thread，而不是模拟出来的线程。</p>
<a id="more"></a>

<p>Python的标准库提供了两个模块：<code>_thread</code>和<code>threading</code>，<code>_thread</code>是低级模块，<code>threading</code>是高级模块，对<code>_thread</code>进行了封装。绝大多数情况下，我们只需要使用<code>threading</code>这个高级模块。</p>
<p>启动一个线程就是把一个函数传入并创建Thread实例，然后调用start()开始执行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time,threading</span><br><span class="line"></span><br><span class="line"><span class="comment">#新线程的执行代码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">'thread %s is running ...'</span>%threading.current_thread().name)</span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> n &lt; <span class="number">5</span> :</span><br><span class="line">        n = n + <span class="number">1</span></span><br><span class="line">        print(<span class="string">'thread %s &gt;&gt;&gt; %s'</span>%(threading.current_thread().name,n))</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">    print(<span class="string">'thread %s is ended.'</span>%threading.current_thread().name)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'thread %s is running ...'</span>%threading.current_thread().name)</span><br><span class="line">t=threading.Thread(target=loop,name=<span class="string">'LoopThread'</span>)</span><br><span class="line">t.start()</span><br><span class="line">t.join()</span><br><span class="line">print(<span class="string">'thread %s ended.'</span>%threading.current_thread().name)</span><br><span class="line">thread MainThread <span class="keyword">is</span> running ...</span><br><span class="line">thread LoopThread <span class="keyword">is</span> running ...</span><br><span class="line">thread LoopThread &gt;&gt;&gt; <span class="number">1</span></span><br><span class="line">thread LoopThread &gt;&gt;&gt; <span class="number">2</span></span><br><span class="line">thread LoopThread &gt;&gt;&gt; <span class="number">3</span></span><br><span class="line">thread LoopThread &gt;&gt;&gt; <span class="number">4</span></span><br><span class="line">thread LoopThread &gt;&gt;&gt; <span class="number">5</span></span><br><span class="line">thread LoopThread <span class="keyword">is</span> ended.</span><br><span class="line">thread MainThread ended.</span><br></pre></td></tr></table></figure>

<p>由于任何进程默认就会启动一个线程，我们把该线程称为主线程，主线程又可以启动新的线程，Python的threading模块有个<code>current_thread()</code>函数，它永远返回当前线程的实例。主线程实例的名字叫MainThread，子线程的名字在创建时指定，我们用LoopThread命名子线程。名字仅仅在打印时用来显示，完全没有其他意义，如果不起名字Python就自动给线程命名为Thread-1，Thread-2……</p>
<h4 id="Lock"><a href="#Lock" class="headerlink" title="Lock"></a>Lock</h4><p>多线程和多进程最大的不同在于，多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响，而多线程中，<strong>所有变量都由所有线程共享</strong> ，所以，任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。</p>
<p>来看看多个线程同时操作一个变量怎么把内容给改乱了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time,threading</span><br><span class="line"></span><br><span class="line"><span class="comment">#假定这是你的银行存款,balance有结余的意思（get了）</span></span><br><span class="line">balance=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_balance</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="comment">#先存款，后取款，结果应为0，在这里的global关键字，定义的一个全局变量，对应于函数外的balance</span></span><br><span class="line">    <span class="keyword">global</span> balance</span><br><span class="line">    balance = balance + n</span><br><span class="line">    balance = balance - n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_thread</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000000</span>):</span><br><span class="line">        change_balance(n)</span><br><span class="line"></span><br><span class="line">t1=threading.Thread(target=run_thread,name=<span class="string">'t1'</span>,args=(<span class="number">5</span>,))</span><br><span class="line">t2=threading.Thread(target=run_thread,name=<span class="string">'t2'</span>,args=(<span class="number">8</span>,))</span><br><span class="line">t1.start()</span><br><span class="line">t2.start()</span><br><span class="line">t1.join()</span><br><span class="line">t2.join()</span><br><span class="line">print(balance)</span><br><span class="line"><span class="number">90</span></span><br></pre></td></tr></table></figure>

<p>我们定义了一个共享变量balance，初始值为0，并且启动两个线程，先存后取，理论上结果应该为0，但是，由于<strong>线程的调度是由操作系统决定的</strong>，当t1、t2交替执行时，只要<strong>循环次数足够多</strong> ，balance的结果就不一定是0了。</p>
<p>原因是因为高级语言的一条语句在CPU执行时是若干条语句，即使一个简单的计算： <code>balance = balance + n</code> 也分为两步：</p>
<ul>
<li>计算balance + n，存入临时变量中</li>
<li>将临时变量的值赋给balance 也就是可以看成：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x = balance + n</span><br><span class="line">balance = x</span><br></pre></td></tr></table></figure>

<p>由于x是局部变量，两个线程各自都有自己的x，当代码正常执行时：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">初始值 balance = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">t1: x1 = balance + <span class="number">5</span> <span class="comment"># x1 = 0 + 5 = 5</span></span><br><span class="line">t1: balance = x1     <span class="comment"># balance = 5</span></span><br><span class="line">t1: x1 = balance - <span class="number">5</span> <span class="comment"># x1 = 5 - 5 = 0</span></span><br><span class="line">t1: balance = x1     <span class="comment"># balance = 0</span></span><br><span class="line"></span><br><span class="line">t2: x2 = balance + <span class="number">8</span> <span class="comment"># x2 = 0 + 8 = 8</span></span><br><span class="line">t2: balance = x2     <span class="comment"># balance = 8</span></span><br><span class="line">t2: x2 = balance - <span class="number">8</span> <span class="comment"># x2 = 8 - 8 = 0</span></span><br><span class="line">t2: balance = x2     <span class="comment"># balance = 0</span></span><br><span class="line">    </span><br><span class="line">结果 balance = <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>但是t1和t2是交替运行的，如果操作系统是以下面的顺序执行的话：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">初始值 balance = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">t1: x1 = balance + <span class="number">5</span>  <span class="comment"># x1 = 0 + 5 = 5</span></span><br><span class="line"></span><br><span class="line">t2: x2 = balance + <span class="number">8</span>  <span class="comment"># x2 = 0 + 8 = 8</span></span><br><span class="line">t2: balance = x2      <span class="comment"># balance = 8</span></span><br><span class="line"></span><br><span class="line">t1: balance = x1      <span class="comment"># balance = 5</span></span><br><span class="line">t1: x1 = balance - <span class="number">5</span>  <span class="comment"># x1 = 5 - 5 = 0</span></span><br><span class="line">t1: balance = x1      <span class="comment"># balance = 0</span></span><br><span class="line"></span><br><span class="line">t2: x2 = balance - <span class="number">8</span>  <span class="comment"># x2 = 0 - 8 = -8</span></span><br><span class="line">t2: balance = x2   <span class="comment"># balance = -8</span></span><br><span class="line"></span><br><span class="line">结果 balance = <span class="number">-8</span></span><br></pre></td></tr></table></figure>

<p>究其原因，是因为修改balance需要多条语句，而<strong>执行这几条语句时，线程可能中断</strong>，从而导致多个线程把同一个对象的内容改乱了。</p>
<p>两个线程同时一存一取，就可能导致余额不对，你肯定不希望你的银行存款莫名其妙地变成了负数，所以，我们必须确保一个线程在修改balance的时候，别的线程一定不能改。</p>
<p>如果我们要确保balance计算正确，就要给<code>change_balance()</code>上一把锁，当某个线程开始执行<code>change_balance()</code>时，我们说，该线程因为获得了锁，因此其他线程不能同时执行<code>change_balance()</code>，只能等待，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少线程，同一时刻最多只有一个线程持有该锁(就是OS相关的互斥)，所以，不会造成修改的冲突。创建一个锁就是通过<code>threading.Lock()</code>来实现：</p>
<h5 id="增加锁后的代码，运行时间稍微有一点点"><a href="#增加锁后的代码，运行时间稍微有一点点" class="headerlink" title="增加锁后的代码，运行时间稍微有一点点"></a>增加锁后的代码，运行时间稍微有一点点</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time,threading</span><br><span class="line"></span><br><span class="line"><span class="comment">#假定这是你的银行存款,balance有结余的意思（get了）</span></span><br><span class="line">balance=<span class="number">0</span></span><br><span class="line"><span class="comment">#创建一个锁</span></span><br><span class="line">lock=threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">change_balance</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="comment">#先存款，后取款，结果应为0，在这里的global关键字，定义的一个全局变量，对应于函数外的balance</span></span><br><span class="line">    <span class="keyword">global</span> balance</span><br><span class="line">    balance = balance + n</span><br><span class="line">    balance = balance - n</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_thread</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000000</span>):</span><br><span class="line">        <span class="comment">#先要获取锁</span></span><br><span class="line">        lock.acquire()</span><br><span class="line">        <span class="comment">#try/finally语句不管有没有异常，都会执行finally语句</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment">#放心的修改</span></span><br><span class="line">            change_balance(n)</span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">            <span class="comment">#改完了一次就释放锁</span></span><br><span class="line">            lock.release()</span><br><span class="line"></span><br><span class="line">t1=threading.Thread(target=run_thread,name=<span class="string">'t1'</span>,args=(<span class="number">5</span>,))</span><br><span class="line">t2=threading.Thread(target=run_thread,name=<span class="string">'t2'</span>,args=(<span class="number">8</span>,))</span><br><span class="line">t1.start()</span><br><span class="line">t2.start()</span><br><span class="line">t1.join()</span><br><span class="line">t2.join()</span><br><span class="line">print(balance)</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>当多个线程同时执行lock.acquire()时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。</p>
<p>获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程（饿死）。所以我们用try…finally来确保锁一定会被释放。</p>
<p>锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，</p>
<ul>
<li>首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。</li>
<li>其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。</li>
</ul>
<h4 id="多核CPU"><a href="#多核CPU" class="headerlink" title="多核CPU"></a>多核CPU</h4><p>如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。</p>
<p>如果写一个死循环的话，会出现什么情况呢？</p>
<p>打开Mac OS X的Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。</p>
<p>我们可以监控到一个死循环线程会100%占用一个CPU。</p>
<p>如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。</p>
<p>要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。</p>
<p>试试用Python写个死循环：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading,multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span><span class="params">()</span>:</span></span><br><span class="line">    x=<span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        x=x^<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(multiprocessing.cpu_count()):</span><br><span class="line">    t=threading.Thread(target=loop)</span><br><span class="line">    t.start()</span><br></pre></td></tr></table></figure>

<p>启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。</p>
<p>但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？</p>
<p>因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个<strong>GIL锁</strong>：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行<strong>100条字节码</strong>，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。</p>
<p>GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。</p>
<p>所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。</p>
<p>不过，也不用过于担心，Python虽然<strong>不能利用多线程实现多核任务，但可以通过多进程实现多核任务</strong>。多个Python进程有各自独立的GIL锁，互不影响。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span><span class="params">()</span>:</span></span><br><span class="line">    x=<span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        x=x^<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(multiprocessing.cpu_count()):</span><br><span class="line">    p=multiprocessing.Process(target=loop)</span><br><span class="line">    p.start()</span><br></pre></td></tr></table></figure>

<p>但是查看Task manager 并没有发现CPU利用率很高= =</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>多线程编程，模型复杂，容易发生冲突，必须用锁加以隔离，同时，又要小心死锁的发生。</p>
<p>Python解释器由于设计时有GIL全局锁，导致了多线程无法利用多核。多线程的并发在Python中就是一个美丽的梦。</p>
<h3 id="ThreadLocal"><a href="#ThreadLocal" class="headerlink" title="ThreadLocal"></a>ThreadLocal</h3><p>在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。</p>
<p>但是局部变量也有问题，就是在函数调用的时候，传递起来很麻烦：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_student</span><span class="params">(name)</span>:</span></span><br><span class="line">    std = Student(name)</span><br><span class="line">    <span class="comment"># std是局部变量，但是每个函数都要用它，因此必须传进去：</span></span><br><span class="line">    do_task_1(std)</span><br><span class="line">    do_task_2(std)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_task_1</span><span class="params">(std)</span>:</span></span><br><span class="line">    do_subtask_1(std)</span><br><span class="line">    do_subtask_2(std)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_task_2</span><span class="params">(std)</span>:</span></span><br><span class="line">    do_subtask_2(std)</span><br><span class="line">    do_subtask_2(std)</span><br></pre></td></tr></table></figure>

<p>每个函数一层一层调用都这么传参数那还得了？用全局变量？也不行，因为每个线程处理不同的Student对象，不能共享。</p>
<p>如果用一个全局<code>dict</code>存放所有的Student对象，然后以<code>thread</code>自身作为<code>key</code>获得线程对应的Student对象如何？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">global_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">std_thread</span><span class="params">(name)</span>:</span></span><br><span class="line">    std = Student(name)</span><br><span class="line">    <span class="comment"># 把std放到全局变量global_dict中：</span></span><br><span class="line">    global_dict[threading.current_thread()] = std</span><br><span class="line">    do_task_1()</span><br><span class="line">    do_task_2()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_task_1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 不传入std，而是根据当前线程查找：</span></span><br><span class="line">    std = global_dict[threading.current_thread()]</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_task_2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 任何函数都可以查找出当前线程的std变量：</span></span><br><span class="line">    std = global_dict[threading.current_thread()]</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<p>这种方式理论上是可行的，它最大的优点是消除了std对象在每层函数中的传递问题，但是，每个函数获取std的代码有点丑。</p>
<p>有没有更简单的方式？</p>
<p>ThreadLocal应运而生，不用查找dict，ThreadLocal帮你自动做这件事：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建全局ThreadLocal对象</span></span><br><span class="line">local_school=threading.local()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_thread</span><span class="params">(name)</span>:</span></span><br><span class="line">    <span class="comment">#绑定ThreadLocal的student</span></span><br><span class="line">    local_school.student=name</span><br><span class="line">    process_student()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_student</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#获取当前线程关联的student</span></span><br><span class="line">    std=local_school.student</span><br><span class="line">    print(<span class="string">'Hello , %s (in %s)\n'</span>%(std,threading.current_thread().name))</span><br><span class="line"></span><br><span class="line">t1 = threading.Thread(target=process_thread,args=(<span class="string">'lemon'</span>,),name=<span class="string">'thread-lemon'</span>)</span><br><span class="line">t2 = threading.Thread(target=process_thread,args=(<span class="string">'leocode'</span>,),name=<span class="string">'thread-leocode'</span>)</span><br><span class="line">t1.start()</span><br><span class="line">t2.start()</span><br><span class="line">t1.join()</span><br><span class="line">t2.join()</span><br><span class="line">print(<span class="string">'end!'</span>)</span><br><span class="line">Hello , lemon (<span class="keyword">in</span> thread-lemon)</span><br><span class="line"></span><br><span class="line">Hello , leocode (<span class="keyword">in</span> thread-leocode)</span><br><span class="line"></span><br><span class="line">end!</span><br></pre></td></tr></table></figure>

<p>全局变量<code>local_school</code>就是一个<code>ThreadLocal</code>对象，每个Thread对它都可以读写student属性，但互不影响。你可以把<code>local_school</code>看成全局变量，但每个属性如<code>local_school.student</code>都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。</p>
<p>可以理解为全局变量<code>local_school</code>是一个dict，不但可以用<code>local_school.student</code>，还可以绑定其他变量，如<code>local_school.teacher</code>等等。</p>
<p>ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。</p>
<h4 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h4><p>一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题。</p>
<h3 id="进程VS线程"><a href="#进程VS线程" class="headerlink" title="进程VS线程"></a>进程VS线程</h3><p>我们介绍了多进程和多线程，这是实现多任务最常用的两种方式。现在，我们来讨论一下这两种方式的优缺点。</p>
<p>首先，要实现多任务，通常我们会设计Master-Worker模式，Master负责分配任务，Worker负责执行任务，因此，多任务环境下，通常是一个Master，多个Worker。</p>
<h4 id="简单介绍一下Master-Worker模式："><a href="#简单介绍一下Master-Worker模式：" class="headerlink" title="简单介绍一下Master-Worker模式："></a>简单介绍一下Master-Worker模式：</h4><ul>
<li>Master-Worker模式是常用的并行设计模式。它的核心思想是，系统有两个进程协议工作：Master进程和Worker进程。Master进程负责接收和分配任务，Worker进程负责处理子任务。当各个Worker进程将子任务处理完后，将结果返回给Master进程，由Master进行归纳和汇总，从而得到系统结果。</li>
<li><img src="/ck3bmvcxh002ybsg46e5o99cl/1.jpg" class=""></li>
<li>Master-Worker模式的好处是，它能将大任务分解成若干个小任务，并发执行，从而提高系统性能。而对于系统请求者Client来说，任务一旦提交，Master进程就会立刻分配任务并立即返回，并不会等系统处理完全部任务再返回，其处理过程是异步的。</li>
</ul>
<h4 id="Master-Worker模式结构"><a href="#Master-Worker模式结构" class="headerlink" title="Master-Worker模式结构"></a>Master-Worker模式结构</h4><ul>
<li><img src="/ck3bmvcxh002ybsg46e5o99cl/2.jpg" class=""></li>
<li>如上图所示，Master进程是主要进程，它维护着一个Worker进程队列、子任务队列和子结果集，Worker进程中的Worker进程不断的从任务队列中提取要处理的子任务，并将子任务的处理结果放入到子结果集中。</li>
<li>在上图中，Master：用于任务的分配和最终结果的合并；Worker：用于实际处理一个任务；客户端进程：用于启动系统，调度开启Master。</li>
</ul>
<p>如果用多进程实现Master-Worker，主进程就是Master，其他进程就是Worker。</p>
<p>如果用多线程实现Master-Worker，主线程就是Master，其他线程就是Worker。</p>
<p>多进程模式最大的优点就是稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。（当然主进程挂了所有进程就全挂了，但是Master进程只负责分配任务，挂掉的概率低）著名的Apache最早就是采用多进程模式。</p>
<p>多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用fork调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题。</p>
<p>多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。在Windows上，如果一个线程执行的代码出了问题，你经常可以看到这样的提示：“该程序执行了非法操作，即将关闭”，其实往往是某个线程出了问题，但是操作系统会强制结束整个进程。</p>
<p>在Windows下，多线程的效率比多进程要高，所以微软的IIS服务器默认采用多线程模式。由于多线程存在稳定性的问题，IIS的稳定性就不如Apache。为了缓解这个问题，IIS和Apache现在又有多进程+多线程的混合模式，真是把问题越搞越复杂(= =)。</p>
<h4 id="线程切换"><a href="#线程切换" class="headerlink" title="线程切换"></a>线程切换</h4><p>无论是多进程还是多线程，只要数量一多，效率肯定上不去，为什么呢？</p>
<p>我们打个比方，假设你不幸正在准备中考，每天晚上需要做语文、数学、英语、物理、化学这5科的作业，每项作业耗时1小时。</p>
<p>如果你先花1小时做语文作业，做完了，再花1小时做数学作业，这样，依次全部做完，一共花5小时，这种方式称为单任务模型，或者批处理任务模型。</p>
<p>假设你打算切换到多任务模型，可以先做1分钟语文，再切换到数学作业，做1分钟，再切换到英语，以此类推，只要切换速度足够快，这种方式就和单核CPU执行多任务是一样的了，以幼儿园小朋友的眼光来看，你就正在同时写5科作业。</p>
<p>但是，切换作业是有代价的，比如从语文切到数学，要先收拾桌子上的语文书本、钢笔（这叫<strong>保存现场</strong>），然后，打开数学课本、找出圆规直尺（这叫<strong>准备新环境</strong>），才能开始做数学作业。操作系统在切换进程或者线程时也是一样的，它需要先保存当前执行的现场环境（CPU寄存器状态、内存页等），然后，把新任务的执行环境准备好（恢复上次的寄存器状态，切换内存页等），才能开始执行。这个切换过程虽然很快，但是也需要耗费时间。如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于<strong>假死状态</strong>。</p>
<p>所以，多任务一旦多到一个限度，就会消耗掉系统所有的资源，结果效率急剧下降，所有任务都做不好。</p>
<h4 id="计算密集型-vs-IO密集型"><a href="#计算密集型-vs-IO密集型" class="headerlink" title="计算密集型 vs. IO密集型"></a>计算密集型 vs. IO密集型</h4><p>是否采用多任务的第二个考虑是任务的类型。我们可以把任务分为计算密集型和IO密集型。</p>
<p>计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。</p>
<p>计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。<strong>对于计算密集型任务，最好用C语言编写</strong>。</p>
<p>第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。</p>
<p>IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。<strong>对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差</strong>。</p>
<h4 id="异步IO"><a href="#异步IO" class="headerlink" title="异步IO"></a>异步IO</h4><p>考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，我们才需要多进程模型或者多线程模型来支持多任务并发执行。</p>
<p>现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为事件驱动模型，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用<strong>异步IO编程模型来实现多任务是一个主要的趋势</strong>。</p>
<p>对应到Python语言，<strong>单线程的异步编程模型称为协程</strong>，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。我们会在后面讨论如何编写协程。</p>
<h4 id="分布式进程"><a href="#分布式进程" class="headerlink" title="分布式进程"></a>分布式进程</h4><p>在Thread和Process中，应当优选Process，因为Process更稳定，而且，<strong>Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上</strong>。</p>
<p>Python的<code>multiprocessing</code>模块不但支持多进程，其中<code>managers</code>子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于<code>managers</code>模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。</p>
<p>举个例子：如果我们已经有一个通过Queue通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。怎么用分布式进程实现？</p>
<p>原有的Queue可以继续使用，但是，通过<code>managers</code>模块<strong>把Queue通过网络暴露出去，就可以让其他机器的进程访问Queue了</strong>。</p>
<p>我们先看服务进程，服务进程负责启动Queue，把Queue注册到网络上，然后往Queue里面写入任务：</p>
<ul>
<li>task_master.py，相当于分布式的主进程</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random,time,queue</span><br><span class="line"><span class="keyword">from</span> multiprocessing.managers <span class="keyword">import</span> BaseManager</span><br><span class="line"></span><br><span class="line"><span class="comment">#发送任务的队列：</span></span><br><span class="line">task_queue=queue.Queue()</span><br><span class="line"><span class="comment">#接收结果的队列</span></span><br><span class="line">result_queue=queue.Queue()</span><br><span class="line"></span><br><span class="line"><span class="comment">#从BaseManager继承的QueueManager</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QueueManager</span><span class="params">(BaseManager)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#把两个Queue都注册到网络上，callale参数关联了Queue对象：</span></span><br><span class="line"><span class="comment">#lambda是匿名函数，可以理解为一次函数的执行过程</span></span><br><span class="line">QueueManager.register(<span class="string">'get_task_queue'</span>,callable=<span class="keyword">lambda</span> : task_queue)</span><br><span class="line">QueueManager.register(<span class="string">'get_result_queue'</span>,callable=<span class="keyword">lambda</span> : result_queue)</span><br><span class="line"><span class="comment">#绑定端口5000，设置验证码为'abc'</span></span><br><span class="line">manager=QueueManager(address=(<span class="string">''</span>,<span class="number">5000</span>),authkey=<span class="string">b'abc'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#启动Queue</span></span><br><span class="line">manager.start()</span><br><span class="line"><span class="comment">#获得通过网络访问的Queue对象</span></span><br><span class="line">task=manager.get_task_queue()</span><br><span class="line">result=manager.get_result_queue()</span><br><span class="line"></span><br><span class="line"><span class="comment">#放几个任务进去</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    n=random.randint(<span class="number">0</span>,<span class="number">10000</span>)</span><br><span class="line">    print(<span class="string">'put task %s'</span>%n)</span><br><span class="line">    task.put(n)</span><br><span class="line"></span><br><span class="line"><span class="comment">#从result队列读取结果</span></span><br><span class="line">print(<span class="string">'Try get results'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    r=result.get(timeout=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">'result : %s'</span>%r)</span><br><span class="line"></span><br><span class="line"><span class="comment">#关闭</span></span><br><span class="line">manager.shutdown()</span><br><span class="line">print(<span class="string">'master.exit.'</span>)</span><br></pre></td></tr></table></figure>

<p>请注意，当我们在一台机器上写多进程程序时，创建的Queue可以直接拿来用，但是，在分布式多进程环境下，添加任务到Queue不可以直接对原始的task_queue进行操作，那样就绕过了QueueManager的封装，必须通过manager.get_task_queue()获得的Queue接口添加。</p>
<p>然后，在另一台机器上启动任务进程（本机上启动也可以）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time,sys,queue</span><br><span class="line"><span class="keyword">from</span> multiprocessing.managers <span class="keyword">import</span> BaseManager</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建类似的QueueManager</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QueueManager</span><span class="params">(BaseManager)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#由于这个QueueManager只能从网络上获取Queue，所以注册时只提供名字</span></span><br><span class="line">QueueManager.register(<span class="string">'get_task_queue'</span>)</span><br><span class="line">QueueManager.register(<span class="string">'get_result_queue'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#连接到服务器，也就是运行task_master.py的机器</span></span><br><span class="line">server_addr=<span class="string">'127.0.0.1'</span></span><br><span class="line">print(<span class="string">'Connect to server %s...'</span>%server_addr)</span><br><span class="line"></span><br><span class="line"><span class="comment">#端口和验证码注意保持和task_master.py这只的完全一致</span></span><br><span class="line">m=QueueManager(address=(server_addr,<span class="number">5000</span>),authkey=<span class="string">b'abc'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#从网络连接</span></span><br><span class="line">m.connect()</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取Queue的对象</span></span><br><span class="line">task=m.get_task_queue()</span><br><span class="line">result=m.get_result_queue()</span><br><span class="line"></span><br><span class="line"><span class="comment">#从task任务队列取任务，并把结果写入到result队列</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        n=task.get(timeout=<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">'run task %d * %d...'</span>%(n,n))</span><br><span class="line">        r=<span class="string">'%d * %d = %d'</span>%(n,n,n*n)</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line">        result.put(r)</span><br><span class="line">    <span class="keyword">except</span> Queue.Empty:</span><br><span class="line">        print(<span class="string">'task queue is empty.'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#处理结束</span></span><br><span class="line">print(<span class="string">'worker exit.'</span>)</span><br></pre></td></tr></table></figure>

<p>任务进程要通过网络连接到服务进程，所以要指定服务进程的IP。</p>
<p>现在，可以试试分布式进程的工作效果了。先启动<code>task_master.py</code>服务进程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">liuchuang@ubuntu<span class="number">-4</span>:~/python$ python3 task_master.py</span><br><span class="line">put task <span class="number">4270</span></span><br><span class="line">put task <span class="number">7611</span></span><br><span class="line">put task <span class="number">6983</span></span><br><span class="line">put task <span class="number">2863</span></span><br><span class="line">put task <span class="number">9611</span></span><br><span class="line">put task <span class="number">2283</span></span><br><span class="line">put task <span class="number">3883</span></span><br><span class="line">put task <span class="number">9928</span></span><br><span class="line">put task <span class="number">5946</span></span><br><span class="line">put task <span class="number">7511</span></span><br><span class="line">Try get results</span><br></pre></td></tr></table></figure>

<p><code>task_master.py</code>进程发送完任务后，开始等待result队列的结果。现在启动<code>task_worker.py</code>进程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">liuchuang@ubuntu<span class="number">-4</span>:~/python$ python3 task_worker.py</span><br><span class="line">Connect to server <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>...</span><br><span class="line">run task <span class="number">4270</span> * <span class="number">4270.</span>..</span><br><span class="line">run task <span class="number">7611</span> * <span class="number">7611.</span>..</span><br><span class="line">run task <span class="number">6983</span> * <span class="number">6983.</span>..</span><br><span class="line">run task <span class="number">2863</span> * <span class="number">2863.</span>..</span><br><span class="line">run task <span class="number">9611</span> * <span class="number">9611.</span>..</span><br><span class="line">run task <span class="number">2283</span> * <span class="number">2283.</span>..</span><br><span class="line">run task <span class="number">3883</span> * <span class="number">3883.</span>..</span><br><span class="line">run task <span class="number">9928</span> * <span class="number">9928.</span>..</span><br><span class="line">run task <span class="number">5946</span> * <span class="number">5946.</span>..</span><br><span class="line">run task <span class="number">7511</span> * <span class="number">7511.</span>..</span><br><span class="line">worker exit.</span><br></pre></td></tr></table></figure>

<p><code>task_worker.py</code>进程结束，在<code>task_master.py</code>进程中会继续打印出结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">liuchuang@ubuntu<span class="number">-4</span>:~/python$ python3 task_master.py</span><br><span class="line">put task <span class="number">4270</span></span><br><span class="line">put task <span class="number">7611</span></span><br><span class="line">put task <span class="number">6983</span></span><br><span class="line">put task <span class="number">2863</span></span><br><span class="line">put task <span class="number">9611</span></span><br><span class="line">put task <span class="number">2283</span></span><br><span class="line">put task <span class="number">3883</span></span><br><span class="line">put task <span class="number">9928</span></span><br><span class="line">put task <span class="number">5946</span></span><br><span class="line">put task <span class="number">7511</span></span><br><span class="line">Try get results</span><br><span class="line">result : <span class="number">4270</span> * <span class="number">4270</span> = <span class="number">18232900</span></span><br><span class="line">result : <span class="number">7611</span> * <span class="number">7611</span> = <span class="number">57927321</span></span><br><span class="line">result : <span class="number">6983</span> * <span class="number">6983</span> = <span class="number">48762289</span></span><br><span class="line">result : <span class="number">2863</span> * <span class="number">2863</span> = <span class="number">8196769</span></span><br><span class="line">result : <span class="number">9611</span> * <span class="number">9611</span> = <span class="number">92371321</span></span><br><span class="line">result : <span class="number">2283</span> * <span class="number">2283</span> = <span class="number">5212089</span></span><br><span class="line">result : <span class="number">3883</span> * <span class="number">3883</span> = <span class="number">15077689</span></span><br><span class="line">result : <span class="number">9928</span> * <span class="number">9928</span> = <span class="number">98565184</span></span><br><span class="line">result : <span class="number">5946</span> * <span class="number">5946</span> = <span class="number">35354916</span></span><br><span class="line">result : <span class="number">7511</span> * <span class="number">7511</span> = <span class="number">56415121</span></span><br><span class="line">master.exit.</span><br></pre></td></tr></table></figure>

<p>这个简单的Master/Worker模型有什么用？其实这就是一个简单但真正的分布式计算，把代码稍加改造，启动多个worker，就可以把任务分布到几台甚至几十台机器上，比如把计算<code>n*n</code>的代码换成发送邮件，就实现了邮件队列的异步发送。</p>
<p>Queue对象存储在哪？注意到task_worker.py中根本没有创建Queue的代码，所以，Queue对象存储在task_master.py进程中： <img src="/ck3bmvcxh002ybsg46e5o99cl/3.jpg" class="">而Queue之所以能通过网络访问，就是通过QueueManager实现的。由于QueueManager管理的不止一个Queue，所以，要给每个Queue的网络调用接口起个名字，比如get_task_queue。</p>
<p>authkey有什么用？这是为了保证两台机器正常通信，不被其他机器恶意干扰。如果task_worker.py的authkey和task_master.py的authkey不一致，肯定连接不上。</p>
<h4 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h4><p>Python的分布式进程接口简单，封装良好，适合需要把繁重任务分布到多台机器的环境下。</p>
<p>注意Queue的作用是用来传递任务和接收结果，每个任务的描述数据量要尽量小。比如发送一个处理日志文件的任务，就不要发送几百兆的日志文件本身，而是发送日志文件存放的完整路径，由Worker进程再去共享的磁盘上读取文件。</p>
]]></content>
      <categories>
        <category>Python</category>
        <category>OS</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>OS</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch_Day3</title>
    <url>/ck3bmvcx2002ibsg4gnoabowp.html</url>
    <content><![CDATA[<h2 id="训练一个分类器"><a href="#训练一个分类器" class="headerlink" title="训练一个分类器"></a>训练一个分类器</h2><p>在了解了如何定义一个神经网络、计算损失值和更新网络的权重之后，那么数据从哪里来呢？</p>
<p>关于数据<br>通常，当你处理图像，文本，音频和视频数据时，你可以使用标准的Python包来加载数据到一个numpy数组中.然后把这个数组转换成torch.Tensor。</p>
<a id="more"></a>

<ul>
<li>对于图像,有诸如Pillow,OpenCV包等非常实用</li>
<li>对于音频,有诸如scipy和librosa包</li>
<li>对于文本,可以用原始Python和Cython来加载,或者使用NLTK和SpaCy </li>
<li>对于视觉,我们创建了一个torchvision包,包含常见数据集的数据加载,比如Imagenet,CIFAR10,MNIST等,和图像转换器,也就是torchvision.datasets和torch.utils.data.DataLoader。</li>
</ul>
<p>在之前，我用keras训练了Cifar10训练集，现在尝试使用PyTorch来训练一个分类器。</p>
<ul>
<li>cifar10：它有如下10个类别:’airplane’,’automobile’,’bird’,’cat’,’deer’,’dog’,’frog’,’horse’,’ship’,’truck’。这个数据集中的图像大小为32x32,即,3通道,32x32像素。</li>
</ul>
<h4 id="训练一个图像分类器"><a href="#训练一个图像分类器" class="headerlink" title="训练一个图像分类器"></a>训练一个图像分类器</h4><ul>
<li>使用totchvision加载和归一化Cifar10训练集和测试集</li>
<li>定义一个卷积神经网络</li>
<li>定义损失函数</li>
<li>在训练集上训练网络</li>
<li>在测试及上测试网络</li>
</ul>
<h4 id="加载和归一化cifar10"><a href="#加载和归一化cifar10" class="headerlink" title="加载和归一化cifar10"></a>加载和归一化cifar10</h4><p>使用torchvision加载cifar10是非常容易的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>

<h4 id="torchvision的输出是-0-1-的PILimage（python中的图像处理库PIL）图像，我们将其转换为-1-1-的张量"><a href="#torchvision的输出是-0-1-的PILimage（python中的图像处理库PIL）图像，我们将其转换为-1-1-的张量" class="headerlink" title="torchvision的输出是[0,1]的PILimage（python中的图像处理库PIL）图像，我们将其转换为[-1,1]的张量"></a>torchvision的输出是[0,1]的PILimage（python中的图像处理库PIL）图像，我们将其转换为[-1,1]的张量</h4><p>transforms.Compose</p>
<ul>
<li>将多个transform组合起来使用。</li>
<li>transforms： 由transform构成的列表</li>
<li>channel=（channel-mean）/std(因为transforms.ToTensor()已经把数据处理成[0,1],那么(x-0.5)/0.5就是[-1.0, 1.0])</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform=transforms.Compose(</span><br><span class="line">    <span class="comment">#前者是均值，后者是标准差</span></span><br><span class="line">    [transforms.ToTensor(),transforms.Normalize((<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>),(<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>))]</span><br><span class="line">)</span><br><span class="line">    </span><br><span class="line">trainset=torchvision.datasets.CIFAR10(root=<span class="string">'/data'</span>,train=<span class="literal">True</span>,download=<span class="literal">False</span>,transform=transform)</span><br><span class="line"><span class="comment">#shuffle 随机排序</span></span><br><span class="line">trainloader=torch.utils.data.DataLoader(trainset,batch_size=<span class="number">4</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">testset=torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>,train=<span class="literal">False</span>,download=<span class="literal">False</span>,transform=transform)</span><br><span class="line">testloader=torch.utils.data.DataLoader(testset,batch_size=<span class="number">4</span>,shuffle=<span class="literal">False</span>,num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes=(<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,<span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure>

<p>展示一些有趣的训练图像</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个显示图片的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img=img/<span class="number">2</span>+<span class="number">0.5</span></span><br><span class="line">    npimg=img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line">    plt.show();</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取一些随机的训练图片</span></span><br><span class="line">dataiter=iter(trainloader)</span><br><span class="line">images,labels=dataiter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示图片</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出标签</span></span><br><span class="line">print(<span class="string">' '</span>.join(<span class="string">'%11s'</span>%classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>


<img src="/ck3bmvcx2002ibsg4gnoabowp/1.png" class="">


<pre><code>plane       truck       plane         car</code></pre><h4 id="定义一个卷积神经网络"><a href="#定义一个卷积神经网络" class="headerlink" title="定义一个卷积神经网络"></a>定义一个卷积神经网络</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        <span class="comment">#输入是三通道的图，然后输出的是6通道的图，采用5x5的卷积框</span></span><br><span class="line">        self.conv1=nn.Conv2d(<span class="number">3</span>,<span class="number">6</span>,<span class="number">5</span>)</span><br><span class="line">        <span class="comment">#2x2的最大池化层</span></span><br><span class="line">        self.pool=nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">        <span class="comment">#这里的输入时6通道，因为上边输出的6通道，我们用来接收，最后输出15通道的图，继续采用5x5的卷积框</span></span><br><span class="line">        self.conv2=nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>)</span><br><span class="line">        <span class="comment">#定义三个全连接层，对输入数据做线性变换，最后输出图像是120通道数</span></span><br><span class="line">        self.fc1=nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>,<span class="number">120</span>)</span><br><span class="line">        self.fc2=nn.Linear(<span class="number">120</span>,<span class="number">84</span>)</span><br><span class="line">        self.fc3=nn.Linear(<span class="number">84</span>,<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#上边定义的并没有激活</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        x=self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x=self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x=x.view(<span class="number">-1</span>,<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>)</span><br><span class="line">        x=F.relu(self.fc1(x))</span><br><span class="line">        x=F.relu(self.fc2(x))</span><br><span class="line">        x=self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">net=Net()</span><br></pre></td></tr></table></figure>

<h4 id="定义损失函数和优化器"><a href="#定义损失函数和优化器" class="headerlink" title="定义损失函数和优化器"></a>定义损失函数和优化器</h4><ul>
<li>使用交叉熵作为损失函数</li>
<li>使用带动量的随机梯度下降</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment">#损失函数标准为交叉熵</span></span><br><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#动量因子momentum</span></span><br><span class="line">optimizer=optim.SGD(net.parameters(),lr=<span class="number">0.01</span>,momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<h4 id="训练网络"><a href="#训练网络" class="headerlink" title="训练网络"></a>训练网络</h4><ul>
<li>这时开始有趣的时刻，我们只需要在数据迭代器上循环，把数据输入给网络并优化</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#epoch很熟悉了，这是数据集训练的轮次</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    </span><br><span class="line">    running_loss=<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i,data <span class="keyword">in</span> enumerate(trainloader,<span class="number">0</span>):</span><br><span class="line">        <span class="comment">#获得输入</span></span><br><span class="line">        inputs,labels=data</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#梯度置0</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#forward+backward+optimize</span></span><br><span class="line">        outputs=net(inputs)</span><br><span class="line">        loss=criterion(outputs,labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#打印统计结果</span></span><br><span class="line">        running_loss+=loss.item()</span><br><span class="line">        <span class="comment">#每2000个为一个小批量，进行打印</span></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">2000</span>==<span class="number">1999</span>:</span><br><span class="line">            print(<span class="string">'[%d,%5d] loss:%.3f'</span>%(epoch+<span class="number">1</span>,i+<span class="number">1</span>,running_loss/<span class="number">2000</span>))</span><br><span class="line">            running_loss=<span class="number">0.0</span></span><br><span class="line">            </span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>[1, 2000] loss:2.112
[1, 4000] loss:1.947
[1, 6000] loss:1.926
[1, 8000] loss:1.899
[1,10000] loss:1.930
[1,12000] loss:1.905
[2, 2000] loss:1.936
[2, 4000] loss:1.900
[2, 6000] loss:1.932
[2, 8000] loss:1.962
[2,10000] loss:1.984
[2,12000] loss:1.940
Finished Training</code></pre><h4 id="在GPU上训练"><a href="#在GPU上训练" class="headerlink" title="在GPU上训练"></a>在GPU上训练</h4><p>你是如何把一个Tensor转换GPU上,你就如何把一个神经网络移动到GPU上训练。这个操作会递归遍历有所模块,并将其参数和缓冲区转换为CUDA张量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>

<pre><code>cuda:0</code></pre><p>接下来假设我们有一台CUDA的机器，然后这些方法将递归遍历所有模块并将其参数和缓冲区转换为CUDA张量：</p>
<ul>
<li>注意优化器我们之前定义过（那是基于没有移动到GPU上的net，所以这里要再定义一次）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.to(device)</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment">#损失函数标准为交叉熵</span></span><br><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#动量因子momentum</span></span><br><span class="line">optimizer=optim.SGD(net.parameters(),lr=<span class="number">0.01</span>,momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<p>请记住，你也必须在每一步中把你的输入和目标值转换到GPU上</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#epoch很熟悉了，这是数据集训练的轮次</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    </span><br><span class="line">    running_loss=<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i,data <span class="keyword">in</span> enumerate(trainloader,<span class="number">0</span>):</span><br><span class="line">        <span class="comment">#获得输入</span></span><br><span class="line">        inputs,labels=data</span><br><span class="line">        inputs=inputs.to(device)</span><br><span class="line">        labels=labels.to(device)</span><br><span class="line">            </span><br><span class="line">        <span class="comment">#梯度置0</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#forward+backward+optimize</span></span><br><span class="line">        outputs=net(inputs)</span><br><span class="line">        loss=criterion(outputs,labels)</span><br><span class="line">        loss=loss.to(device)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#打印统计结果</span></span><br><span class="line">        running_loss+=loss.item()</span><br><span class="line">        <span class="comment">#每2000个为一个小批量，进行打印</span></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">2000</span>==<span class="number">1999</span>:</span><br><span class="line">            print(<span class="string">'[%d,%5d] loss:%.3f'</span>%(epoch+<span class="number">1</span>,i+<span class="number">1</span>,running_loss/<span class="number">2000</span>))</span><br><span class="line">            running_loss=<span class="number">0.0</span></span><br><span class="line">            </span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>[1, 2000] loss:1.937
[1, 4000] loss:1.940
[1, 6000] loss:1.943
[1, 8000] loss:1.987
[1,10000] loss:2.013
[1,12000] loss:1.979
[2, 2000] loss:1.950
[2, 4000] loss:2.005
[2, 6000] loss:1.965
[2, 8000] loss:1.974
[2,10000] loss:1.970
[2,12000] loss:1.962
Finished Training</code></pre><h4 id="在测试集上测试网络"><a href="#在测试集上测试网络" class="headerlink" title="在测试集上测试网络"></a>在测试集上测试网络</h4><p>我们在整个训练集上训练了两次网络,但是我们还需要检查网络是否从数据集中学习到东西。</p>
<p>我们通过预测神经网络输出的类别标签并根据实际情况进行检测，如果预测正确,我们把该样本添加到正确预测列表。</p>
<p>第一步，显示测试集中的图片一遍熟悉图片内容。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataiter=iter(testloader)</span><br><span class="line">images,labels=dataiter.next()</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(<span class="string">'GroundTruth:'</span>,<span class="string">' '</span>.join(<span class="string">'%5s'</span>%classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>

<h4 id="注意：这里网络和数据都要加cuda-，因为网络是在GPU上训练的，数据也都是再GPU上跑的，这里不加会报错，教程里没加，是因为他没放到GPU上运行。"><a href="#注意：这里网络和数据都要加cuda-，因为网络是在GPU上训练的，数据也都是再GPU上跑的，这里不加会报错，教程里没加，是因为他没放到GPU上运行。" class="headerlink" title="注意：这里网络和数据都要加cuda()，因为网络是在GPU上训练的，数据也都是再GPU上跑的，这里不加会报错，教程里没加，是因为他没放到GPU上运行。"></a>注意：这里网络和数据都要加cuda()，因为网络是在GPU上训练的，数据也都是再GPU上跑的，这里不加会报错，教程里没加，是因为他没放到GPU上运行。</h4><p>现在我们来看看神经网络认为以上图片是什么?</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net=net.cuda()</span><br><span class="line">images=images.cuda()</span><br><span class="line">outputs = net(images)</span><br></pre></td></tr></table></figure>

<p>输出是10个标签的概率。一个类别的概率越大,神经网络越认为他是这个类别。所以让我们得到最高概率的标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">_,predicted=torch.max(outputs.data,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span> % classes[predicted[j]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>

<pre><code>Predicted:    cat   dog   dog horse</code></pre><p>接下来让我们看看网络在整个测试集上的结果如何。</p>
<ul>
<li>出现错误’weight’相关的，基本上都是没有将数据放在gpu上，在其后加一个.cuda()即可</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images,labels=data</span><br><span class="line">        net=net.cuda()</span><br><span class="line">        labels=labels.cuda()</span><br><span class="line">        images=images.cuda()</span><br><span class="line">        outputs=net(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()        </span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (<span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy of the network on the 10000 test images: 26 %</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        net=net.cuda()</span><br><span class="line">        labels=labels.cuda()</span><br><span class="line">        images=images.cuda()</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.max(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    print(<span class="string">'Accuracy of %5s : %2d %%'</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Python之进程和线程</title>
    <url>/ck3bmvcxn0033bsg41p0hfek9.html</url>
    <content><![CDATA[<p>刚开始学习Python的时候，只是简单的学习了基础的语法和面向对象相关的资料，但是对于进程和线程并没有花时间来看，并且到这一章，和底层实现就密切相关了，和OS更是密不可分，面试非常喜欢问这类的问题，所以还是花点时间来看看，不妨多看看，多找找相关资料，很有帮助，本人是看的廖雪峰的教程，感觉还可以，就是这一章，最好还是在ubuntu环境下跑，因为linux和windows在进程和线程实现细节上有些不同！</p>
<a id="more"></a>

<p>很多同学都听说过，现代操作系统比如Mac OS X，UNIX，Linux，Windows等，都是支持“多任务”的操作系统。</p>
<p>什么叫“多任务”呢？简单地说，就是操作系统可以同时运行多个任务。打个比方，你一边在用浏览器上网，一边在听MP3，一边在用Word赶作业，这就是多任务，至少同时有3个任务正在运行。还有很多任务悄悄地在后台同时运行着，只是桌面上没有显示而已。</p>
<p>现在，多核CPU已经非常普及了，但是，即使过去的单核CPU，也可以执行多任务。由于CPU执行代码都是顺序执行的，那么，单核CPU是怎么执行多任务的呢？</p>
<p>答案就是<strong>操作系统轮流让各个任务交替执行</strong>，任务1执行0.01秒，切换到任务2，任务2执行0.01秒，再切换到任务3，执行0.01秒……这样反复执行下去。表面上看，每个任务都是交替执行的，但是，由于CPU的执行速度实在是太快了，我们感觉就像所有任务都在同时执行一样。</p>
<p>真正的并行执行多任务只能在多核CPU上实现，但是，由于任务数量远远多于CPU的核心数量，所以，操作系统也会自动把很多任务轮流调度到每个核心上执行。</p>
<p>对于操作系统来说，一个任务就是一个进程（Process），比如打开一个浏览器就是启动一个浏览器进程，打开一个记事本就启动了一个记事本进程，打开两个记事本就启动了两个记事本进程，打开一个Word就启动了一个Word进程。</p>
<p>有些进程还不止同时干一件事，比如Word，它可以同时进行打字、拼写检查、打印等事情。在一个进程内部，要同时干多件事，就需要同时运行多个“子任务”，我们把进程内的这些“子任务”称为线程（Thread）。</p>
<p>由于每个进程至少要干一件事，所以，一个进程至少有一个线程。当然，像Word这种复杂的进程可以有多个线程，多个线程可以同时执行，多线程的执行方式和多进程是一样的，也是由操作系统在多个线程之间快速切换，让每个线程都短暂地交替运行，看起来就像同时执行一样。当然，真正地同时执行多线程需要多核CPU才可能实现。</p>
<p>我们前面编写的所有的Python程序，都是执行单任务的进程，也就是只有一个线程。如果我们要同时执行多个任务怎么办？</p>
<p>有两种解决方案：</p>
<ul>
<li><p>一种是启动多个进程，每个进程虽然只有一个线程，但多个进程可以一块执行多个任务。</p>
</li>
<li><p>还有一种方法是启动一个进程，在一个进程内启动多个线程，这样，多个线程也可以一块执行多个任务。</p>
</li>
<li><p>当然还有第三种方法，就是启动多个进程，每个进程再启动多个线程，这样同时执行的任务就更多了，当然这种模型更复杂，实际很少采用。</p>
</li>
</ul>
<p>总结一下就是，多任务的实现有3种方式：</p>
<ul>
<li>多进程模式；</li>
<li>多线程模式；</li>
<li>多进程+多线程模式</li>
</ul>
<p>同时执行多个任务通常各个任务之间并不是没有关联的，而是需要相互通信和协调，有时，任务1必须暂停等待任务2完成后才能继续执行，有时，任务3和任务4又不能同时执行，所以，多进程和多线程的程序的复杂度要远远高于我们前面写的单进程单线程的程序。</p>
<p>因为复杂度高，调试困难，所以，不是迫不得已，我们也不想编写多任务。但是，有很多时候，没有多任务还真不行。想想在电脑上看电影，就必须由一个线程播放视频，另一个线程播放音频，否则，单线程实现的话就只能先把视频播放完再播放音频，或者先把音频播放完再播放视频，这显然是不行的。</p>
<p>Python既支持多进程，又支持多线程，我们会讨论如何编写这两种多任务程序。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>线程是最小的执行单元，而进程由至少一个线程组成。如何调度进程和线程，完全由操作系统决定，程序自己不能决定什么时候执行，执行多长时间。</p>
<p>多进程和多线程的程序涉及到同步、数据共享的问题，编写起来更复杂。</p>
<h4 id="多进程"><a href="#多进程" class="headerlink" title="多进程"></a>多进程</h4><p>要让Python程序实现多进程（multiprocessing），我们先了解操作系统的相关知识。</p>
<p>Unix/Linux操作系统提供了一个<code>fork()</code> 系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是fork()调用一次，<strong>返回两次</strong> ，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。</p>
<p>子进程永远返回<code>0</code>，而父进程返回子进程的ID。这样做的理由是，一个父进程可以fork出很多子进程，所以，父进程要记下每个子进程的ID，而子进程只需要调用<code>getppid()</code>就可以拿到父进程的ID。</p>
<p>Python的os模块封装了常见的系统调用，其中就包括fork，可以在Python程序中轻松创建子进程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Process (%s) start...'</span>%os.getpid())</span><br><span class="line">pid=os.fork()</span><br><span class="line"><span class="keyword">if</span> pid == <span class="number">0</span>:</span><br><span class="line">    print(<span class="string">'I am child process (%s) and my parent is %s'</span>%(os.getpid(),os.getppid()))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'I (%s) just created a child process (%s)'</span>%(os.getpid(),pid))</span><br></pre></td></tr></table></figure>

<p>由于Windows没有fork调用，上面的代码在Windows上无法运行。<br>这是我在ubuntu下运行的结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Process (96787) start...</span><br><span class="line">I (96787) just created a child process (96788)</span><br><span class="line">I am child process (96788) and my parent is 96787</span><br></pre></td></tr></table></figure>

<p>有了fork调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的Apache服务器就是由父进程监听端口，每当有新的http请求时，就fork出子进程来处理新的http请求。</p>
<h4 id="multiprocessing"><a href="#multiprocessing" class="headerlink" title="multiprocessing"></a>multiprocessing</h4><p>如果你打算编写多进程的服务程序，Unix/Linux无疑是正确的选择。由于Windows没有fork调用，难道在Windows上无法用Python编写多进程的程序？</p>
<p>由于Python是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing模块就是跨平台版本的多进程模块。</p>
<p>multiprocessing模块提供了一个Process类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#子进程要做的事情</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_proc</span><span class="params">(name)</span>:</span></span><br><span class="line">    print(<span class="string">'Run child process %s (%s)...'</span>%(name,os.getpid()))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">'Parent process %s.'</span>%os.getpid())</span><br><span class="line">    p = Process(target=run_proc,args=(<span class="string">'test'</span>,))</span><br><span class="line">    print(<span class="string">'Child process will start.'</span>)</span><br><span class="line">    p.start()</span><br><span class="line">    p.join()</span><br><span class="line">    print(<span class="string">'Child process end.'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Parent process 2816.
Child process will start.
Child process end.</code></pre><p>这是我在ubuntu下运行的结果，不知道windows为什么有问题</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Parent process 98189.</span><br><span class="line">Child process will start.</span><br><span class="line">Run child process test (98190)...</span><br><span class="line">Child process end.</span><br></pre></td></tr></table></figure>

<p>创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动，这样创建进程比fork()还要简单。</p>
<p>join()方法可以<strong>等待子进程结束后再继续往下运行</strong> ，通常用于进程间的同步。<br>下面是将<code>p.join()</code>注释掉的结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Parent process 98628.</span><br><span class="line">Child process will start.</span><br><span class="line">Child process end.</span><br><span class="line">Run child process test (98629)...</span><br></pre></td></tr></table></figure>

<h4 id="Pool进程池"><a href="#Pool进程池" class="headerlink" title="Pool进程池"></a>Pool进程池</h4><p>如果要启动大量的子进程，可以用进程池的方式批量创建子进程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">import</span> os,time,random</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_time_rask</span><span class="params">(name)</span>:</span></span><br><span class="line">    print(<span class="string">'Run task %s (%s)...'</span> % (name, os.getpid()))</span><br><span class="line">    start=time.time()</span><br><span class="line">    time.sleep(random.random()*<span class="number">3</span>)</span><br><span class="line">    end=time.time()</span><br><span class="line">    print(<span class="string">'Task %s runs %0.2f seconds.'</span> % (name, (end - start)))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">'Parent process %s.'</span> % os.getpid())</span><br><span class="line">    p=Pool(<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        p.apply_async(long_time_rask,args=(i,))</span><br><span class="line">    print(<span class="string">'Waiting for all subprocesses done...'</span>)</span><br><span class="line">    p.close()</span><br><span class="line">    p.join()</span><br><span class="line">    print(<span class="string">'All subprocesses done.'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Parent process 2816.
Waiting for all subprocesses done...</code></pre><p>哎，这一章节真的不能在windows下运行，如上，在windows运行，会卡住，下面是我在ubuntu下运行的结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Parent process 99268.</span><br><span class="line">Waiting for all subprocesses done...</span><br><span class="line">Run task 0 (99270)...</span><br><span class="line">Run task 1 (99271)...</span><br><span class="line">Run task 2 (99272)...</span><br><span class="line">Run task 3 (99273)...</span><br><span class="line">Task 0 runs 0.30 seconds.</span><br><span class="line">Run task 4 (99270)...</span><br><span class="line">Task 1 runs 0.37 seconds.</span><br><span class="line">Task 4 runs 0.74 seconds.</span><br><span class="line">Task 2 runs 1.85 seconds.</span><br><span class="line">Task 3 runs 2.58 seconds.</span><br><span class="line">All subprocesses done.</span><br></pre></td></tr></table></figure>

<p>对Pool对象调用join()方法会等待所有子进程执行完毕，<strong>调用join()之前必须先调用close()</strong> ，调用close()之后就不能继续添加新的Process了。</p>
<p>请注意输出的结果，task 0，1，2，3是立刻执行的（也就是说同一时刻，这四个进程是同时进行的），而task 4要等待前面某个task完成后才执行（因为前边有四个进程在执行，而我们设定的同时能运行的进程数4，所以只能等待），这是因为Pool的默认大小在我的电脑上是4，因此，最多同时执行4个进程。这是Pool有意设计的限制，并不是操作系统的限制。如果改成：<br><code>p = Pool(5)</code>，就可以同时跑5个进程。<br>由于Pool的默认大小是CPU的核数，如果你不幸拥有8核CPU，你要提交至少9个子进程才能看到上面的等待效果。</p>
<h4 id="子进程"><a href="#子进程" class="headerlink" title="子进程"></a>子进程</h4><p>很多时候，子进程并不是自身，而是一个外部进程。我们创建了子进程后，还需要控制子进程的输入和输出。</p>
<p>subprocess模块可以让我们非常方便地启动一个子进程，然后控制其输入和输出。</p>
<p>下面的例子演示了如何在Python代码中运行命令 <code>nslookup www.python.org</code>，这和命令行直接运行的效果是一样的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="comment">#nslookup命令用于查询DNS的记录，查看域名解析是否正常，在网络故障的时候用来诊断网络问题</span></span><br><span class="line">print(<span class="string">'$ nslookup www.python.org'</span>)</span><br><span class="line">r=subprocess.call([<span class="string">'nslookup'</span>,<span class="string">'www.python.org'</span>])</span><br><span class="line">print(<span class="string">'exit code:'</span>,r)</span><br></pre></td></tr></table></figure>

<pre><code>$ nslookup www.python.org
exit code: 0</code></pre><p>这是我在Ubuntu下运行的结果</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ nslookup www.python.org</span><br><span class="line">Server:         202.204.48.8</span><br><span class="line">Address:        202.204.48.8#53</span><br><span class="line"></span><br><span class="line">Non-authoritative answer:</span><br><span class="line">www.python.org  canonical name = dualstack.python.map.fastly.net.</span><br><span class="line">Name:   dualstack.python.map.fastly.net</span><br><span class="line">Address: 151.101.24.223</span><br><span class="line"></span><br><span class="line">exit code:</span><br></pre></td></tr></table></figure>

<p>如果子进程还需要输入，则可以通过communicate()方法输入：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">'$ nslookup'</span>)</span><br><span class="line">p = subprocess.Popen([<span class="string">'nslookup'</span>], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)</span><br><span class="line">output,err=p.communicate(<span class="string">b'set q=mx\npython.org\nexit\n'</span>)</span><br><span class="line">print(output.decode(<span class="string">'utf-8'</span>))</span><br><span class="line">print(<span class="string">'exit code:'</span>,p.returncode)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ nslookup</span><br><span class="line">Server:         202.204.48.8</span><br><span class="line">Address:        202.204.48.8#53</span><br><span class="line"></span><br><span class="line">Non-authoritative answer:</span><br><span class="line">python.org      mail exchanger = 50 mail.python.org.</span><br><span class="line"></span><br><span class="line">Authoritative answers can be found from:</span><br><span class="line">mail.python.org internet address = 188.166.95.178</span><br><span class="line">mail.python.org has AAAA address 2a03:b0c0:2:d0::71:1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">exit code: 0</span><br></pre></td></tr></table></figure>

<h3 id="进程间通信"><a href="#进程间通信" class="headerlink" title="进程间通信"></a>进程间通信</h3><p>Process之间肯定是需要通信的，操作系统提供了很多机制来实现进程间的通信。Python的<code>multiprocessing</code>模块包装了底层的机制，提供了<code>Queue</code>、<code>Pipes</code>等多种方式来交换数据。</p>
<p>我们以<code>Queue</code>为例，在父进程中创建两个子进程，一个往<code>Queue</code>里写数据，一个从<code>Queue</code>里读数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#写数据进程执行的代码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write</span><span class="params">(q)</span>:</span></span><br><span class="line">    print(<span class="string">'Process to write: %s'</span> % os.getpid())</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> [<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>]:</span><br><span class="line">        print(<span class="string">'Put %s to queue...'</span> % value)</span><br><span class="line">        q.put(value)</span><br><span class="line">        <span class="comment">#有了这一步，有了一定的时间间隔，那么在这个过程中，也是可以读的，如果注释掉，那么ABCD一股脑全部写入</span></span><br><span class="line">        time.sleep(random.random())</span><br><span class="line"></span><br><span class="line"><span class="comment">#读数据进程执行的代码</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span><span class="params">(q)</span>:</span></span><br><span class="line">    print(<span class="string">'Process to read: %s '</span> % os.getpid())</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        value=q.get(<span class="literal">True</span>)</span><br><span class="line">        print(<span class="string">'Get %s from queue.'</span> % value)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">#父进程创建Queue，并传给各个子进程</span></span><br><span class="line">    q=Queue()</span><br><span class="line">    pw=Process(target=write,args=(q,))</span><br><span class="line">    pr=Process(target=read,args=(q,))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#启动子进程pw，写入数据：</span></span><br><span class="line">    pw.start()</span><br><span class="line">    <span class="comment">#启动子进程pr，读出数据：</span></span><br><span class="line">    pr.start()</span><br><span class="line">    <span class="comment">#等待pw结束</span></span><br><span class="line">    pw.join()</span><br><span class="line">    <span class="comment">#pr进程里是死循环，无法等待其结束，只能强行终止：</span></span><br><span class="line">    pr.terminate()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Process to write: 99800</span><br><span class="line">Put A to queue...</span><br><span class="line">Process to read: 99801 </span><br><span class="line">Get A from queue.</span><br><span class="line">Put B to queue...</span><br><span class="line">Get B from queue.</span><br><span class="line">Put C to queue...</span><br><span class="line">Get C from queue.</span><br><span class="line">Put D to queue...</span><br><span class="line">Get D from queue.</span><br></pre></td></tr></table></figure>

<p>在Unix/Linux下，<code>multiprocessing</code>模块封装了<code>fork()</code>调用，使我们不需要关注<code>fork()</code>的细节。由于Windows没有<code>fork</code>调用，因此，<code>multiprocessing</code>需要“模拟”出fork的效果，父进程所有Python对象都必须通过<code>pickle</code>序列化再传到子进程去，所以，如果<code>multiprocessing</code>在Windows下调用失败了，要先考虑是不是pickle失败了。</p>
]]></content>
      <categories>
        <category>Python</category>
        <category>OS</category>
      </categories>
      <tags>
        <tag>OS</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Django项目的自动化测试</title>
    <url>/ck3bmvcwh0027bsg4egxqdho1.html</url>
    <content><![CDATA[<p>最近，实验室有个小项目，关于企业上云，然后我们这边要做一个Django的web(已经做了很多功能，需要添加和修改一些功能)，所以要给项目添加一些测试，以备后期更改，自动化测试看的是Django 的官方文档。网址是： <a href="https://docs.djangoproject.com/zh-hans/2.2/intro/tutorial05/" target="_blank" rel="noopener">https://docs.djangoproject.com/zh-hans/2.2/intro/tutorial05/</a> 。</p>
<a id="more"></a>

<h3 id="自动化测试简介"><a href="#自动化测试简介" class="headerlink" title="自动化测试简介"></a>自动化测试简介</h3><h4 id="自动化测试是什么"><a href="#自动化测试是什么" class="headerlink" title="自动化测试是什么"></a>自动化测试是什么</h4><p>测试，是用来检查代码正确性的一些简单的程序。</p>
<p>测试在不同的层次中都存在。有些测试只关注某个很小的细节（某个模型的某个方法的返回值是否满足预期？），而另一些测试可能检查对某个软件的一系列操作（<em>某一用户输入序列是否造成了预期的结果？</em>）。其实这和我们在 <a href="https://docs.djangoproject.com/zh-hans/2.2/intro/tutorial02/" target="_blank" rel="noopener">教程第 2 部分</a>，里做的并没有什么不同，我们使用 <a href="https://docs.djangoproject.com/zh-hans/2.2/ref/django-admin/#django-admin-shell" target="_blank" rel="noopener"><code>shell</code></a> 来测试某一方法的功能，或者运行某个应用并输入数据来检查它的行为。</p>
<p>真正不同的地方在于，<em>自动化</em> 测试是由某个系统帮你自动完成的。当你创建好了一系列测试，每次修改应用代码后，就可以自动检查出修改后的代码是否还像你曾经预期的那样正常工作。你不需要花费大量时间来进行手动测试。</p>
<h4 id="为什么你需要写测试？"><a href="#为什么你需要写测试？" class="headerlink" title="为什么你需要写测试？"></a>为什么你需要写测试？</h4><p>但是，为什么需要测试呢？又为什么是现在呢？</p>
<p>你可能觉得学 Python/Django 对你来说已经很满足了，再学一些新东西的话看起来有点负担过重并且没什么必要。毕竟，我们的投票应用（官方文档的一个项目举例）看起来已经完美工作了。写一些自动测试并不能让它工作的更好。如果写一个投票应用是你想用 Django 完成的唯一工作，那你确实没必要学写测试。<strong>但是如果你还想写更复杂的项目，现在就是学习测试写法的最好时机了。</strong></p>
<ul>
<li><p>测试将节约你的时间</p>
<p>在某种程度上，能够「判断出代码是否正常工作」的测试，就称得上是个令人满意的了。在更复杂的应用程序中，组件之间可能会有数十个复杂的交互。</p>
<p>在更加复杂的应用中，各种组件之间的交互可能会及其的复杂。改变其中某一组件的行为，也有可能会造成意想不到的结果。判断「代码是否正常工作」意味着你需要用大量的数据来完整的测试全部代码的功能，以确保你的小修改没有对应用整体造成破坏——这太费时间了。</p>
<p>尤其是当你发现自动化测试能在几秒钟之内帮你完成这件事时，就更会觉得手动测试实在是太浪费时间了。当某人写出错误的代码时，自动化测试还能帮助你定位错误代码的位置。</p>
<p>有时候你会觉得，和富有创造性和生产力的业务代码比起来，编写枯燥的测试代码实在是太无聊了，特别是当你知道你的代码完全没有问题的时候。</p>
<p><strong>然而，编写测试还是要比花费几个小时手动测试你的应用，或者为了找到某个小错误而胡乱翻看代码要有意义的多。</strong></p>
</li>
<li><p>测试不仅能发现错误，还能预防错误</p>
<p>「测试是开发的对立面」，这种思想是不对的。</p>
<p>如果没有测试，整个应用的行为意图会变得更加的不清晰。甚至当你在看自己写的代码时也是这样，有时候你需要仔细研读一段代码才能搞清楚它有什么用。</p>
<p>而测试的出现改变了这种情况。测试就好像是从内部仔细检查你的代码，当有些地方出错时，这些地方将会变得很显眼——<em>就算你自己没有意识到那里写错了</em>。</p>
</li>
<li><p>测试使你的代码更有吸引力</p>
<p>你也许遇到过这种情况：你编写了一个绝赞的软件，但是其他开发者看都不看它一眼，因为它缺少测试。<strong>没有测试的代码不值得信任。</strong> Django 最初开发者之一的 Jacob Kaplan-Moss 说过：“项目规划时没有包含测试是不科学的。”</p>
<p>其他的开发者希望在正式使用你的代码前看到它通过了测试，这是你需要写测试的另一个重要原因。</p>
</li>
<li><p>测试有助于团队协作</p>
<p>前面的几点都是从单人开发的角度来说的。复杂的应用可能由团队维护。测试的存在保证了协作者不会不小心破坏了了你的代码（也保证你不会不小心弄坏他们的）。如果你想作为一个 Django 程序员谋生的话，你必须擅长编写测试！ </p>
</li>
</ul>
<h4 id="基础测试策略"><a href="#基础测试策略" class="headerlink" title="基础测试策略"></a>基础测试策略</h4><p>有好几种不同的方法可以写测试。</p>
<p>一些开发者遵循 “<a href="https://en.wikipedia.org/wiki/Test-driven_development" target="_blank" rel="noopener">测试驱动</a>“ 的开发原则，他们在写代码之前先写测试。这种方法看起来有点反直觉，但事实上，这和大多数人日常的做法是相吻合的。我们会先描述一个问题，然后写代码来解决它。「测试驱动」的开发方法只是将问题的描述抽象为了 Python 的测试样例。</p>
<p>更普遍的情况是，一个刚接触自动化测试的新手更倾向于先写代码，然后再写测试。虽然提前写测试可能更好，但是晚点写起码也比没有强。</p>
<p>有时候很难决定从哪里开始下手写测试。如果你才写了几千行 Python 代码，选择从哪里开始写测试确实不怎么简单。如果是这种情况，那么在你下次修改代码（比如加新功能，或者修复 Bug）之前写个测试是比较合理且有效的。</p>
<h4 id="Django中的测试"><a href="#Django中的测试" class="headerlink" title="Django中的测试"></a>Django中的测试</h4><p>随着网站的增长，他们越来越难以手动测试。不仅要进行更多的测试，而且随着组件之间的交互变得越来越复杂，一个区域的小改变可能会影响到其他区域，所以需要做更多的改变来确保一切正常运行，并且在进行更多更改时不会引入错误。减轻这些问题的一种方法是编写自动化测试，每当您进行更改时，都可以轻松可靠地运行测试。</p>
<p>测试一个 Web 应用是一项复杂的工作，因为 Web 应用包含了多层业务逻辑——从 HTTP 层响应请求，到表单有效性检测和处理，再到模板渲染。利用 Django 的测试执行框架和配套的工具，你可以模拟其你去，插入测试数据，检查应用的输出，以此检验你的代码是否按照期望运行。</p>
<p>最大的优点是，它非常简单。</p>
<p>此外，自动化测试可以充当代码的第一个真实“用户”，迫使您严格定义和记录网站的行为方式。它们通常是您的代码示例，和文档的基础。由于这些原因，一些软件开发过程，从测试定义和实现开始，之后编写代码以匹配所需的行为（例如，测试驱动<a href="https://en.wikipedia.org/wiki/Test-driven_development" target="_blank" rel="noopener">test-driven</a> 和行为驱动 <a href="https://en.wikipedia.org/wiki/Behavior-driven_development" target="_blank" rel="noopener">behaviour-driven</a>的开发）。 </p>
<h4 id="测试的类型"><a href="#测试的类型" class="headerlink" title="测试的类型"></a>测试的类型</h4><ul>
<li>单元测试<ul>
<li>验证各个组件的功能行为，通常是类别和功能级别 </li>
</ul>
</li>
<li>回归测试<ul>
<li>测试重现历史错误。最初运行每个测试，以验证错误是否已修复，然后重新运行，以确保在以后更改代码之后，未重新引入该错误。 （有点像测试后发现错误，然后修改错误，再进行测试的意思）</li>
</ul>
</li>
<li>集成测试<ul>
<li>验证组件分组在一起使用时的工作方式。集成测试了解组件之间所需的交互，但不一定了解每个组件的内部操作。它们可能涵盖整个网站的简单组件分组。 （这就有点像全部来一次测试，验证各个组件之间是否能够正常运行）</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>Django</category>
        <category>自动化测试</category>
      </categories>
      <tags>
        <tag>Django</tag>
        <tag>自动化测试</tag>
      </tags>
  </entry>
  <entry>
    <title>279完全平方数</title>
    <url>/ck3bmvcto000ebsg403op2a0w.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p> 给定正整数 <em>n</em>，找到若干个完全平方数（比如 <code>1, 4, 9, 16, ...</code>）使得它们的和等于 <em>n</em>。你需要让组成和的完全平方数的个数最少。 </p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: n = <span class="number">12</span></span><br><span class="line">输出: <span class="number">3</span> </span><br><span class="line">解释: <span class="number">12</span> = <span class="number">4</span> + <span class="number">4</span> + <span class="number">4.</span></span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: n = <span class="number">13</span></span><br><span class="line">输出: <span class="number">2</span></span><br><span class="line">解释: <span class="number">13</span> = <span class="number">4</span> + <span class="number">9.</span></span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="解题思路：本题思考了一会，在想怎么使用DP呢？然后灵光一现，这不就是兑换零钱的变种吗，只不过零钱的面额在这一题中，不再是固定的了，比如12，那么对应的“面额”就应该是1，4，9，这些完全平方数，以此类推，第一次的代码，我是将这些“面额”循环插入到一个数组中了（其实没必要，可以合并，进行代码优化），其他的思想和兑换零钱一样。"><a href="#解题思路：本题思考了一会，在想怎么使用DP呢？然后灵光一现，这不就是兑换零钱的变种吗，只不过零钱的面额在这一题中，不再是固定的了，比如12，那么对应的“面额”就应该是1，4，9，这些完全平方数，以此类推，第一次的代码，我是将这些“面额”循环插入到一个数组中了（其实没必要，可以合并，进行代码优化），其他的思想和兑换零钱一样。" class="headerlink" title="解题思路：本题思考了一会，在想怎么使用DP呢？然后灵光一现，这不就是兑换零钱的变种吗，只不过零钱的面额在这一题中，不再是固定的了，比如12，那么对应的“面额”就应该是1，4，9，这些完全平方数，以此类推，第一次的代码，我是将这些“面额”循环插入到一个数组中了（其实没必要，可以合并，进行代码优化），其他的思想和兑换零钱一样。"></a>解题思路：本题思考了一会，在想怎么使用DP呢？然后灵光一现，这不就是兑换零钱的变种吗，只不过零钱的面额在这一题中，不再是固定的了，比如12，那么对应的“面额”就应该是1，4，9，这些完全平方数，以此类推，第一次的代码，我是将这些“面额”循环插入到一个数组中了（其实没必要，可以合并，进行代码优化），其他的思想和兑换零钱一样。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">numSquares</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;dp(n+<span class="number">1</span>,n);</span><br><span class="line">        dp[<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;=n;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j*j&lt;=n;j++)&#123;</span><br><span class="line">                <span class="keyword">int</span> cur=j*j;</span><br><span class="line">                <span class="keyword">if</span>(i&lt;cur)<span class="keyword">break</span>;</span><br><span class="line">                dp[i]=min(<span class="number">1</span>+dp[i-cur],dp[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>152乘积最大子序列</title>
    <url>/ck3bmvcsu0008bsg40ojegsk8.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p> 给定一个整数数组 <code>nums</code> ，找出一个序列中乘积最大的连续子序列（该序列至少包含一个数）。 </p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: [<span class="number">2</span>,<span class="number">3</span>,<span class="number">-2</span>,<span class="number">4</span>]</span><br><span class="line">输出: <span class="number">6</span></span><br><span class="line">解释: 子数组 [<span class="number">2</span>,<span class="number">3</span>] 有最大乘积 <span class="number">6</span>。</span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: [<span class="number">-2</span>,<span class="number">0</span>,<span class="number">-1</span>]</span><br><span class="line">输出: <span class="number">0</span></span><br><span class="line">解释: 结果不能为 <span class="number">2</span>, 因为 [<span class="number">-2</span>,<span class="number">-1</span>] 不是子数组。</span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="解题思路：刚开始，我觉得非常简单，是一道特别基础的DP题目，但是后来提交的时候才发现，欠考虑了，dp-i-代表以当前节点为终止结点的最大连乘值，但是这一题特别就特别在他有负的值，所以我们不能只保存最大的连乘值，我们还要保存最小的连乘值，然后取前一个最大连乘值的时候就要注意，如果当前节点是负数，那我就要去最小的连乘值了，所以采用两个dp，一个存最大，一个存最小，（最大的来源要有最小这个因素）最后去取最大即可。"><a href="#解题思路：刚开始，我觉得非常简单，是一道特别基础的DP题目，但是后来提交的时候才发现，欠考虑了，dp-i-代表以当前节点为终止结点的最大连乘值，但是这一题特别就特别在他有负的值，所以我们不能只保存最大的连乘值，我们还要保存最小的连乘值，然后取前一个最大连乘值的时候就要注意，如果当前节点是负数，那我就要去最小的连乘值了，所以采用两个dp，一个存最大，一个存最小，（最大的来源要有最小这个因素）最后去取最大即可。" class="headerlink" title="解题思路：刚开始，我觉得非常简单，是一道特别基础的DP题目，但是后来提交的时候才发现，欠考虑了，dp[i]代表以当前节点为终止结点的最大连乘值，但是这一题特别就特别在他有负的值，所以我们不能只保存最大的连乘值，我们还要保存最小的连乘值，然后取前一个最大连乘值的时候就要注意，如果当前节点是负数，那我就要去最小的连乘值了，所以采用两个dp，一个存最大，一个存最小，（最大的来源要有最小这个因素）最后去取最大即可。"></a>解题思路：刚开始，我觉得非常简单，是一道特别基础的DP题目，但是后来提交的时候才发现，欠考虑了，dp[i]代表以当前节点为终止结点的最大连乘值，但是这一题特别就特别在他有负的值，所以我们不能只保存最大的连乘值，我们还要保存最小的连乘值，然后取前一个最大连乘值的时候就要注意，如果当前节点是负数，那我就要去最小的连乘值了，所以采用两个dp，一个存最大，一个存最小，（最大的来源要有最小这个因素）最后去取最大即可。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxProduct</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> size=nums.size();</span><br><span class="line">        <span class="keyword">int</span> mul_pre,mul_pre2,res,max_temp,min_temp;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;dp(size);</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;dp2(size);</span><br><span class="line">        dp[<span class="number">0</span>]=nums[<span class="number">0</span>];</span><br><span class="line">        dp2[<span class="number">0</span>]=nums[<span class="number">0</span>];</span><br><span class="line">        res=dp[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;size;i++)&#123;</span><br><span class="line">            mul_pre=nums[i]*dp[i<span class="number">-1</span>];</span><br><span class="line">            mul_pre2=nums[i]*dp2[i<span class="number">-1</span>];</span><br><span class="line">            max_temp=max(mul_pre,mul_pre2);</span><br><span class="line">            dp[i]=max(max_temp,nums[i]);</span><br><span class="line">            min_temp=min(mul_pre,mul_pre2);</span><br><span class="line">            dp2[i]=min(min_temp,nums[i]);</span><br><span class="line">            <span class="keyword">if</span>(res&lt;dp[i])res=dp[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>Python回顾之OOP</title>
    <url>/ck3bmvcxq0036bsg412zaah91.html</url>
    <content><![CDATA[<h3 id="面向对象的两个基本概念"><a href="#面向对象的两个基本概念" class="headerlink" title="面向对象的两个基本概念"></a>面向对象的两个基本概念</h3><p>编程语言中，一般有两种编程思维，面向过程和面向对象。</p>
<p>面向过程，看重的是解决问题的过程。</p>
<p>这好比我们解决日常生活问题差不多，分析解决问题的步骤，然后一步一步的解决。</p>
<p>而面向对象是一种抽象，抽象是指用分类的眼光去看世界的一种方法。</p>
<a id="more"></a>

<p>Python 就是一门面向对象的语言,</p>
<p>如果你学过 Java ，就知道 Java 的编程思想就是：万事万物皆对象。Python 也不例外，在解决实际问题的过程中，可以把构成问题事务分解成各个对象。</p>
<p>面向对象都有两个基本的概率，分别是类和对象。</p>
<ul>
<li><p>类<br>用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。</p>
</li>
<li><p>对象<br>通过类定义的数据结构实例</p>
</li>
</ul>
<h3 id="面向对象的三大特性"><a href="#面向对象的三大特性" class="headerlink" title="面向对象的三大特性"></a>面向对象的三大特性</h3><p>面向对象的编程语言，也有三大特性，继承，多态和封装性。</p>
<ul>
<li><p>继承<br>即一个派生类（derived class）继承基类（base class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如：一个 Dog 类型的对象派生自 Animal 类，这是模拟”是一个（is-a）”关系（例图，Dog 是一个 Animal ）。</p>
</li>
<li><p>多态<br>它是指对不同类型的变量进行相同的操作，它会根据对象（或类）类型的不同而表现出不同的行为。</p>
</li>
<li><p>封装性<br>“封装”就是将抽象得到的数据和行为（或功能）相结合，形成一个有机的整体（即类）；封装的目的是增强安全性和简化编程，使用者不必了解具体的实现细节，而只是要通过外部接口，一特定的访问权限来使用类的成员。</p>
</li>
</ul>
<h4 id="类方法如何调用类属性"><a href="#类方法如何调用类属性" class="headerlink" title="类方法如何调用类属性"></a>类方法如何调用类属性</h4><p>如果没有声明是类方法，方法参数中就没有 cls , 就没法通过 cls 获取到类属性。</p>
<p>因此类方法，想要调用类属性，需要以下步骤：</p>
<ul>
<li>在方法上面，用 @classmethon 声明该方法是类方法。只有声明了是类方法，才能使用类属性</li>
<li>类方法想要使用类属性，在第一个参数中，需要写上 cls , cls 是 class 的缩写，其实意思就是把这个类作为参数，传给自己，这样就可以使用类属性了。</li>
<li>类属性的使用方式就是 cls.变量名</li>
</ul>
<p>记住，无论是 @classmethon 还是 cls ,都是不能省去的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">()</span>:</span></span><br><span class="line">    name=<span class="string">"lc"</span></span><br><span class="line">    age=<span class="number">18</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_name</span><span class="params">(cls)</span>:</span></span><br><span class="line">        print(<span class="string">"name is "</span>+cls.name)</span><br><span class="line"></span><br><span class="line">Person.get_name()</span><br></pre></td></tr></table></figure>

<pre><code>name is lc</code></pre><h4 id="类方法传参"><a href="#类方法传参" class="headerlink" title="类方法传参"></a>类方法传参</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">()</span>:</span></span><br><span class="line">    name=<span class="string">"lc"</span></span><br><span class="line">    age=<span class="number">18</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_name</span><span class="params">(cls,str)</span>:</span></span><br><span class="line">        print(<span class="string">"name is "</span>+cls.name+str)</span><br><span class="line"></span><br><span class="line">Person.get_name(<span class="string">"1998"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>name is lc1998</code></pre><h4 id="修改和增加类属性"><a href="#修改和增加类属性" class="headerlink" title="修改和增加类属性"></a>修改和增加类属性</h4><ul>
<li>从内部增加和修改类属性</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">()</span>:</span></span><br><span class="line">    name=<span class="string">"lc"</span></span><br><span class="line">    age=<span class="number">18</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rename</span><span class="params">(cls)</span>:</span></span><br><span class="line">        print(<span class="string">"original name is "</span>+cls.name)</span><br><span class="line">        cls.name=input(<span class="string">'please input your name:'</span>)</span><br><span class="line">        print(<span class="string">"current name is "</span>+cls.name)</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rename_customize</span><span class="params">(cls,str)</span>:</span></span><br><span class="line">        print(<span class="string">"original name is "</span>+cls.name)</span><br><span class="line">        cls.name=str</span><br><span class="line">        print(<span class="string">"current name is "</span>+cls.name)</span><br><span class="line">Person.rename_customize(<span class="string">"lemon"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>original name is lc
current name is lemon</code></pre><ul>
<li>从外部增加和修改类属性</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lc=Person()</span><br><span class="line">print(lc.name)</span><br><span class="line">lc.name=<span class="string">"lc"</span></span><br><span class="line">print(lc.name)</span><br></pre></td></tr></table></figure>

<pre><code>12
lc</code></pre><h3 id="对象的实例化"><a href="#对象的实例化" class="headerlink" title="对象的实例化"></a>对象的实例化</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#属性</span></span><br><span class="line">    name=<span class="string">"lc"</span></span><br><span class="line">    age=<span class="number">18</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#类方法</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rename</span><span class="params">(cls)</span>:</span></span><br><span class="line">        print(<span class="string">"original name is "</span>+cls.name)</span><br><span class="line">        cls.name=input(<span class="string">'please input your name:'</span>)</span><br><span class="line">        print(<span class="string">"current name is "</span>+cls.name)</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rename_customize</span><span class="params">(cls,str)</span>:</span></span><br><span class="line">        print(<span class="string">"original name is "</span>+cls.name)</span><br><span class="line">        cls.name=str</span><br><span class="line">        print(<span class="string">"current name is "</span>+cls.name)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_age</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"age is "</span>+str(self.age))</span><br><span class="line"></span><br><span class="line">lc=Person()</span><br><span class="line">lc.get_age()</span><br></pre></td></tr></table></figure>

<pre><code>age is 18</code></pre><p>只不过使用 cls 和 self 是我们的编程习惯，这也是我们的编程规范。</p>
<p>因为 cls 是 class 的缩写，代表这类 ， 而 self 代表这对象的意思。</p>
<p>所以啊，这里我们实例化对象的时候，就使用 self 。</p>
<p>而且 self 是所有类方法位于首位、默认的特殊参数。</p>
<p>除此之外，在这里，还要强调一个概念，当你把类实例化之后，里面的属性和方法，就不叫类属性和类方法了，改为叫实例</p>
<p>属性和实例方法，也可以叫对象属性和对象方法。</p>
<h4 id="实例属性和类属性"><a href="#实例属性和类属性" class="headerlink" title="实例属性和类属性"></a>实例属性和类属性</h4><ul>
<li>发现类属性改变了，实例属性也会改变，因为我们的实例对象就是根据类来复制出来的，类属性改变了，实例对象的属性也会跟着改变。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lemon=Person()</span><br><span class="line">Person.age=<span class="number">19</span></span><br><span class="line">lemon.get_age()</span><br></pre></td></tr></table></figure>

<pre><code>age is 19</code></pre><ul>
<li>当我们修改实例对象的属性是，类属性是不会改变的,因为每个实例都是单独的个体，不能影响到类的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lemon=Person()</span><br><span class="line">print(Person.age)</span><br><span class="line">lemon.age=<span class="number">19</span></span><br><span class="line">print(Person.age)</span><br></pre></td></tr></table></figure>

<pre><code>18
18</code></pre><h4 id="实例方法和类方法"><a href="#实例方法和类方法" class="headerlink" title="实例方法和类方法"></a>实例方法和类方法</h4><ul>
<li>如果类方法改变了，实例方法会不会跟着改变呢？答案是肯定</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#属性</span></span><br><span class="line">    name=<span class="string">"lc"</span></span><br><span class="line">    age=<span class="number">18</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#类方法</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rename</span><span class="params">(cls)</span>:</span></span><br><span class="line">        print(<span class="string">"original name is "</span>+cls.name)</span><br><span class="line">        cls.name=input(<span class="string">'please input your name:'</span>)</span><br><span class="line">        print(<span class="string">"current name is "</span>+cls.name)</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rename_customize</span><span class="params">(cls,str)</span>:</span></span><br><span class="line">        print(<span class="string">"original name is "</span>+cls.name)</span><br><span class="line">        cls.name=str</span><br><span class="line">        print(<span class="string">"current name is "</span>+cls.name)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_age</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"age is "</span>+str(self.age))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lc=Person()</span><br><span class="line">lc.get_age()</span><br></pre></td></tr></table></figure>

<pre><code>age is 18</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_get_age</span><span class="params">(self)</span>:</span></span><br><span class="line">    print(<span class="string">"age + 1 is "</span>+str(self.age+<span class="number">1</span>))</span><br><span class="line">Person.get_age=new_get_age</span><br><span class="line">lc.get_age()</span><br></pre></td></tr></table></figure>

<pre><code>age + 1 is 19</code></pre><p>在这个例子中，我们需要改变类方法，就用到了<strong>类的重写</strong>。</p>
<p>我们使用了 类.原始函数 = 新函数 就完成类的重写。</p>
<h4 id="要注意的是，这里的赋值是在替换方法，并不是调用函数。所以是不能加上括号的，也就是-类-原始函数-新函数-这个写法是不对的。"><a href="#要注意的是，这里的赋值是在替换方法，并不是调用函数。所以是不能加上括号的，也就是-类-原始函数-新函数-这个写法是不对的。" class="headerlink" title="要注意的是，这里的赋值是在替换方法，并不是调用函数。所以是不能加上括号的，也就是 类.原始函数() = 新函数() 这个写法是不对的。"></a>要注意的是，这里的赋值是在替换方法，并不是调用函数。所以是不能加上括号的，也就是 类.原始函数() = 新函数() 这个写法是不对的。</h4><ul>
<li>那么如果实例方法改变了，类方法会改变吗？答案是肯定不会的</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#属性</span></span><br><span class="line">    name=<span class="string">"lc"</span></span><br><span class="line">    age=<span class="number">18</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#类方法</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rename</span><span class="params">(cls)</span>:</span></span><br><span class="line">        print(<span class="string">"original name is "</span>+cls.name)</span><br><span class="line">        cls.name=input(<span class="string">'please input your name:'</span>)</span><br><span class="line">        print(<span class="string">"current name is "</span>+cls.name)</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rename_customize</span><span class="params">(cls,str)</span>:</span></span><br><span class="line">        print(<span class="string">"original name is "</span>+cls.name)</span><br><span class="line">        cls.name=str</span><br><span class="line">        print(<span class="string">"current name is "</span>+cls.name)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_age</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"age is "</span>+str(self.age))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_name</span><span class="params">(self,str)</span>:</span></span><br><span class="line">        print(<span class="string">"name is "</span>+str)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lc=Person()</span><br><span class="line">lc.get_age()</span><br></pre></td></tr></table></figure>

<pre><code>age is 18</code></pre><h4 id="我们发现也能改写（但是教程上说不可以，它出错的原因是缺少参数self）"><a href="#我们发现也能改写（但是教程上说不可以，它出错的原因是缺少参数self）" class="headerlink" title="我们发现也能改写（但是教程上说不可以，它出错的原因是缺少参数self）"></a>我们发现也能改写（但是教程上说不可以，它出错的原因是缺少参数self）</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">new_get_name</span><span class="params">(str)</span>:</span></span><br><span class="line">        print(<span class="string">"your name is "</span>+str)</span><br><span class="line">lc.get_name=new_get_name</span><br><span class="line">lc.get_name(<span class="string">"leocode"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>your name is leocode</code></pre><h4 id="修改实例方法，只对于这一个实例起作用"><a href="#修改实例方法，只对于这一个实例起作用" class="headerlink" title="修改实例方法，只对于这一个实例起作用"></a>修改实例方法，只对于这一个实例起作用</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">leo=Person()</span><br><span class="line">leo.get_name(<span class="string">"lc"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>name is lc</code></pre><h3 id="初始化函数（也就是构造函数）"><a href="#初始化函数（也就是构造函数）" class="headerlink" title="初始化函数（也就是构造函数）"></a>初始化函数（也就是构造函数）</h3><ul>
<li>初始化函数的意思是，当你创建一个实例的时候，这个函数就会被调用。</li>
<li>当代码在执行 dog=Animals() 的语句时，就自动调用了 _<em>init_</em>(self) 函数。</li>
<li>而这个 _<em>init_</em>(self) 函数就是初始化函数，也叫构造函数</li>
<li>构造函数格式：<code>def __init__(self,[参数1，参数2...]):</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animals</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#初始化函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"初始化成功！"</span>)</span><br><span class="line">        </span><br><span class="line">dog=Animals()</span><br></pre></td></tr></table></figure>

<pre><code>初始化成功！</code></pre><h4 id="构造函数也可以传递参数"><a href="#构造函数也可以传递参数" class="headerlink" title="构造函数也可以传递参数"></a>构造函数也可以传递参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animals</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#初始化函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,str)</span>:</span></span><br><span class="line">        print(<span class="string">"初始化成功！"</span>+str)</span><br><span class="line">        </span><br><span class="line">dog=Animals(<span class="string">"leocode"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>初始化成功！leocode</code></pre><h3 id="析构函数（用于销毁实例）"><a href="#析构函数（用于销毁实例）" class="headerlink" title="析构函数（用于销毁实例）"></a>析构函数（用于销毁实例）</h3><ul>
<li>一个在创建的时候，会调用构造函数，那么理所当然，这个当一个类销毁的时候，就会调用析构函数</li>
<li>格式：<code>def __del__(self,[参数1，参数2...]):</code></li>
<li>使用del调用析构函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animals</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#初始化函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,str)</span>:</span></span><br><span class="line">        print(<span class="string">"初始化成功！"</span>+str)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__del__</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"实例已销毁"</span>)</span><br><span class="line">        </span><br><span class="line">dog=Animals(<span class="string">"leocode"</span>)</span><br><span class="line"><span class="keyword">del</span> dog</span><br></pre></td></tr></table></figure>

<pre><code>初始化成功！leocode
实例已销毁</code></pre><h3 id="类的继承"><a href="#类的继承" class="headerlink" title="类的继承"></a>类的继承</h3><ul>
<li>定义类的继承</li>
</ul>
<p>说到继承，你一定会联想到继承你老爸的家产之类的。</p>
<p>类的继承也是一样。</p>
<p>比如有一个旧类，是可以算平均数的。然后这时候有一个新类，也要用到算平均数，那么这时候我们就可以使用继承的方式。新类继承旧类，这样子新类也就有这个功能了。</p>
<p>通常情况下，我们叫旧类为父类，新类为子类。</p>
<p>在定义类的时候，可以在括号里写继承的类，如果不用继承类的时候，也要写继承 object 类（也可以不写），因为在 Python 中 object 类是一切类的父类。</p>
<p>当然上面的是单继承，Python 也是支持多继承的，具体的语法如下：</p>
<p>多继承有一点需要注意的：若是父类中有相同的方法名，而在子类使用时未指定，python 在圆括号中父类的顺序，从左至右搜索 ， 即方法在子类中未找到时，从左到右查找父类中是否包含方法。</p>
<p>那么继承的子类可以干什么呢？</p>
<p>继承的子类的好处：</p>
<ul>
<li>会继承父类的属性和方法</li>
<li>可以自己定义，覆盖父类的属性和方法</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Animals</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,kind,name)</span>:</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.kind=kind</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_name</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"its name is "</span>+self.name)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eat</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(self.kind+<span class="string">"在吃东西"</span>)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span><span class="params">(Animals)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,kind,name,age)</span>:</span></span><br><span class="line">        super(Dog,self).__init__(kind,name)</span><br><span class="line">        self.age=age</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eat</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(self.kind+<span class="string">"在吃东西,并且它"</span>+str(self.age)+<span class="string">"岁了"</span>)</span><br><span class="line">dog_a=Dog(<span class="string">"dog"</span>,<span class="string">"dz"</span>,<span class="number">2</span>)</span><br><span class="line">dog_a.get_name()</span><br><span class="line">dog_a.eat()</span><br></pre></td></tr></table></figure>

<pre><code>its name is dz
dog在吃东西,并且它2岁了</code></pre><h4 id="子类的类型判断"><a href="#子类的类型判断" class="headerlink" title="子类的类型判断"></a>子类的类型判断</h4><p>对于 class 的继承关系来说，有些时候我们需要判断 class 的类型，该怎么办呢？</p>
<p>可以使用 isinstance() 函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User2</span><span class="params">(User1)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User3</span><span class="params">(User2)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">user1=User1()</span><br><span class="line">user2=User2()</span><br><span class="line">user3=User3()</span><br><span class="line"></span><br><span class="line">print(isinstance(user1,User1))</span><br><span class="line">print(isinstance(user2,User1))</span><br><span class="line">print(isinstance(user3,User2))</span><br><span class="line">print(isinstance(<span class="number">1321</span>,str))</span><br><span class="line">print(isinstance(<span class="number">1321</span>,int))</span><br><span class="line">print(type(user1))</span><br></pre></td></tr></table></figure>

<pre><code>True
True
True
False
True
&lt;class &apos;__main__.User1&apos;&gt;</code></pre><ul>
<li>可以看到 isinstance() 不仅可以告诉我们，一个对象是否是某种类型，也可以用于基本类型的判断</li>
</ul>
<h3 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h3><p>多态的概念其实不难理解，它是指对不同类型的变量进行相同的操作，它会根据对象（或类）类型的不同而表现出不同的行为。</p>
<p>事实上，我们经常用到多态的性质，比如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt;1 + 2</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt;&apos;a&apos; + &apos;b&apos;</span><br><span class="line">&apos;ab&apos;</span><br></pre></td></tr></table></figure>

<p>可以看到，我们对两个整数进行 + 操作，会返回它们的和，对两个字符进行相同的 + 操作，会返回拼接后的字符串。</p>
<p>也就是说，不同类型的对象对同一消息会作出不同的响应。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name,age)</span>:</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.age=age</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">work</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(self.name+<span class="string">"在工作！他现在"</span>+str(self.age)+<span class="string">"岁！"</span>)</span><br><span class="line">        </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name,age)</span>:</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self.age=age</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">work</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">"他现在"</span>+str(self.age)+<span class="string">"岁！"</span>+self.name+<span class="string">"在工作！"</span>)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inter_work</span><span class="params">(user)</span>:</span></span><br><span class="line">    user.work()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lc=User1(<span class="string">"lc"</span>,<span class="number">22</span>)</span><br><span class="line">lemon=User2(<span class="string">"lemon"</span>,<span class="number">21</span>)</span><br><span class="line">inter_work(lc)</span><br><span class="line">inter_work(lemon)</span><br></pre></td></tr></table></figure>

<pre><code>lc在工作！他现在22岁！
他现在21岁！lemon在工作！</code></pre><p>可以看到，lc 和 lemon 是两个不同的对象，对它们调用 inter_work 方法，它们会自动调用实际类型的 work 方法，作出不同的响应。这就是多态的魅力。</p>
<p>要注意喔，有了继承，才有了多态，也会有不同类的对象对同一消息会作出不同的相应。</p>
<h3 id="类的访问控制"><a href="#类的访问控制" class="headerlink" title="类的访问控制"></a>类的访问控制</h3><ul>
<li>类属性的访问控制<br>在 Java 中，有 public （公共）属性 和 private （私有）属性，这可以对属性进行访问控制。</li>
</ul>
<p>那么在 Python 中有没有属性的访问控制呢？</p>
<p>一般情况下，我们会使用 __private_attrs 两个下划线开头，声明该属性为私有，不能在类地外部被使用或直接访问。在类内部的方法中使用时 self.__private_attrs。</p>
<p>为什么只能说一般情况下呢？</p>
<p>因为实际上， Python 中是没有提供私有属性等功能的。</p>
<p>但是 Python 对属性的访问控制是靠程序员自觉的。为什么这么说呢？</p>
<p>看看下面的示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserInfo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,name,age,account)</span>:</span></span><br><span class="line">        self.name=name</span><br><span class="line">        self._age=age</span><br><span class="line">        self.__account=account</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_account</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">return</span> self.__account</span><br><span class="line">        </span><br><span class="line">userInfo=UserInfo(<span class="string">"leocode"</span>,<span class="number">21</span>,<span class="number">199831</span>)</span><br><span class="line"><span class="comment">#输出所有属性</span></span><br><span class="line">print(dir(userInfo))</span><br><span class="line"><span class="comment">#输出构造函数中的属性</span></span><br><span class="line">print(userInfo.__dict__)</span><br><span class="line"><span class="comment">#用于验证双下划线是否是真正的私有属性</span></span><br><span class="line">print(userInfo._UserInfo__account)</span><br><span class="line">print(userInfo._age)</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;_UserInfo__account&apos;, &apos;__class__&apos;, &apos;__delattr__&apos;, &apos;__dict__&apos;, &apos;__dir__&apos;, &apos;__doc__&apos;, &apos;__eq__&apos;, &apos;__format__&apos;, &apos;__ge__&apos;, &apos;__getattribute__&apos;, &apos;__gt__&apos;, &apos;__hash__&apos;, &apos;__init__&apos;, &apos;__init_subclass__&apos;, &apos;__le__&apos;, &apos;__lt__&apos;, &apos;__module__&apos;, &apos;__ne__&apos;, &apos;__new__&apos;, &apos;__reduce__&apos;, &apos;__reduce_ex__&apos;, &apos;__repr__&apos;, &apos;__setattr__&apos;, &apos;__sizeof__&apos;, &apos;__str__&apos;, &apos;__subclasshook__&apos;, &apos;__weakref__&apos;, &apos;_age&apos;, &apos;name&apos;]
{&apos;name&apos;: &apos;leocode&apos;, &apos;_age&apos;: 21, &apos;_UserInfo__account&apos;: 199831}
199831
21</code></pre><h3 id="类专有的方法"><a href="#类专有的方法" class="headerlink" title="类专有的方法"></a>类专有的方法</h3><ul>
<li>一个类创建的时候，就会包含一些方法</li>
</ul>
<h3 id="方法的访问控制"><a href="#方法的访问控制" class="headerlink" title="方法的访问控制"></a>方法的访问控制</h3><ul>
<li>方法也可以看成类的属性（只是看成），用法是一样的</li>
</ul>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>300最长上升子序列</title>
    <url>/ck3bmvctw000ibsg40e1k0ppm.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p> 给定一个无序的整数数组，找到其中最长上升子序列的长度。 </p>
<a id="more"></a>

<blockquote>
<p><strong>示例:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: [<span class="number">10</span>,<span class="number">9</span>,<span class="number">2</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">101</span>,<span class="number">18</span>]</span><br><span class="line">输出: <span class="number">4</span> </span><br><span class="line">解释: 最长的上升子序列是 [<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">101</span>]，它的长度是 <span class="number">4</span>。</span><br></pre></td></tr></table></figure>

<p><strong>说明:</strong></p>
<ul>
<li>可能会有多种最长上升子序列的组合，你只需要输出对应的长度即可。</li>
<li>你算法的时间复杂度应该为 O(<em>n2</em>) 。</li>
</ul>
<p><strong>进阶:</strong> 你能将算法的时间复杂度降低到 O(<em>n</em> log <em>n</em>) 吗?</p>
</blockquote>
<h6 id="解题思路：本题思路不是很难，但是时间复杂度在O-n-2-，想要让时间复杂度降低到-O-n-log-n-，暂时还没有想到，而且使用的已经是动态规划了（记录以当前结点为终止结点的子序列的最长长度），但是我发现其实还是多级算了一些结点，其实并不一定要一个一个遍历，我们只需要找到在当前结点之前最大的（但又是必须要小于他）的结点即可，所以暂时停留在这一步，明天重构一下，用我现在的思路。"><a href="#解题思路：本题思路不是很难，但是时间复杂度在O-n-2-，想要让时间复杂度降低到-O-n-log-n-，暂时还没有想到，而且使用的已经是动态规划了（记录以当前结点为终止结点的子序列的最长长度），但是我发现其实还是多级算了一些结点，其实并不一定要一个一个遍历，我们只需要找到在当前结点之前最大的（但又是必须要小于他）的结点即可，所以暂时停留在这一步，明天重构一下，用我现在的思路。" class="headerlink" title="解题思路：本题思路不是很难，但是时间复杂度在O(n^2)，想要让时间复杂度降低到 O(n log n)，暂时还没有想到，而且使用的已经是动态规划了（记录以当前结点为终止结点的子序列的最长长度），但是我发现其实还是多级算了一些结点，其实并不一定要一个一个遍历，我们只需要找到在当前结点之前最大的（但又是必须要小于他）的结点即可，所以暂时停留在这一步，明天重构一下，用我现在的思路。"></a>解题思路：本题思路不是很难，但是时间复杂度在O(n^2)，想要让时间复杂度降低到 O(<em>n</em> log <em>n</em>)，暂时还没有想到，而且使用的已经是动态规划了（记录以当前结点为终止结点的子序列的最长长度），但是我发现其实还是多级算了一些结点，其实并不一定要一个一个遍历，我们只需要找到在当前结点之前最大的（但又是必须要小于他）的结点即可，所以暂时停留在这一步，明天重构一下，用我现在的思路。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">lengthOfLIS</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> size=nums.size();</span><br><span class="line">        <span class="keyword">if</span>(size==<span class="number">0</span>)<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> max_val=<span class="number">1</span>;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;dp(size,<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;size;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=i<span class="number">-1</span>;j&gt;=<span class="number">0</span>;j--)&#123;</span><br><span class="line">                <span class="keyword">if</span>(nums[j]&lt;nums[i])dp[i]=max(dp[i],dp[j]+<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(max_val&lt;dp[i])max_val=dp[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max_val;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h6 id="法二：其实题目规定将算法时间复杂度降到O-nlogn-这就是在提示我们使用二分法，但是这个二分真的很难想，dp-i-代表的是i-1长度的序列的尾元素的最小值（因为采取最小值，这样后边来一个值，更容易造成上升这个趋势），并且dp数组满足一定是一个递增的数组的条件（这一方法需要再思考）"><a href="#法二：其实题目规定将算法时间复杂度降到O-nlogn-这就是在提示我们使用二分法，但是这个二分真的很难想，dp-i-代表的是i-1长度的序列的尾元素的最小值（因为采取最小值，这样后边来一个值，更容易造成上升这个趋势），并且dp数组满足一定是一个递增的数组的条件（这一方法需要再思考）" class="headerlink" title="法二：其实题目规定将算法时间复杂度降到O(nlogn)这就是在提示我们使用二分法，但是这个二分真的很难想，dp[i]代表的是i+1长度的序列的尾元素的最小值（因为采取最小值，这样后边来一个值，更容易造成上升这个趋势），并且dp数组满足一定是一个递增的数组的条件（这一方法需要再思考）"></a>法二：其实题目规定将算法时间复杂度降到O(nlogn)这就是在提示我们使用二分法，但是这个二分真的很难想，dp[i]代表的是i+1长度的序列的尾元素的最小值（因为采取最小值，这样后边来一个值，更容易造成上升这个趋势），并且dp数组满足一定是一个递增的数组的条件（这一方法需要再思考）</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">lengthOfLIS</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> size=nums.size();</span><br><span class="line">        <span class="keyword">if</span>(size==<span class="number">0</span>)<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> max_val,cur_max,res;</span><br><span class="line">        max_val=<span class="number">1</span>;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;dp(size);</span><br><span class="line">        res=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> num:nums)&#123;</span><br><span class="line">            <span class="keyword">int</span> l,r,m;</span><br><span class="line">            l=<span class="number">0</span>;r=res;</span><br><span class="line">            <span class="keyword">while</span>(l&lt;r)&#123;</span><br><span class="line">                m=(r+l)/<span class="number">2</span>;</span><br><span class="line">                <span class="keyword">if</span>(dp[m]&gt;=num)r=m;</span><br><span class="line">                <span class="keyword">else</span> l=m+<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            dp[l]=num;</span><br><span class="line">            <span class="keyword">if</span>(res==r)res++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>leetcode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>62不同路径</title>
    <url>/ck3bmvcvx001rbsg4fpwjd4un.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>一个机器人位于一个 <em>m x n</em> 网格的左上角 （起始点在下图中标记为“Start” ）。</p>
<p>机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。</p>
<p>问总共有多少条不同的路径？</p>
<a id="more"></a>

<blockquote>
<p><strong>说明：</strong>m 和 <em>n</em> 的值均不超过 100。</p>
<p><strong>示例 1:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: m = <span class="number">3</span>, n = <span class="number">2</span></span><br><span class="line">输出: <span class="number">3</span></span><br><span class="line">解释:</span><br><span class="line">从左上角开始，总共有 <span class="number">3</span> 条路径可以到达右下角。</span><br><span class="line"><span class="number">1.</span> 向右 -&gt; 向右 -&gt; 向下</span><br><span class="line"><span class="number">2.</span> 向右 -&gt; 向下 -&gt; 向右</span><br><span class="line"><span class="number">3.</span> 向下 -&gt; 向右 -&gt; 向右</span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: m = <span class="number">7</span>, n = <span class="number">3</span></span><br><span class="line">输出: <span class="number">28</span></span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="解题思路：本题其实还是蛮简单的，就是模拟一下走的路径，只能向下走和向右走，用一个二维数组模拟即可，注意的是第一行和第一列都是1，代表当前走到这里可能的路径数目（如果只有起点，算一个路径）"><a href="#解题思路：本题其实还是蛮简单的，就是模拟一下走的路径，只能向下走和向右走，用一个二维数组模拟即可，注意的是第一行和第一列都是1，代表当前走到这里可能的路径数目（如果只有起点，算一个路径）" class="headerlink" title="解题思路：本题其实还是蛮简单的，就是模拟一下走的路径，只能向下走和向右走，用一个二维数组模拟即可，注意的是第一行和第一列都是1，代表当前走到这里可能的路径数目（如果只有起点，算一个路径）"></a>解题思路：本题其实还是蛮简单的，就是模拟一下走的路径，只能向下走和向右走，用一个二维数组模拟即可，注意的是第一行和第一列都是1，代表当前走到这里可能的路径数目（如果只有起点，算一个路径）</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">uniquePaths</span><span class="params">(<span class="keyword">int</span> m, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> dp[m][n];</span><br><span class="line">        <span class="keyword">int</span> i,j;</span><br><span class="line">        <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;n;i++)&#123;</span><br><span class="line">            dp[<span class="number">0</span>][i]=<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;m;i++)&#123;</span><br><span class="line">            dp[i][<span class="number">0</span>]=<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;m;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(j=<span class="number">1</span>;j&lt;n;j++)&#123;</span><br><span class="line">                dp[i][j]=dp[i<span class="number">-1</span>][j]+dp[i][j<span class="number">-1</span>];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[m<span class="number">-1</span>][n<span class="number">-1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>55跳跃游戏</title>
    <url>/ck3bmvcvm001kbsg43rwm989u.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>给定一个非负整数数组，你最初位于数组的第一个位置。</p>
<p>数组中的每个元素代表你在该位置可以跳跃的最大长度。</p>
<p>判断你是否能够到达最后一个位置。</p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: [<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">4</span>]</span><br><span class="line">输出: <span class="literal">true</span></span><br><span class="line">解释: 我们可以先跳 <span class="number">1</span> 步，从位置 <span class="number">0</span> 到达 位置 <span class="number">1</span>, 然后再从位置 <span class="number">1</span> 跳 <span class="number">3</span> 步到达最后一个位置。</span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: [<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">4</span>]</span><br><span class="line">输出: <span class="literal">false</span></span><br><span class="line">解释: 无论怎样，你总会到达索引为 <span class="number">3</span> 的位置。但该位置的最大跳跃长度是 <span class="number">0</span> ， 所以你永远不可能到达最后一个位置。</span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="解题思路：本题很想走楼梯那一题，思路其实是类似的，dp-i-代表这当前结点能向后走的步数，那么如果他是0，就代表不能向后走，即返回false，状态转移方程为dp-i-max-nums-i-dp-i-1-1"><a href="#解题思路：本题很想走楼梯那一题，思路其实是类似的，dp-i-代表这当前结点能向后走的步数，那么如果他是0，就代表不能向后走，即返回false，状态转移方程为dp-i-max-nums-i-dp-i-1-1" class="headerlink" title="解题思路：本题很想走楼梯那一题，思路其实是类似的，dp[i]代表这当前结点能向后走的步数，那么如果他是0，就代表不能向后走，即返回false，状态转移方程为dp[i]=max(nums[i],dp[i-1]-1)"></a>解题思路：本题很想走楼梯那一题，思路其实是类似的，dp[i]代表这当前结点能向后走的步数，那么如果他是0，就代表不能向后走，即返回false，状态转移方程为dp[i]=max(nums[i],dp[i-1]-1)</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">canJump</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> size=nums.size();</span><br><span class="line">        <span class="keyword">if</span>(size==<span class="number">1</span>)<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;dp(size);</span><br><span class="line">        dp[<span class="number">0</span>]=nums[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">if</span>(dp[<span class="number">0</span>]==<span class="number">0</span>)<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;size;i++)&#123;</span><br><span class="line">            dp[i]=max(nums[i],dp[i<span class="number">-1</span>]<span class="number">-1</span>);</span><br><span class="line">            <span class="keyword">if</span>(dp[i]==<span class="number">0</span>&amp;&amp;i!=size<span class="number">-1</span>)<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>198打家劫舍</title>
    <url>/ck3bmvcsz0009bsg4aj1eaqvo.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，<strong>如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警</strong>。</p>
<p>给定一个代表每个房屋存放金额的非负整数数组，计算你<strong>在不触动警报装置的情况下，</strong>能够偷窃到的最高金额。</p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>]</span><br><span class="line">输出: <span class="number">4</span></span><br><span class="line">解释: 偷窃 <span class="number">1</span> 号房屋 (金额 = <span class="number">1</span>) ，然后偷窃 <span class="number">3</span> 号房屋 (金额 = <span class="number">3</span>)。</span><br><span class="line">     偷窃到的最高金额 = <span class="number">1</span> + <span class="number">3</span> = <span class="number">4</span> 。</span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: [<span class="number">2</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">3</span>,<span class="number">1</span>]</span><br><span class="line">输出: <span class="number">12</span></span><br><span class="line">解释: 偷窃 <span class="number">1</span> 号房屋 (金额 = <span class="number">2</span>), 偷窃 <span class="number">3</span> 号房屋 (金额 = <span class="number">9</span>)，接着偷窃 <span class="number">5</span> 号房屋 (金额 = <span class="number">1</span>)。</span><br><span class="line">     偷窃到的最高金额 = <span class="number">2</span> + <span class="number">9</span> + <span class="number">1</span> = <span class="number">12</span> 。</span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="解题思路：本题还算是比较简单，仔细分析一下还是能做出来的，状态转移方程dp-i-max-dp-i-2-nums-i-dp-i-3-nums-i-因为要隔着一个，那么就只能考虑这俩个，因为再往前就会影响到其他的房间了（不是相互独立的了），还有就是如果i-3没有的话，就直接选dp-i-2-nums-i-即可。"><a href="#解题思路：本题还算是比较简单，仔细分析一下还是能做出来的，状态转移方程dp-i-max-dp-i-2-nums-i-dp-i-3-nums-i-因为要隔着一个，那么就只能考虑这俩个，因为再往前就会影响到其他的房间了（不是相互独立的了），还有就是如果i-3没有的话，就直接选dp-i-2-nums-i-即可。" class="headerlink" title="解题思路：本题还算是比较简单，仔细分析一下还是能做出来的，状态转移方程dp[i]=max(dp[i-2]+nums[i],dp[i-3]+nums[i]);因为要隔着一个，那么就只能考虑这俩个，因为再往前就会影响到其他的房间了（不是相互独立的了），还有就是如果i-3没有的话，就直接选dp[i-2]+nums[i]即可。"></a>解题思路：本题还算是比较简单，仔细分析一下还是能做出来的，状态转移方程dp[i]=max(dp[i-2]+nums[i],dp[i-3]+nums[i]);因为要隔着一个，那么就只能考虑这俩个，因为再往前就会影响到其他的房间了（不是相互独立的了），还有就是如果i-3没有的话，就直接选dp[i-2]+nums[i]即可。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">rob</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> size=nums.size();</span><br><span class="line">        <span class="keyword">if</span>(size==<span class="number">1</span>)<span class="keyword">return</span> nums[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">if</span>(size==<span class="number">0</span>)<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;dp(size);</span><br><span class="line">        dp[<span class="number">0</span>]=nums[<span class="number">0</span>];</span><br><span class="line">        dp[<span class="number">1</span>]=nums[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">int</span> result=max(dp[<span class="number">0</span>],dp[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>;i&lt;size;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(i<span class="number">-3</span>&lt;<span class="number">0</span>)dp[i]=dp[i<span class="number">-2</span>]+nums[i];</span><br><span class="line">            <span class="keyword">else</span> dp[i]=max(dp[i<span class="number">-2</span>]+nums[i],dp[i<span class="number">-3</span>]+nums[i]);</span><br><span class="line">            result=max(dp[i],result);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>53最大子序和</title>
    <url>/ck3bmvcv9001cbsg41dbi2v2y.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p> 给定一个整数数组 <code>nums</code> ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 </p>
<a id="more"></a>

<blockquote>
<p><strong>示例:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: [-2,1,-3,4,-1,2,1,-5,4],</span><br><span class="line">输出: 6</span><br><span class="line">解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。</span><br></pre></td></tr></table></figure>

<p><strong>进阶:</strong></p>
<p>如果你已经实现复杂度为 O(<em>n</em>) 的解法，尝试使用更为精妙的分治法求解。</p>
</blockquote>
<h6 id="解题思路：本题刚开始我就在纠结于，遍历到当前元素时dp-i-的值，但是发现自己想的有点乱，借鉴了别人的思路，发现很不错，dp-i-代表以当前元素为终止元素的子序和（dp的选值就是max-以当前元素为终止结点的序列和，当前节点值-），因为我们要找的子序列一定是以某个元素为终止元素的，所以这么找一定会找到最终的结果，但是要注意的是，虽然当前元素会只用到前一个节点的dp值，但是最终结点的dp值不一定是最大的（这个要注意，和兑换零钱那种不太一样，要区别一下），最后找出其中最大的值即可。"><a href="#解题思路：本题刚开始我就在纠结于，遍历到当前元素时dp-i-的值，但是发现自己想的有点乱，借鉴了别人的思路，发现很不错，dp-i-代表以当前元素为终止元素的子序和（dp的选值就是max-以当前元素为终止结点的序列和，当前节点值-），因为我们要找的子序列一定是以某个元素为终止元素的，所以这么找一定会找到最终的结果，但是要注意的是，虽然当前元素会只用到前一个节点的dp值，但是最终结点的dp值不一定是最大的（这个要注意，和兑换零钱那种不太一样，要区别一下），最后找出其中最大的值即可。" class="headerlink" title="解题思路：本题刚开始我就在纠结于，遍历到当前元素时dp[i]的值，但是发现自己想的有点乱，借鉴了别人的思路，发现很不错，dp[i]代表以当前元素为终止元素的子序和（dp的选值就是max(以当前元素为终止结点的序列和，当前节点值)），因为我们要找的子序列一定是以某个元素为终止元素的，所以这么找一定会找到最终的结果，但是要注意的是，虽然当前元素会只用到前一个节点的dp值，但是最终结点的dp值不一定是最大的（这个要注意，和兑换零钱那种不太一样，要区别一下），最后找出其中最大的值即可。"></a>解题思路：本题刚开始我就在纠结于，遍历到当前元素时dp[i]的值，但是发现自己想的有点乱，借鉴了别人的思路，发现很不错，dp[i]代表以当前元素为终止元素的子序和（dp的选值就是max(以当前元素为终止结点的序列和，当前节点值)），因为我们要找的子序列一定是以某个元素为终止元素的，所以这么找一定会找到最终的结果，但是要注意的是，虽然当前元素会只用到前一个节点的dp值，但是最终结点的dp值不一定是最大的（这个要注意，和兑换零钱那种不太一样，要区别一下），最后找出其中最大的值即可。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> size=nums.size();</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;dp(size);</span><br><span class="line">        dp[<span class="number">0</span>]=nums[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> result=dp[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;dp.size();i++)&#123;</span><br><span class="line">            dp[i]=max(dp[i<span class="number">-1</span>]+nums[i],nums[i]);</span><br><span class="line">            result=max(dp[i],result);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>121买卖股票的最佳时机</title>
    <url>/ck3bmvcs90002bsg4ew49df6c.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>给定一个数组，它的第 <em>i</em> 个元素是一支给定股票第 <em>i</em> 天的价格。</p>
<p>如果你最多只允许完成一笔交易（即买入和卖出一支股票），设计一个算法来计算你所能获取的最大利润。</p>
<p>注意你不能在买入股票前卖出股票。</p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: [7,1,5,3,6,4]</span><br><span class="line">输出: 5</span><br><span class="line">解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。</span><br><span class="line">     注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格。</span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: [7,6,4,3,1]</span><br><span class="line">输出: 0</span><br><span class="line">解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。</span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="解题思路：可能是刚接触动态规划不久，一开始做这一题的时候，一直找不到状态转移方程（找这个真的不简单）-最终借鉴别人的思想，dp-i-max-前i-1天的最大利润，（当前价格-前i-1天最低的价格-，豁然开朗，找最低的价格，我想到的是使用multiset（因为set插入即有序的，比较方便，但是时间可能会慢一点）。（暴力破解也能过-）"><a href="#解题思路：可能是刚接触动态规划不久，一开始做这一题的时候，一直找不到状态转移方程（找这个真的不简单）-最终借鉴别人的思想，dp-i-max-前i-1天的最大利润，（当前价格-前i-1天最低的价格-，豁然开朗，找最低的价格，我想到的是使用multiset（因为set插入即有序的，比较方便，但是时间可能会慢一点）。（暴力破解也能过-）" class="headerlink" title="解题思路：可能是刚接触动态规划不久，一开始做这一题的时候，一直找不到状态转移方程（找这个真的不简单）,最终借鉴别人的思想，dp[i]=max(前i-1天的最大利润，（当前价格-前i-1天最低的价格))，豁然开朗，找最低的价格，我想到的是使用multiset（因为set插入即有序的，比较方便，但是时间可能会慢一点）。（暴力破解也能过= =）"></a>解题思路：可能是刚接触动态规划不久，一开始做这一题的时候，一直找不到状态转移方程（找这个真的不简单）,最终借鉴别人的思想，dp[i]=max(前i-1天的最大利润，（当前价格-前i-1天最低的价格))，豁然开朗，找最低的价格，我想到的是使用multiset（因为set插入即有序的，比较方便，但是时间可能会慢一点）。（暴力破解也能过= =）</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*class Solution &#123;</span></span><br><span class="line"><span class="comment">public:</span></span><br><span class="line"><span class="comment">    int maxProfit(vector&lt;int&gt;&amp; prices) &#123;</span></span><br><span class="line"><span class="comment">        int max=0;</span></span><br><span class="line"><span class="comment">        int cur_sub;</span></span><br><span class="line"><span class="comment">        for(int j=0;j&lt;prices.size();j++)&#123;</span></span><br><span class="line"><span class="comment">            for(int i=j+1;i&lt;prices.size();i++)&#123;</span></span><br><span class="line"><span class="comment">                if(prices[j]&lt;prices[i])&#123;</span></span><br><span class="line"><span class="comment">                    cur_sub=prices[i]-prices[j];</span></span><br><span class="line"><span class="comment">                    if(max&lt;cur_sub)max=cur_sub;</span></span><br><span class="line"><span class="comment">                &#125;</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        return max;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">&#125;;*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">maxProfit</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; prices)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> size=prices.size();</span><br><span class="line">        <span class="keyword">if</span>(size==<span class="number">0</span>)<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">multiset</span>&lt;<span class="keyword">int</span>&gt;mset;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;dp(size);</span><br><span class="line">        dp[<span class="number">0</span>]=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> cur_sub_pre;</span><br><span class="line">        <span class="built_in">multiset</span>&lt;<span class="keyword">int</span>&gt;::iterator it;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;dp.size();i++)&#123;</span><br><span class="line">            mset.insert(prices[i<span class="number">-1</span>]);</span><br><span class="line">            it=mset.begin();</span><br><span class="line">            cur_sub_pre=prices[i]-*(it);</span><br><span class="line">            dp[i]=max(dp[i<span class="number">-1</span>],cur_sub_pre);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[size<span class="number">-1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>70爬楼梯</title>
    <url>/ck3bmvcw7001ybsg45inmefxj.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>假设你正在爬楼梯。需要 <em>n</em> 阶你才能到达楼顶。每次你可以爬 1 或 2 个台阶。</p>
<p>你有多少种不同的方法可以爬到楼顶呢？</p>
<p><strong>注意：</strong>给定 <em>n</em> 是一个正整数。</p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入： 2</span><br><span class="line">输出： 2</span><br><span class="line">解释： 有两种方法可以爬到楼顶。</span><br><span class="line">1.  1 阶 + 1 阶</span><br><span class="line">2.  2 阶</span><br></pre></td></tr></table></figure>

<p><strong>示例 2：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入： 3</span><br><span class="line">输出： 3</span><br><span class="line">解释： 有三种方法可以爬到楼顶。</span><br><span class="line">1.  1 阶 + 1 阶 + 1 阶</span><br><span class="line">2.  1 阶 + 2 阶</span><br><span class="line">3.  2 阶 + 1 阶</span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="解题思路：这一题刚开始在想，这题和动态规划有关系吗？也没有什么最优子结构什么的啊？细细一想，这不就是斐波那契数吗-（原理很类似），我们这样考虑：假设我们当前处于第四层，那么我们是怎么上来的呢？一共有两种可能，第一：从第三层上来的，第二：从第二层上来的，那么此时我们就会有两种可能，同理，第三层，第二层也都是这么上来的，故我们自底向上（动态规划的思想出现了），第一层只有一种可能，第二层有两种可能，从第三层开始，当前层-n-1-层-n-2-层-（这不就是斐波那契数列的原型吗-，只不过数不一样了）。当然，空间复杂度还可以优化为O-1-，如第二种方法所示。"><a href="#解题思路：这一题刚开始在想，这题和动态规划有关系吗？也没有什么最优子结构什么的啊？细细一想，这不就是斐波那契数吗-（原理很类似），我们这样考虑：假设我们当前处于第四层，那么我们是怎么上来的呢？一共有两种可能，第一：从第三层上来的，第二：从第二层上来的，那么此时我们就会有两种可能，同理，第三层，第二层也都是这么上来的，故我们自底向上（动态规划的思想出现了），第一层只有一种可能，第二层有两种可能，从第三层开始，当前层-n-1-层-n-2-层-（这不就是斐波那契数列的原型吗-，只不过数不一样了）。当然，空间复杂度还可以优化为O-1-，如第二种方法所示。" class="headerlink" title="解题思路：这一题刚开始在想，这题和动态规划有关系吗？也没有什么最优子结构什么的啊？细细一想，这不就是斐波那契数吗= =（原理很类似），我们这样考虑：假设我们当前处于第四层，那么我们是怎么上来的呢？一共有两种可能，第一：从第三层上来的，第二：从第二层上来的，那么此时我们就会有两种可能，同理，第三层，第二层也都是这么上来的，故我们自底向上（动态规划的思想出现了），第一层只有一种可能，第二层有两种可能，从第三层开始，当前层=(n-1)层+(n-2)层,（这不就是斐波那契数列的原型吗= =，只不过数不一样了）。当然，空间复杂度还可以优化为O(1)，如第二种方法所示。"></a>解题思路：这一题刚开始在想，这题和动态规划有关系吗？也没有什么最优子结构什么的啊？细细一想，这不就是斐波那契数吗= =（原理很类似），我们这样考虑：假设我们当前处于第四层，那么我们是怎么上来的呢？一共有两种可能，第一：从第三层上来的，第二：从第二层上来的，那么此时我们就会有两种可能，同理，第三层，第二层也都是这么上来的，故我们自底向上（动态规划的思想出现了），第一层只有一种可能，第二层有两种可能，从第三层开始，当前层=(n-1)层+(n-2)层,（这不就是斐波那契数列的原型吗= =，只不过数不一样了）。当然，空间复杂度还可以优化为O(1)，如第二种方法所示。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">class Solution &#123;</span></span><br><span class="line"><span class="comment">public:</span></span><br><span class="line"><span class="comment">    int climbStairs(int n) &#123;</span></span><br><span class="line"><span class="comment">        vector&lt;int&gt;dp(n+2);</span></span><br><span class="line"><span class="comment">        dp[1]=1;</span></span><br><span class="line"><span class="comment">        dp[2]=2;</span></span><br><span class="line"><span class="comment">        for(int i=3;i&lt;=n;i++)&#123;</span></span><br><span class="line"><span class="comment">            dp[i]=dp[i-1]+dp[i-2];</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        return dp[n];</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">&#125;;*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">climbStairs</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">1</span>)<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> pre=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> cur=<span class="number">2</span>;</span><br><span class="line">        <span class="keyword">int</span> sum;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n<span class="number">-2</span>;i++)&#123;</span><br><span class="line">            sum=cur+pre;</span><br><span class="line">            pre=cur;</span><br><span class="line">            cur=sum;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> cur;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>leetcode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>322零钱兑换</title>
    <url>/ck3bmvcu0000jbsg490y92yh2.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p> 给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 <code>-1</code>。 </p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: coins = [1, 2, 5], amount = 11</span><br><span class="line">输出: 3 </span><br><span class="line">解释: 11 = 5 + 5 + 1</span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: coins = [2], amount = 3</span><br><span class="line">输出: -1</span><br></pre></td></tr></table></figure>

<p><strong>说明</strong>:<br>你可以认为每种硬币的数量是无限的。</p>
</blockquote>
<h6 id="解题思路：这一题是动态规划的入门题目，可以说是很经典的一道题，对于动态规划，关键在于你是否能够写出状态转移方程（能写出来就已经成功了80-），这一题的状态转移方程主要是dp-i-min-dp-i-1-dp-i-coin-其中coin为零钱的面额，最后返回dp-总金额-即可。"><a href="#解题思路：这一题是动态规划的入门题目，可以说是很经典的一道题，对于动态规划，关键在于你是否能够写出状态转移方程（能写出来就已经成功了80-），这一题的状态转移方程主要是dp-i-min-dp-i-1-dp-i-coin-其中coin为零钱的面额，最后返回dp-总金额-即可。" class="headerlink" title="解题思路：这一题是动态规划的入门题目，可以说是很经典的一道题，对于动态规划，关键在于你是否能够写出状态转移方程（能写出来就已经成功了80%），这一题的状态转移方程主要是dp[i]=min(dp[i],1+dp[i-coin])其中coin为零钱的面额，最后返回dp[总金额]即可。"></a>解题思路：这一题是动态规划的入门题目，可以说是很经典的一道题，对于动态规划，关键在于你是否能够写出状态转移方程（能写出来就已经成功了80%），这一题的状态转移方程主要是dp[i]=min(dp[i],1+dp[i-coin])其中coin为零钱的面额，最后返回dp[总金额]即可。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">class Solution &#123;</span></span><br><span class="line"><span class="comment">public:</span></span><br><span class="line"><span class="comment">    int coinChange(vector&lt;int&gt;&amp; coins, int amount) &#123;</span></span><br><span class="line"><span class="comment">        vector&lt;int&gt;dp(amount+1,amount+1);</span></span><br><span class="line"><span class="comment">        dp[0]=0;</span></span><br><span class="line"><span class="comment">        for(int i=0;i&lt;=amount;i++)&#123;</span></span><br><span class="line"><span class="comment">            for(int coin:coins)&#123;</span></span><br><span class="line"><span class="comment">                if(i&lt;coin)continue;</span></span><br><span class="line"><span class="comment">                dp[i]=min(dp[i],1+dp[i-coin]);</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        return (dp[amount]==amount+1)?-1:dp[amount];</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">&#125;;*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">coinChange</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; coins, <span class="keyword">int</span> amount)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;memo(amount+<span class="number">1</span>,<span class="number">-2</span>);</span><br><span class="line">        <span class="keyword">return</span> helper(coins,amount,memo);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">helper</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; coins, <span class="keyword">int</span> amount,<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; memo)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(amount==<span class="number">0</span>)<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(memo[amount]!=<span class="number">-2</span>)<span class="keyword">return</span> memo[amount];</span><br><span class="line">        <span class="keyword">int</span> ans=__INT_MAX__;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> coin:coins)&#123;</span><br><span class="line">            <span class="keyword">if</span>(amount&lt;coin)<span class="keyword">continue</span>;</span><br><span class="line">            <span class="keyword">int</span> sub_curcoin=helper(coins,amount-coin,memo);</span><br><span class="line">            <span class="keyword">if</span>(sub_curcoin==<span class="number">-1</span>)<span class="keyword">continue</span>;</span><br><span class="line">            ans=min(ans,<span class="number">1</span>+sub_curcoin);</span><br><span class="line">        &#125;</span><br><span class="line">        memo[amount]=(ans==__INT_MAX__)?<span class="number">-1</span>:ans;</span><br><span class="line">        <span class="keyword">return</span> memo[amount];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划</title>
    <url>/ck3bmvcxy003ebsg43oqi5rvf.html</url>
    <content><![CDATA[<p>最近在知乎上看到了一篇讲动态规划的文章，感觉还可以，比较好理解。</p>
<p>出于对作者的尊重，特别感谢作者，其知乎： <a href="https://www.zhihu.com/search?type=content&q=动态规划" target="_blank" rel="noopener">https://www.zhihu.com/search?type=content&amp;q=%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92</a> </p>
<p>动态规划算法似乎是一种很高深莫测的算法，你会在一些面试或算法书籍的高级技巧部分看到相关内容，什么状态转移方程，重叠子问题，最优子结构等高大上的词汇也可能让你望而却步。</p>
<a id="more"></a>

<p>而且，当你去看用动态规划解决某个问题的代码时，你会觉得这样解决问题竟然如此巧妙，但却难以理解，你可能惊讶于人家是怎么想到这种解法的。</p>
<p>实际上，动态规划是一种常见的「算法设计技巧」，并没有什么高深莫测，至于各种高大上的术语，那是吓唬别人用的，只要你亲自体验几把，这些名词的含义其实显而易见，再简单不过了。</p>
<p>至于为什么最终的解法看起来如此精妙，是因为动态规划遵循一套固定的流程：<strong>递归的暴力解法 -&gt; 带备忘录的递归解法 -&gt; 非递归的动态规划解法</strong>。这个过程是层层递进的解决问题的过程，你如果没有前面的铺垫，直接看最终的非递归动态规划解法，当然会觉得牛逼而不可及了。</p>
<p>当然，见的多了，思考多了，是可以一步写出非递归的动态规划解法的。任何技巧都需要练习，我们先遵循这个流程走，算法设计也就这些套路，除此之外，真的没啥高深的。</p>
<p>以下，先通过两个个比较简单的例子：斐波那契和凑零钱问题，揭开动态规划的神秘面纱，描述上述三个流程。后续还会写几篇文章探讨如何使用动态规划技巧解决比较复杂的经典问题。</p>
<p>首先，第一个快被举烂了的例子，斐波那契数列。<strong>请读者不要嫌弃这个例子简单，因为简单的例子才能让你把精力充分集中在算法背后的通用思想和技巧上</strong>，而不会被那些隐晦的细节问题搞的莫名其妙。后续，困难的例子有的是。</p>
<p> <strong>步骤一、暴力的递归算法</strong> </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fib</span><span class="params">(<span class="keyword">int</span> N)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (N == <span class="number">1</span> || N == <span class="number">2</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> fib(N - <span class="number">1</span>) + fib(N - <span class="number">2</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个不用多说了，学校老师讲递归的时候似乎都是拿这个举例。我们也知道这样写代码虽然简洁易懂，但是十分低效，低效在哪里？假设 n = 20，请画出递归树。</p>
<p>PS：但凡遇到需要递归的问题，最好都画出递归树，这对你分析算法的复杂度，寻找算法低效的原因都有巨大帮助。</p>
<img src="/ck3bmvcxy003ebsg43oqi5rvf/1.jpg" class="">

<p>这个递归树怎么理解？就是说想要计算原问题 f(20)，我就得先计算出子问题 f(19) 和 f(18)，然后要计算 f(19)，我就要先算出子问题 f(18) 和 f(17)，以此类推。最后遇到 f(1) 或者 f(2) 的时候，结果已知，就能直接返回结果，递归树不再向下生长了。</p>
<p> <strong>递归算法的时间复杂度怎么计算？子问题个数乘以解决一个子问题需要的时间。</strong> </p>
<p>子问题个数，即递归树中节点的总数。显然二叉树节点总数为指数级别，所以子问题个数为 O(2^n)。</p>
<p>解决一个子问题的时间，在本算法中，没有循环，只有 f(n - 1) + f(n - 2) 一个加法操作，时间为 O(1)。</p>
<p>所以，这个算法的时间复杂度为 O(2^n)，指数级别，爆炸。</p>
<p>观察递归树，很明显发现了算法低效的原因：存在大量重复计算，比如 f(18) 被计算了两次，而且你可以看到，以 f(18) 为根的这个递归树体量巨大，多算一遍，会耗费巨大的时间。更何况，还不止 f(18) 这一个节点被重复计算，所以这个算法极其低效。</p>
<p> 这就是动态规划问题的第一个性质：<strong>重叠子问题</strong>。下面，我们想办法解决这个问题。 </p>
<p><strong>步骤二、带备忘录的递归解法</strong></p>
<p>明确了问题，其实就已经把问题解决了一半。即然耗时的原因是重复计算，那么我们可以造一个「备忘录」，每次算出某个子问题的答案后别急着返回，先记到「备忘录」里再返回；每次遇到一个子问题先去「备忘录」里查一查，如果发现之前已经解决过这个问题了，直接把答案拿出来用，不要再耗时去计算了。</p>
<p>一般使用一个数组充当这个「备忘录」，当然你也可以使用哈希表（字典），思想都是一样的。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fib</span><span class="params">(<span class="keyword">int</span> N)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (N &lt; <span class="number">1</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 备忘录全初始化为 0</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; memo(N + <span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="comment">// 初始化最简情况</span></span><br><span class="line">    memo[<span class="number">1</span>] = memo[<span class="number">2</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> helper(memo, N);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">helper</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; memo, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 未被计算过</span></span><br><span class="line">    <span class="keyword">if</span> (n &gt; <span class="number">0</span> &amp;&amp; memo[n] == <span class="number">0</span>) </span><br><span class="line">        memo[n] = helper(memo, n - <span class="number">1</span>) + </span><br><span class="line">                  helper(memo, n - <span class="number">2</span>);</span><br><span class="line">    <span class="keyword">return</span> memo[n];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 现在，画出递归树，你就知道「备忘录」到底做了什么。 </p>
<img src="/ck3bmvcxy003ebsg43oqi5rvf/2.jpg" class="">

<p>实际上，带「备忘录」的递归算法，把一棵存在巨量冗余的递归树通过「剪枝」，改造成了一幅不存在冗余的递归图，极大减少了子问题（即递归图中节点）的个数。</p>
<p>递归算法的时间复杂度怎么算？子问题个数乘以解决一个子问题需要的时间。</p>
<p>子问题个数，即图中节点的总数，由于本算法不存在冗余计算，子问题就是 f(1), f(2), f(3) … f(20)，数量和输入规模 n = 20 成正比，所以子问题个数为 O(n)。</p>
<p>解决一个子问题的时间，同上，没有什么循环，时间为 O(1)。</p>
<p>所以，本算法的时间复杂度是 O(n)。比起暴力算法，是降维打击。</p>
<p>至此，带备忘录的递归解法的效率已经和动态规划一样了。实际上，这种解法和动态规划的思想已经差不多了，只不过这种方法叫做「自顶向下」，动态规划叫做「自底向上」。</p>
<p>啥叫「自顶向下」？注意我们刚才画的递归树（或者说图），是从上向下延伸，都是从一个规模较大的原问题比如说 f(20)，向下逐渐分解规模，直到 f(1) 和 f(2) 触底，然后逐层返回答案，这就叫「自顶向下」。</p>
<p>啥叫「自底向上」？反过来，我们直接从最底下，最简单，问题规模最小的 f(1) 和 f(2) 开始往上推，直到推到我们想要的答案 f(20)，这就是动态规划的思路，这也是为什么动态规划一般都脱离了递归，而是由循环迭代完成计算。</p>
<p> <strong>步骤三、动态规划</strong> </p>
<p> 有了上一步「备忘录」的启发，我们可以把这个「备忘录」独立出来成为一张表，就叫做 DP table 吧，在这张表上完成「自底向上」的推算岂不美哉！ </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fib</span><span class="params">(<span class="keyword">int</span> N)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; dp(N + <span class="number">1</span>, <span class="number">0</span>);</span><br><span class="line">    dp[<span class="number">1</span>] = dp[<span class="number">2</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">3</span>; i &lt;= N; i++)</span><br><span class="line">        dp[i] = dp[i - <span class="number">1</span>] + dp[i - <span class="number">2</span>];</span><br><span class="line">    <span class="keyword">return</span> dp[N];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<img src="/ck3bmvcxy003ebsg43oqi5rvf/3.jpg" class="">

<p>画个图就很好理解了，而且你发现这个 DP table 特别像之前那个「剪枝」后的结果，只是反过来算而已。实际上，带备忘录的递归解法中的「备忘录」，最终完成后就是这个 DP table，所以说这两种解法其实是差不多的，大部分情况下，效率也基本相同。</p>
<p>这里，引出「动态转移方程」这个名词，实际上就是描述问题结构的数学形式：</p>
<img src="/ck3bmvcxy003ebsg43oqi5rvf/4.jpg" class="">

<p>为啥叫「状态转移方程」？为了听起来高端。你把 f(n) 想做一个状态 n，这个状态 n 是由状态 n - 1 和状态 n - 2 相加转移而来，这就叫状态转移，仅此而已。</p>
<p>你会发现，上面的几种解法中的所有操作，例如 return f(n - 1) + f(n - 2)，dp[i] = dp[i - 1] + dp[i - 2]，以及对备忘录或 DP table 的初始化操作，都是围绕这个方程式的不同表现形式。可见列出「状态转移方程」的重要性，它是解决问题的核心。很容易发现，其实状态转移方程直接代表着暴力解法。</p>
<p><strong>千万不要看不起暴力解，动态规划问题最困难的就是写出状态转移方程</strong>，即这个暴力解。优化方法无非是用备忘录或者 DP table，再无奥妙可言。</p>
<p>这个例子的最后，讲一个细节优化。细心的读者会发现，根据斐波那契数列的状态转移方程，当前状态只和之前的两个状态有关，其实并不需要那么长的一个 DP table 来存储所有的状态，只要想办法存储之前的两个状态就行了。所以，可以进一步优化，把空间复杂度降为 O(1)：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fib</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt; <span class="number">2</span>) <span class="keyword">return</span> n;</span><br><span class="line">    <span class="keyword">int</span> prev = <span class="number">0</span>, curr = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n - <span class="number">1</span>; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> sum = prev + curr;</span><br><span class="line">        prev = curr;</span><br><span class="line">        curr = sum;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> curr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>有人会问，动态规划的另一个重要特性「最优子结构」，怎么没有涉及？下面会涉及。斐波那契数列的例子严格来说不算动态规划，以上旨在演示算法设计螺旋上升的过程。当问题中要求求一个最优解或在代码中看到循环和 max、min 等函数时，十有八九，需要动态规划大显身手。</p>
<p>下面，看第二个例子，凑零钱问题，有了上面的详细铺垫，这个问题会很快解决。</p>
<p>题目：给你 k 种面值的硬币，面值分别为 c1, c2 … ck，再给一个总金额 n，问你最少需要几枚硬币凑出这个金额，如果不可能凑出，则回答 -1 。</p>
<p>比如说，k = 3，面值分别为 1，2，5，总金额 n = 11，那么最少需要 3 枚硬币，即 11 = 5 + 5 + 1 。下面走流程。</p>
<p><strong>一、暴力解法</strong></p>
<p>首先是最困难的一步，写出状态转移方程，这个问题比较好写：</p>
<img src="/ck3bmvcxy003ebsg43oqi5rvf/5.jpg" class="">

<p>其实，这个方程就用到了<strong>「最优子结构」性质：原问题的解由子问题的最优解构成。</strong>即 f(11) 由 f(10), f(9), f(6) 的最优解转移而来。</p>
<p>记住，<strong>要符合「最优子结构」，子问题间必须互相独立。</strong>啥叫相互独立？你肯定不想看数学证明，我用一个直观的例子来讲解。</p>
<p>比如说，你的原问题是考出最高的总成绩，那么你的子问题就是要把语文考到最高，数学考到最高…… 为了每门课考到最高，你要把每门课相应的选择题分数拿到最高，填空题分数拿到最高…… 当然，最终就是你每门课都是满分，这就是最高的总成绩。</p>
<p>得到了正确的结果：最高的总成绩就是总分。因为这个过程符合最优子结构，“每门科目考到最高”这些子问题是互相独立，互不干扰的。</p>
<p>但是，如果加一个条件：你的语文成绩和数学成绩会互相制约，此消彼长。这样的话，显然你能考到的最高总成绩就达不到总分了，按刚才那个思路就会得到错误的结果。因为子问题并不独立，语文数学成绩无法同时最优，所以最优子结构被破坏。</p>
<p>回到凑零钱问题，显然子问题之间没有相互制约，而是互相独立的。所以这个状态转移方程是可以得到正确答案的。</p>
<p>之后就没啥难点了，按照方程写暴力递归算法即可。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">coinChange</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; coins, <span class="keyword">int</span> amount)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (amount == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> ans = INT_MAX;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> coin : coins) &#123;</span><br><span class="line">        <span class="comment">// 金额不可达</span></span><br><span class="line">        <span class="keyword">if</span> (amount - coin &lt; <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">int</span> subProb = coinChange(coins, amount - coin);</span><br><span class="line">        <span class="comment">// 子问题无解</span></span><br><span class="line">        <span class="keyword">if</span> (subProb == <span class="number">-1</span>) <span class="keyword">continue</span>;</span><br><span class="line">        ans = min(ans, subProb + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans == INT_MAX ? <span class="number">-1</span> : ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 画出递归树： </p>
<img src="/ck3bmvcxy003ebsg43oqi5rvf/6.jpg" class="">

<p> 时间复杂度分析：子问题总数 x 每个子问题的时间。子问题总数为递归树节点个数，这个比较难看出来，是 O(n^k)，总之是指数级别的。每个子问题中含有一个 for 循环，复杂度为 O(k)。所以总时间复杂度为 O(k*n^k)，指数级别。 </p>
<p><strong>二、带备忘录的递归算法</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">coinChange</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; coins, <span class="keyword">int</span> amount)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 备忘录初始化为 -2</span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; memo(amount + <span class="number">1</span>, <span class="number">-2</span>);</span><br><span class="line">    <span class="keyword">return</span> helper(coins, amount, memo);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">helper</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; coins, <span class="keyword">int</span> amount, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; memo)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (amount == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (memo[amount] != <span class="number">-2</span>) <span class="keyword">return</span> memo[amount];</span><br><span class="line">    <span class="keyword">int</span> ans = INT_MAX;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> coin : coins) &#123;</span><br><span class="line">        <span class="comment">// 金额不可达</span></span><br><span class="line">        <span class="keyword">if</span> (amount - coin &lt; <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">int</span> subProb = helper(coins, amount - coin, memo);</span><br><span class="line">        <span class="comment">// 子问题无解</span></span><br><span class="line">        <span class="keyword">if</span> (subProb == <span class="number">-1</span>) <span class="keyword">continue</span>;</span><br><span class="line">        ans = min(ans, subProb + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 记录本轮答案</span></span><br><span class="line">    memo[amount] = (ans == INT_MAX) ? <span class="number">-1</span> : ans;</span><br><span class="line">    <span class="keyword">return</span> memo[amount];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 不画图了，很显然「备忘录」大大减小了子问题数目，完全消除了子问题的冗余，所以子问题总数不会超过金额数 n，即子问题数目为 O(n)。处理一个子问题的时间不变，仍是 O(k)，所以总的时间复杂度是 O(kn)。 </p>
<p> <strong>三、动态规划</strong> </p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">coinChange</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; coins, <span class="keyword">int</span> amount)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; dp(amount + <span class="number">1</span>, amount + <span class="number">1</span>);</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dp.size(); i++) &#123;</span><br><span class="line">        <span class="comment">// 内层 for 在求所有子问题 + 1 的最小值</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> coin : coins) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i - coin &lt; <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">            dp[i] = min(dp[i], <span class="number">1</span> + dp[i - coin]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (dp[amount] == amount + <span class="number">1</span>) ? <span class="number">-1</span> : dp[amount];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<img src="/ck3bmvcxy003ebsg43oqi5rvf/7.jpg" class="">

<p><strong>最后总结</strong></p>
<p>如果你不太了解动态规划，还能看到这里，真得给你鼓掌，相信你已经掌握了这个算法的设计技巧。</p>
<p>计算机解决问题其实没有任何奇技淫巧，它唯一的解决办法就是穷举，穷举所有可能性。算法设计无非就是先思考“如何穷举”，然后再追求“如何聪明地穷举”。</p>
<p>列出动态转移方程，就是在解决“如何穷举”的问题。之所以说它难，一是因为很多穷举需要递归实现，二是因为有的问题本身的解空间复杂，不那么容易穷举完整。</p>
<p>备忘录、DP table 就是在追求“如何聪明地穷举”。用空间换时间的思路，是降低时间复杂度的不二法门，除此之外，试问，还能玩出啥花活？</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch_Day2</title>
    <url>/ck3bmvcwz002fbsg4bt4a7pep.html</url>
    <content><![CDATA[<h2 id="Autograd-自动求导机制"><a href="#Autograd-自动求导机制" class="headerlink" title="Autograd: 自动求导机制"></a>Autograd: 自动求导机制</h2><p>PyTorch 中所有神经网络的核心是 autograd 包。 我们先简单介绍一下这个包，然后训练第一个简单的神经网络。<br>autograd包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。</p>
<h3 id="张量Tensor"><a href="#张量Tensor" class="headerlink" title="张量Tensor"></a>张量Tensor</h3><ul>
<li>torch.Tensor 是包的核心类。如果将其属性 .requires_grad 设置为 True，则会开始跟踪针对 tensor 的所有操作。完成计算后，您可以调用 .backward() 来自动计算所有梯度。该张量的梯度将累积到 .grad 属性中。</li>
<li>要停止 tensor 历史记录的跟踪，您可以调用 .detach()，它将其与计算历史记录分离，并防止将来的计算被跟踪。</li>
<li>要停止跟踪历史记录（和使用内存），您还可以将代码块使用 with torch.no_grad(): 包装起来。在评估模型时，这是特别有用，因为模型在训练阶段具有 requires_grad = True 的可训练参数有利于调参，但在评估阶段我们不需要梯度。</li>
<li>还有一个类对于 autograd 实现非常重要那就是 Function。Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则grad_fn 是 None ）。</li>
<li>如果你想计算导数，你可以调用 Tensor.backward()。如果 Tensor 是标量（即它包含一个元素数据），则不需要指定任何参数backward()，但是如果它有更多元素，则需要指定一个gradient 参数来指定张量的形状。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

<h4 id="创建一个tensor张量，并且设置requires-grad-True用来追踪他的计算历史"><a href="#创建一个tensor张量，并且设置requires-grad-True用来追踪他的计算历史" class="headerlink" title="创建一个tensor张量，并且设置requires_grad=True用来追踪他的计算历史"></a>创建一个tensor张量，并且设置requires_grad=True用来追踪他的计算历史</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.ones(<span class="number">2</span>,<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1.],
        [1., 1.]], requires_grad=True)</code></pre><h4 id="结果y已经被计算出来了，所以，grad-fn已经被自动生成了"><a href="#结果y已经被计算出来了，所以，grad-fn已经被自动生成了" class="headerlink" title="结果y已经被计算出来了，所以，grad_fn已经被自动生成了"></a>结果y已经被计算出来了，所以，grad_fn已经被自动生成了</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y=x+<span class="number">2</span></span><br><span class="line">print(y)</span><br><span class="line">print(y .grad_fn)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[3., 3.],
        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)
&lt;AddBackward0 object at 0x000001A29043E080&gt;</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z=y*y*<span class="number">3</span></span><br><span class="line"><span class="comment">#mean() 返回数组的算术平均值</span></span><br><span class="line">out=z.mean()</span><br><span class="line">print(z,out)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[27., 27.],
        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) tensor(27., grad_fn=&lt;MeanBackward0&gt;)</code></pre><h4 id="requires-grad-…-可以改变现有张量的-requires-grad属性。-如果没有指定的话，默认输入的flag是-False"><a href="#requires-grad-…-可以改变现有张量的-requires-grad属性。-如果没有指定的话，默认输入的flag是-False" class="headerlink" title=".requires_grad_( … ) 可以改变现有张量的 requires_grad属性。 如果没有指定的话，默认输入的flag是 False"></a>.requires_grad_( … ) 可以改变现有张量的 requires_grad属性。 如果没有指定的话，默认输入的flag是 False</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.randn(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">a=(a*<span class="number">3</span>)/(a<span class="number">-1</span>)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b=(a*a).sum()</span><br><span class="line">print(b.grad_fn)</span><br></pre></td></tr></table></figure>

<pre><code>False
True
&lt;SumBackward0 object at 0x000001A29097A748&gt;</code></pre><h3 id="梯度Gradient"><a href="#梯度Gradient" class="headerlink" title="梯度Gradient"></a>梯度Gradient</h3><p>反向传播，因为上文中的out是一个纯量(scalar)，out.backward()等于out.backwad(torch.tensor(1))</p>
<ul>
<li>x梯度计算不是很理解原理</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#out.backward()       #只能使用一次 </span></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])</code></pre><h4 id="现在让我们看一个雅可比向量积的例子：-不理解"><a href="#现在让我们看一个雅可比向量积的例子：-不理解" class="headerlink" title="现在让我们看一个雅可比向量积的例子：(不理解= =)"></a>现在让我们看一个雅可比向量积的例子：(不理解= =)</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.randn(<span class="number">3</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y=x*<span class="number">2</span></span><br><span class="line"><span class="comment">#torch.norm是对输入的Tensor求范数</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm()&lt;<span class="number">1000</span>:</span><br><span class="line">    y=y*<span class="number">2</span></span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([-340.3439, 1044.5084, -227.5869], grad_fn=&lt;MulBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gradients=torch.tensor([<span class="number">0.1</span>,<span class="number">1.0</span>,<span class="number">0.0001</span>],dtype=torch.float)</span><br><span class="line"><span class="comment">#y.backward(gradients)</span></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])</code></pre><h4 id="如果-requires-grad-True但是你又不希望进行autograd的计算，-那么可以将变量包裹在-with-torch-no-grad-中"><a href="#如果-requires-grad-True但是你又不希望进行autograd的计算，-那么可以将变量包裹在-with-torch-no-grad-中" class="headerlink" title="如果.requires_grad=True但是你又不希望进行autograd的计算， 那么可以将变量包裹在 with torch.no_grad()中:"></a>如果.requires_grad=True但是你又不希望进行autograd的计算， 那么可以将变量包裹在 with torch.no_grad()中:</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x.requires_grad)</span><br><span class="line">print((x**<span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    print((x**<span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure>

<pre><code>True
True
False</code></pre><h2 id="神经网络Neural-Networks"><a href="#神经网络Neural-Networks" class="headerlink" title="神经网络Neural Networks"></a>神经网络Neural Networks</h2><p>神经网络可以通过 torch.nn 包来构建。<br>上一讲已经讲过了autograd，nn包依赖autograd包来定义模型并求导。 一个nn.Module包含各个层和一个forward(input)方法，该方法返回output。<br>例如，看一下数字图片识别的网络：</p>
<img src="/ck3bmvcwz002fbsg4bt4a7pep/1.jpg" class="">
<p>它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。<br>神经网络的典型训练过程如下：</p>
<ul>
<li>定义包含一些可学习的参数(或者叫权重)神经网络模型；</li>
<li>在数据集上迭代；</li>
<li>通过神经网络处理输入；</li>
<li>计算损失(输出结果和正确值的差值大小)；</li>
<li>将梯度反向传播回网络的参数；</li>
<li>更新网络的参数，主要使用如下简单的更新原则： weight = weight - learning_rate * gradient</li>
</ul>
<h3 id="定义一个网络"><a href="#定义一个网络" class="headerlink" title="定义一个网络"></a>定义一个网络</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#继承</span></span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#建立了两个卷积层，self.conv1, self.conv2，注意，这些层都是不包含激活函数的</span></span><br><span class="line">        self.conv1=nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,<span class="number">5</span>)</span><br><span class="line">        self.conv2=nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#三个全连接层</span></span><br><span class="line">        self.fc1=nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>,<span class="number">120</span>)</span><br><span class="line">        self.fc2=nn.Linear(<span class="number">120</span>,<span class="number">84</span>)</span><br><span class="line">        self.fc3=nn.Linear(<span class="number">84</span>,<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="comment">#max_pooling池化操作   2x2的框</span></span><br><span class="line">        x=F.max_pool2d(F.relu(self.conv1(x)),(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">        <span class="comment">#只写一个参数，相当于默认2x2</span></span><br><span class="line">        x=F.max_pool2d(F.relu(self.conv2(x)),<span class="number">2</span>)</span><br><span class="line">        x=x.view(<span class="number">-1</span>,self.num_falt_features(x))</span><br><span class="line">        x=F.relu(self.fc1(x))</span><br><span class="line">        x=F.relu(self.fc2(x))</span><br><span class="line">        x=self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_falt_features</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        size=x.size()[<span class="number">1</span>:]</span><br><span class="line">        num_features=<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features*=s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line">        </span><br><span class="line">net =Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>

<pre><code>Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)</code></pre><ul>
<li>你只需定义forward函数,backward函数(计算梯度)在使用autograd时自动为你创建.你可以在forward函数中使用Tensor的任何操作。</li>
</ul>
<h4 id="net-parameters-返回模型需要学习的参数。"><a href="#net-parameters-返回模型需要学习的参数。" class="headerlink" title="net.parameters()返回模型需要学习的参数。"></a>net.parameters()返回模型需要学习的参数。</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params=list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line">print(params[<span class="number">0</span>].size())</span><br></pre></td></tr></table></figure>

<pre><code>10
torch.Size([6, 1, 5, 5])</code></pre><ul>
<li>为什么是10呢？ 因为不仅有weights，还有bias(偏置)， 10=5*2。</li>
<li>forward的输入和输出都是autograd.Variable.注意:这个网络(LeNet)期望的输入大小是32x32.如果使用MNIST数据集来训练这个网络,请把图片大小重新调整到32x32.</li>
<li>注意，2D卷积层的输入data维数是 batchsize channel height width</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input=torch.randn(<span class="number">1</span>,<span class="number">1</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">out=net(input)</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[-0.0662,  0.0852,  0.0259, -0.0536, -0.0588,  0.1479,  0.1092, -0.1086,
          0.0060, -0.0747]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><h4 id="将所有参数的梯度缓存清零-然后进行随机梯度的的反向传播"><a href="#将所有参数的梯度缓存清零-然后进行随机梯度的的反向传播" class="headerlink" title="将所有参数的梯度缓存清零,然后进行随机梯度的的反向传播."></a>将所有参数的梯度缓存清零,然后进行随机梯度的的反向传播.</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<p>注意</p>
<ul>
<li>torch.nn 只支持小批量输入,整个torch.nn包都只支持小批量样本,而不支持单个样本</li>
<li>例如,nn.Conv2d将接受一个4维的张量,每一维分别是$nSamples\times nChannels\times Height\times Width$(样本数x通道数x高x宽).</li>
<li>如果你有单个样本,只需使用input.unsqueeze(0)来添加其它的维数.</li>
</ul>
<h4 id="在继续之前-我们回顾一下到目前为止见过的所有类"><a href="#在继续之前-我们回顾一下到目前为止见过的所有类" class="headerlink" title="在继续之前,我们回顾一下到目前为止见过的所有类."></a>在继续之前,我们回顾一下到目前为止见过的所有类.</h4><ul>
<li>torch.Tensor-支持自动编程操作（如backward()）的多维数组。 同时保持梯度的张量。</li>
<li>nn.Module-神经网络模块.封装参数,移动到GPU上运行,导出,加载等</li>
<li>nn.Parameter-一种张量,当把它赋值给一个Module时,被自动的注册为参数.</li>
<li>autograd.Function-实现一个自动求导操作的前向和反向定义, 每个张量操作都会创建至少一个Function节点，该节点连接到创建张量并对其历史进行编码的函数。 </li>
</ul>
<h4 id="现在-我们包含了如下内容"><a href="#现在-我们包含了如下内容" class="headerlink" title="现在,我们包含了如下内容:"></a>现在,我们包含了如下内容:</h4><ul>
<li>定义一个神经网络</li>
<li>处理输入和调用backward</li>
</ul>
<h4 id="剩下的内容"><a href="#剩下的内容" class="headerlink" title="剩下的内容:"></a>剩下的内容:</h4><ul>
<li>计算损失值</li>
<li>更新神经网络的权值</li>
</ul>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><ul>
<li>一个损失函数接受一对(output, target)作为输入(output为网络的输出,target为实际值),计算一个值来估计网络的输出和目标值相差多少。</li>
<li>在nn包中有几种不同的损失函数.一个简单的损失函数是:nn.MSELoss,它计算输入和目标之间的均方误差。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out=net(input)</span><br><span class="line"><span class="comment">#一个虚拟的目标（举个例子用的）</span></span><br><span class="line">target=torch.randn(<span class="number">10</span>)</span><br><span class="line">target=target.view(<span class="number">1</span>,<span class="number">-1</span>)</span><br><span class="line">criterion=nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss=criterion(out,target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>

<pre><code>tensor(1.4106, grad_fn=&lt;MseLossBackward&gt;)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(loss.grad_fn)</span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">print(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>&lt;MseLossBackward object at 0x000001A29B005DD8&gt;
&lt;AddmmBackward object at 0x000001A29B005DD8&gt;
&lt;AccumulateGrad object at 0x000001A29B083358&gt;</code></pre><p>现在,你反向跟踪loss,使用它的.grad_fn属性,你会看到向下面这样的一个计算图:</p>
<p>input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss</p>
<p>所以, 当你调用loss.backward(),整个图被区分为损失以及图中所有具有requires_grad = True的张量，并且其.grad 张量的梯度累积。</p>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><ul>
<li><p>为了反向传播误差,我们所需做的是调用loss.backward().你需要清除已存在的梯度,否则梯度将被累加到已存在的梯度。</p>
</li>
<li><p>现在,我们将调用loss.backward(),并查看conv1层的偏置项在反向传播前后的梯度。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line">loss.backward()</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([0., 0., 0., 0., 0., 0.])
tensor([-0.0026,  0.0136, -0.0092, -0.0128, -0.0098, -0.0080])</code></pre><h4 id="更新网络的权重"><a href="#更新网络的权重" class="headerlink" title="更新网络的权重"></a>更新网络的权重</h4><p>实践中最简单的更新规则是随机梯度下降(SGD)．</p>
<ul>
<li>weight=weight−learning_rate∗gradient</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate=<span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data*learning_rate)</span><br></pre></td></tr></table></figure>

<p>然而,当你使用神经网络是,你想要使用各种不同的更新规则,比如SGD,Nesterov-SGD,Adam, RMSPROP等.为了能做到这一点,我们构建了一个包torch.optim实现了所有的这些规则.使用他们非常简单:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment">#自定义优化器</span></span><br><span class="line">optimizer=optim.SGD(net.parameters(),lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#在训练过程中循环</span></span><br><span class="line">optimizer.zero_grad()  <span class="comment">#将梯度缓冲区置0</span></span><br><span class="line">out=net(input)</span><br><span class="line">loss=criterion(out,target)</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment">#更新</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch_Day1</title>
    <url>/ck3bmvcxe002vbsg40u6md762.html</url>
    <content><![CDATA[<p>之前一直在看keras，但是看了之后，给我一种感觉就是：本来深度学习就已经够黑盒了，你再给我封装的这么死，我不想成为调包侠= =，然后Pytorch最近比较流行，在封装性上也没有keras那么死板，所以尝试学习一下。</p>
<a id="more"></a>

<h3 id="PyTorch简介"><a href="#PyTorch简介" class="headerlink" title="PyTorch简介"></a>PyTorch简介</h3><p>PyTorch是一个基于 Python 的科学计算包，主要定位两类人群：</p>
<ul>
<li>NumPy的替代品，可以利用GPU的性能进行计算</li>
<li>深度学习研究平台拥有足够的灵活性和速度</li>
</ul>
<p>要介绍PyTorch之前，不得不说一下Torch。Torch是一个有大量机器学习算法支持的科学计算框架，是一个与Numpy类似的张量（Tensor） 操作库，其特点是特别灵活，但因其采用了小众的编程语言是Lua，所以流行度不高，这也就有了PyTorch的出现。所以其实Torch是 PyTorch的前身，它们的底层语言相同，只是使用了不同的上层包装语言。</p>
<p>PyTorch是一个基于Torch的Python开源机器学习库，用于自然语言处理等应用程序。它主要由Facebookd的人工智能小组开发，不仅能够 实现强大的GPU加速，同时还支持动态神经网络，这一点是现在很多主流框架如TensorFlow都不支持的。 PyTorch提供了两个高级功能： <em>具有强大的GPU加速的张量计算（如Numpy）</em> 包含自动求导系统的深度神经网络。</p>
<p>TensorFlow和Caffe都是命令式的编程语言，而且是静态的，首先必须构建一个神经网络，然后一次又一次使用相同的结构，如果想要改 变网络的结构，就必须从头开始。但是对于PyTorch，通过反向求导技术，可以让你零延迟地任意改变神经网络的行为，而且其实现速度快。正是这一灵活性是PyTorch对比TensorFlow的最大优势。</p>
<p>另外，PyTorch的代码对比TensorFlow而言，更加简洁直观，底层代码也更容易看懂，这对于使用它的人来说理解底层肯定是一件令人激动的事。</p>
<p>所以，总结一下PyTorch的优点： <strong>支持GPU</strong>，灵活，支持动态神经网络 <strong>底层代码易于理解</strong> 命令式体验、 自定义扩展。</p>
<p>当然，现今任何一个深度学习框架都有其缺点，PyTorch也不例外，对比TensorFlow，其全面性处于劣势，目前PyTorch还不支持快速傅里 叶、沿维翻转张量和检查无穷与非数值张量；针对移动端、嵌入式部署以及高性能服务器端的部署其性能表现有待提升；其次因为这个框 架较新，使得他的社区没有那么强大，在文档方面其C库大多数没有文档。 </p>
<h3 id="PyTorch安装"><a href="#PyTorch安装" class="headerlink" title="PyTorch安装"></a>PyTorch安装</h3><p>直接官网下载： <a href="https://pytorch.org" target="_blank" rel="noopener">https://pytorch.org</a>，在anaconda中输入以下命令即可（先查看自己的cudatoolkit是什么版本，选择对应版本）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision cudatoolkit=10.0 -c pytorch</span><br></pre></td></tr></table></figure>

<p>在下载过程中可能由于是网速不快，pytorch又有点大，所以总是下载失败，可以设置一下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda config --<span class="built_in">set</span> remote_read_timeout_secs 600.0</span><br></pre></td></tr></table></figure>

<p>最后检查PyTorch是否在用GPU加速：</p>
<ul>
<li>方法一：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.cuda.is_available()</span><br><span class="line">print(a)</span><br><span class="line">ngpu= <span class="number">1</span></span><br><span class="line"><span class="comment"># Decide which device we want to run on</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">print(device)</span><br><span class="line">print(torch.cuda.get_device_name(<span class="number">0</span>))</span><br><span class="line">print(torch.rand(<span class="number">3</span>,<span class="number">3</span>).cuda())</span><br></pre></td></tr></table></figure>

<p>结果如下即可（每个人显卡不一样，这里是自己的电脑配置）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="literal">True</span></span><br><span class="line">cuda:<span class="number">0</span></span><br><span class="line">GeForce GTX <span class="number">950</span>M</span><br><span class="line">tensor([[<span class="number">0.8102</span>, <span class="number">0.1912</span>, <span class="number">0.2961</span>],</span><br><span class="line">        [<span class="number">0.9527</span>, <span class="number">0.1479</span>, <span class="number">0.8146</span>],</span><br><span class="line">        [<span class="number">0.4283</span>, <span class="number">0.3469</span>, <span class="number">0.3501</span>]], device=<span class="string">'cuda:0'</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>方法二</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure>

<p>输出为<code>True</code>即可</p>
<h3 id="开始学习"><a href="#开始学习" class="headerlink" title="开始学习"></a>开始学习</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure>


<pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.cuda.is_available()</span><br><span class="line">print(a)</span><br><span class="line">ngpu= <span class="number">1</span></span><br><span class="line"><span class="comment"># Decide which device we want to run on</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> (torch.cuda.is_available() <span class="keyword">and</span> ngpu &gt; <span class="number">0</span>) <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">print(device)</span><br><span class="line">print(torch.cuda.get_device_name(<span class="number">0</span>))</span><br><span class="line">print(torch.rand(<span class="number">3</span>,<span class="number">3</span>).cuda())</span><br></pre></td></tr></table></figure>

<pre><code>True
cuda:0
GeForce GTX 950M
tensor([[0.2935, 0.9656, 0.2542],
        [0.3013, 0.2124, 0.9071],
        [0.5502, 0.8500, 0.2255]], device=&apos;cuda:0&apos;)</code></pre><h4 id="Tensors张量，类似于Numpy里的ndarrays，同时Tensors可以使用GPU进行计算"><a href="#Tensors张量，类似于Numpy里的ndarrays，同时Tensors可以使用GPU进行计算" class="headerlink" title="Tensors张量，类似于Numpy里的ndarrays，同时Tensors可以使用GPU进行计算"></a>Tensors张量，类似于Numpy里的ndarrays，同时Tensors可以使用GPU进行计算</h4><ul>
<li>在开头加上from _<em>future_</em> import print_function这句之后，即使在python2.X，使用print就得像python3.X那样加括号使用python2.X中print不需要括号，而在python3.X中则需要。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br></pre></td></tr></table></figure>

<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><h4 id="构造一个5x3的矩阵，不初始化"><a href="#构造一个5x3的矩阵，不初始化" class="headerlink" title="构造一个5x3的矩阵，不初始化"></a>构造一个5x3的矩阵，不初始化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.empty(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[9.2755e-39, 8.4490e-39, 1.0286e-38],
        [1.0102e-38, 1.0837e-38, 1.0286e-38],
        [1.0653e-38, 1.0194e-38, 4.1328e-39],
        [4.2245e-39, 4.2245e-39, 4.2245e-39],
        [4.9592e-39, 9.1836e-39, 1.0561e-38]])</code></pre><h4 id="构造一个随机初始化的矩阵"><a href="#构造一个随机初始化的矩阵" class="headerlink" title="构造一个随机初始化的矩阵"></a>构造一个随机初始化的矩阵</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.rand(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0.9536, 0.5859, 0.3502],
        [0.2301, 0.2587, 0.7727],
        [0.3513, 0.1247, 0.0136],
        [0.6168, 0.2126, 0.5314],
        [0.8289, 0.0799, 0.9224]])</code></pre><h4 id="构造一个矩阵全为0，且数据类型为long"><a href="#构造一个矩阵全为0，且数据类型为long" class="headerlink" title="构造一个矩阵全为0，且数据类型为long"></a>构造一个矩阵全为0，且数据类型为long</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.zeros(<span class="number">5</span>,<span class="number">3</span>,dtype=torch.long)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])</code></pre><h4 id="构造一个张量，直接使用设定的数据"><a href="#构造一个张量，直接使用设定的数据" class="headerlink" title="构造一个张量，直接使用设定的数据"></a>构造一个张量，直接使用设定的数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">5.5</span>,<span class="number">3</span>])</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([5.5000, 3.0000])</code></pre><h4 id="创建一个张量（Tensor）基于已经存在的张量-这些方法将重用输入张量的属性，例如，-dtype，除非设置新的值进行覆盖"><a href="#创建一个张量（Tensor）基于已经存在的张量-这些方法将重用输入张量的属性，例如，-dtype，除非设置新的值进行覆盖" class="headerlink" title="创建一个张量（Tensor）基于已经存在的张量,这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖"></a>创建一个张量（Tensor）基于已经存在的张量,这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖</h4><ul>
<li>new_*方法来创建对象</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=x.new_ones(<span class="number">5</span>,<span class="number">3</span>,dtype=torch.double)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)</code></pre><ul>
<li>覆盖了dtype，但是对象的size还是相同的，只是值和类型发生了变化<ul>
<li>使用size方法与Numpy的shape属性返回的相同，张量也支持shape属性，后面会详细介绍</li>
<li>torch.Size 返回值是 tuple类型, 所以它支持tuple类型的所有操作.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.randn_like(x,dtype=torch.float)</span><br><span class="line">print(x)</span><br><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.3013,  0.9319, -1.0935],
        [-1.9418,  0.3797,  1.4079],
        [ 0.4928, -0.9644, -0.5722],
        [-0.3585,  0.5682, -0.6689],
        [-0.5044, -0.4755, -1.2299]])
torch.Size([5, 3])</code></pre><h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><ul>
<li>加法操作</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y=torch.rand(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">print(x+y)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.7946,  1.0250, -0.8948],
        [-1.2244,  0.6311,  1.6266],
        [ 0.5959, -0.7320,  0.3882],
        [-0.1263,  0.8522,  0.0948],
        [-0.4211,  0.1972, -0.4704]])</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(torch.add(x,y))</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.7946,  1.0250, -0.8948],
        [-1.2244,  0.6311,  1.6266],
        [ 0.5959, -0.7320,  0.3882],
        [-0.1263,  0.8522,  0.0948],
        [-0.4211,  0.1972, -0.4704]])</code></pre><h4 id="提供输出tensor作为参数"><a href="#提供输出tensor作为参数" class="headerlink" title="提供输出tensor作为参数"></a>提供输出tensor作为参数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">result=torch.empty(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">torch.add(x,y,out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.7946,  1.0250, -0.8948],
        [-1.2244,  0.6311,  1.6266],
        [ 0.5959, -0.7320,  0.3882],
        [-0.1263,  0.8522,  0.0948],
        [-0.4211,  0.1972, -0.4704]])</code></pre><h4 id="任何-以-结尾的操作都会用结果替换原变量-例如-x-copy-y-x-t-都会改变x"><a href="#任何-以-结尾的操作都会用结果替换原变量-例如-x-copy-y-x-t-都会改变x" class="headerlink" title="任何 以 _ 结尾的操作都会用结果替换原变量. 例如: x.copy_(y), x.t_(), 都会改变x."></a>任何 以 _ 结尾的操作都会用结果替换原变量. 例如: x.copy_(y), x.t_(), 都会改变x.</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[ <span class="number">0.7946</span>,  <span class="number">1.0250</span>, <span class="number">-0.8948</span>],</span><br><span class="line">        [<span class="number">-1.2244</span>,  <span class="number">0.6311</span>,  <span class="number">1.6266</span>],</span><br><span class="line">        [ <span class="number">0.5959</span>, <span class="number">-0.7320</span>,  <span class="number">0.3882</span>],</span><br><span class="line">        [<span class="number">-0.1263</span>,  <span class="number">0.8522</span>,  <span class="number">0.0948</span>],</span><br><span class="line">        [<span class="number">-0.4211</span>,  <span class="number">0.1972</span>, <span class="number">-0.4704</span>]])</span><br></pre></td></tr></table></figure>


<h4 id="你可以使用与NumPy索引方式相同的操作来进行对张量的操作"><a href="#你可以使用与NumPy索引方式相同的操作来进行对张量的操作" class="headerlink" title="你可以使用与NumPy索引方式相同的操作来进行对张量的操作"></a>你可以使用与NumPy索引方式相同的操作来进行对张量的操作</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(x)</span><br><span class="line">print(x[:,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[ <span class="number">0.3013</span>,  <span class="number">0.9319</span>, <span class="number">-1.0935</span>],</span><br><span class="line">        [<span class="number">-1.9418</span>,  <span class="number">0.3797</span>,  <span class="number">1.4079</span>],</span><br><span class="line">        [ <span class="number">0.4928</span>, <span class="number">-0.9644</span>, <span class="number">-0.5722</span>],</span><br><span class="line">        [<span class="number">-0.3585</span>,  <span class="number">0.5682</span>, <span class="number">-0.6689</span>],</span><br><span class="line">        [<span class="number">-0.5044</span>, <span class="number">-0.4755</span>, <span class="number">-1.2299</span>]])</span><br><span class="line">tensor([ <span class="number">0.9319</span>,  <span class="number">0.3797</span>, <span class="number">-0.9644</span>,  <span class="number">0.5682</span>, <span class="number">-0.4755</span>])</span><br></pre></td></tr></table></figure>


<h4 id="torch-view-可以改变张量的维度和大小-与Numpy的reshape类似"><a href="#torch-view-可以改变张量的维度和大小-与Numpy的reshape类似" class="headerlink" title="torch.view: 可以改变张量的维度和大小, 与Numpy的reshape类似"></a>torch.view: 可以改变张量的维度和大小, 与Numpy的reshape类似</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">print(x.size())</span><br><span class="line">y=x.view(<span class="number">16</span>)</span><br><span class="line"><span class="comment"># size -1 从其他维度推断</span></span><br><span class="line">z=x.view(<span class="number">-1</span>,<span class="number">8</span>)</span><br><span class="line">print(y.size())</span><br><span class="line">print(z.size())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line">torch.Size([<span class="number">16</span>])</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure>


<h4 id="如果你有只有一个元素的张量，使用-item-来得到Python数据类型的数值"><a href="#如果你有只有一个元素的张量，使用-item-来得到Python数据类型的数值" class="headerlink" title="如果你有只有一个元素的张量，使用.item()来得到Python数据类型的数值"></a>如果你有只有一个元素的张量，使用.item()来得到Python数据类型的数值</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([<span class="number">0.0918</span>])</span><br><span class="line"><span class="number">0.0918206200003624</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>347前K个高频元素</title>
    <url>/ck3bmvcu8000obsg40zy5ellu.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p> 给定一个非空的整数数组，返回其中出现频率前 <strong>k</strong> 高的元素。 </p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: nums = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>], k = <span class="number">2</span></span><br><span class="line">输出: [<span class="number">1</span>,<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: nums = [<span class="number">1</span>], k = <span class="number">1</span></span><br><span class="line">输出: [<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p><strong>说明：</strong></p>
<ul>
<li>你可以假设给定的 <em>k</em> 总是合理的，且 1 ≤ k ≤ 数组中不相同的元素的个数。</li>
<li>你的算法的时间复杂度<strong>必须</strong>优于 O(<em>n</em> log <em>n</em>) , <em>n</em> 是数组的大小。</li>
</ul>
</blockquote>
<h6 id="解题思路：其实本题思路还是比较容易想到的，我们把key设计成数组中出现的元素值，value设置成其出现的次数，然后使用sort函数按照value的值进行降序排列，但是sort只能给顺序类容器排序，而map的元素是pair类型，所以我们将一个pair类型插入到数组中，然后自定义排序规则，最后在使用sort排序即可，这种自定义sort排序，还是要掌握一点，这样遇到和排序相关的题目就可以直接用即可（快排一定手撕一下）。"><a href="#解题思路：其实本题思路还是比较容易想到的，我们把key设计成数组中出现的元素值，value设置成其出现的次数，然后使用sort函数按照value的值进行降序排列，但是sort只能给顺序类容器排序，而map的元素是pair类型，所以我们将一个pair类型插入到数组中，然后自定义排序规则，最后在使用sort排序即可，这种自定义sort排序，还是要掌握一点，这样遇到和排序相关的题目就可以直接用即可（快排一定手撕一下）。" class="headerlink" title="解题思路：其实本题思路还是比较容易想到的，我们把key设计成数组中出现的元素值，value设置成其出现的次数，然后使用sort函数按照value的值进行降序排列，但是sort只能给顺序类容器排序，而map的元素是pair类型，所以我们将一个pair类型插入到数组中，然后自定义排序规则，最后在使用sort排序即可，这种自定义sort排序，还是要掌握一点，这样遇到和排序相关的题目就可以直接用即可（快排一定手撕一下）。"></a>解题思路：其实本题思路还是比较容易想到的，我们把key设计成数组中出现的元素值，value设置成其出现的次数，然后使用sort函数按照value的值进行降序排列，但是sort只能给顺序类容器排序，而map的元素是pair类型，所以我们将一个pair类型插入到数组中，然后自定义排序规则，最后在使用sort排序即可，这种自定义sort排序，还是要掌握一点，这样遇到和排序相关的题目就可以直接用即可（快排一定手撕一下）。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; topKFrequent(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k) &#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt;umap;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;result;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.size();i++)&#123;</span><br><span class="line">            umap[nums[i]]++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">vector</span>&lt;pair&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt;&gt;vec_umap(umap.begin(), umap.end());</span><br><span class="line">        sort(vec_umap.begin(), vec_umap.end(), </span><br><span class="line">        [](<span class="keyword">const</span> pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; &amp;x, <span class="keyword">const</span> pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; &amp;y) -&gt; <span class="keyword">int</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> x.second &gt; y.second;</span><br><span class="line">    &#125;);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;k;i++)&#123;</span><br><span class="line">            result.push_back(vec_umap[i].first);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>454四数相加II</title>
    <url>/ck3bmvcv40019bsg4hqt4b234.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>给定四个包含整数的数组列表 A , B , C , D ,计算有多少个元组 <code>(i, j, k, l)</code> ，使得 <code>A[i] + B[j] + C[k] + D[l] = 0</code>。</p>
<p>为了使问题简单化，所有的 A, B, C, D 具有相同的长度 N，且 0 ≤ N ≤ 500 。所有整数的范围在 -2^28 到 2^28 - 1 之间，最终结果不会超过 2^31 - 1 。</p>
<a id="more"></a>

<blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入:</span><br><span class="line">A = [ <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">B = [<span class="number">-2</span>,<span class="number">-1</span>]</span><br><span class="line">C = [<span class="number">-1</span>, <span class="number">2</span>]</span><br><span class="line">D = [ <span class="number">0</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">输出:</span><br><span class="line"><span class="number">2</span></span><br><span class="line"></span><br><span class="line">解释:</span><br><span class="line">两个元组如下:</span><br><span class="line"><span class="number">1.</span> (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>) -&gt; A[<span class="number">0</span>] + B[<span class="number">0</span>] + C[<span class="number">0</span>] + D[<span class="number">1</span>] = <span class="number">1</span> + (<span class="number">-2</span>) + (<span class="number">-1</span>) + <span class="number">2</span> = <span class="number">0</span></span><br><span class="line"><span class="number">2.</span> (<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>) -&gt; A[<span class="number">1</span>] + B[<span class="number">1</span>] + C[<span class="number">0</span>] + D[<span class="number">0</span>] = <span class="number">2</span> + (<span class="number">-1</span>) + (<span class="number">-1</span>) + <span class="number">0</span> = <span class="number">0</span></span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="解题思路：本题刚开始就知道暴力破解肯定不对，O-N-4-的时间复杂度太大，所以就一直在找时间复杂度怎么才能够减小，怎么和hash结合，还是没有想到，有了别人的思路，如果分成两个双循环，我用unordered-multiset（元素可以重复出现），将A-B出现的结果存入其中，之后再双循环去找前者里是否有-C-D-的值，但是不幸的是，超时了，最后一组测试数据没有通过，怀疑是其查找的速度要比unordered-map慢，然后借鉴了别人的思路，将A和B加起来的结果放进一个hash-map中key就是A-B的和，value就是这个结果出现的次数，因为要计算最终的组合数，然后再双循环计算C-D，那么只要在之前的hash-map中去找一个数正好等于-（C-D）即可，这样的话，由于unordered-map的查找时间为O（1），那么其实时间就主要来源于两个双循环，那么时间复杂度就是O（n-2）-500x500-250000，还能接受。"><a href="#解题思路：本题刚开始就知道暴力破解肯定不对，O-N-4-的时间复杂度太大，所以就一直在找时间复杂度怎么才能够减小，怎么和hash结合，还是没有想到，有了别人的思路，如果分成两个双循环，我用unordered-multiset（元素可以重复出现），将A-B出现的结果存入其中，之后再双循环去找前者里是否有-C-D-的值，但是不幸的是，超时了，最后一组测试数据没有通过，怀疑是其查找的速度要比unordered-map慢，然后借鉴了别人的思路，将A和B加起来的结果放进一个hash-map中key就是A-B的和，value就是这个结果出现的次数，因为要计算最终的组合数，然后再双循环计算C-D，那么只要在之前的hash-map中去找一个数正好等于-（C-D）即可，这样的话，由于unordered-map的查找时间为O（1），那么其实时间就主要来源于两个双循环，那么时间复杂度就是O（n-2）-500x500-250000，还能接受。" class="headerlink" title="解题思路：本题刚开始就知道暴力破解肯定不对，O(N^4)的时间复杂度太大，所以就一直在找时间复杂度怎么才能够减小，怎么和hash结合，还是没有想到，有了别人的思路，如果分成两个双循环，我用unordered_multiset（元素可以重复出现），将A+B出现的结果存入其中，之后再双循环去找前者里是否有-(C+D)的值，但是不幸的是，超时了，最后一组测试数据没有通过，怀疑是其查找的速度要比unordered_map慢，然后借鉴了别人的思路，将A和B加起来的结果放进一个hash map中key就是A+B的和，value就是这个结果出现的次数，因为要计算最终的组合数，然后再双循环计算C+D，那么只要在之前的hash map中去找一个数正好等于-（C+D）即可，这样的话，由于unordered_map的查找时间为O（1），那么其实时间就主要来源于两个双循环，那么时间复杂度就是O（n^2）,500x500=250000，还能接受。"></a>解题思路：本题刚开始就知道暴力破解肯定不对，O(N^4)的时间复杂度太大，所以就一直在找时间复杂度怎么才能够减小，怎么和hash结合，还是没有想到，有了别人的思路，如果分成两个双循环，我用unordered_multiset（元素可以重复出现），将A+B出现的结果存入其中，之后再双循环去找前者里是否有-(C+D)的值，但是不幸的是，超时了，最后一组测试数据没有通过，怀疑是其查找的速度要比unordered_map慢，然后借鉴了别人的思路，将A和B加起来的结果放进一个hash map中key就是A+B的和，value就是这个结果出现的次数，因为要计算最终的组合数，然后再双循环计算C+D，那么只要在之前的hash map中去找一个数正好等于-（C+D）即可，这样的话，由于unordered_map的查找时间为O（1），那么其实时间就主要来源于两个双循环，那么时间复杂度就是O（n^2）,500x500=250000，还能接受。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">fourSumCount</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; A, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; B, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; C, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; D)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt;umap;</span><br><span class="line">        <span class="keyword">int</span> size=A.size();</span><br><span class="line">        <span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;size;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;size;j++)&#123;</span><br><span class="line">                umap[A[i]+B[j]]++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;size;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;size;j++)&#123;</span><br><span class="line">                <span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt;::iterator it=umap.find(-(C[i]+D[j]));</span><br><span class="line">                <span class="keyword">if</span>(it!=umap.end())res+=umap[-(C[i]+D[j])];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>652寻找重复的子树</title>
    <url>/ck3bmvcw3001vbsg4eqax781m.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>给定一棵二叉树，返回所有重复的子树。对于同一类的重复子树，你只需要返回其中任意<strong>一棵</strong>的根结点即可。</p>
<p>两棵树重复是指它们具有相同的结构以及相同的结点值。</p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">    1</span><br><span class="line">   / \</span><br><span class="line">  2   3</span><br><span class="line"> /   / \</span><br><span class="line">4   2   4</span><br><span class="line">   /</span><br><span class="line">  4</span><br></pre></td></tr></table></figure>

<p>下面是两个重复的子树：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  2</span><br><span class="line"> /</span><br><span class="line">4</span><br></pre></td></tr></table></figure>

<p>和</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4</span><br></pre></td></tr></table></figure>

<p>因此，你需要以列表的形式返回上述重复子树的根结点。</p>
</blockquote>
<h6 id="解题思路：本题刚开始没有解出来，我刚开始的思路就是把键设计成一个数组，然后将其先序序列插入进去，最后再查找键，如果有和它一样的键的话，那么就是相同的子树，但是发现，这么想其实是错的，因为你先序的话，如果采用的不是递归的话，那么默认是不将空指针当作元素的，所以其实序列如果一样，但是树的结构可能是不一样的！然后借鉴了别人的想法，采用string的方法，将遍历序列转换成一个字符串，如果遇到空指针，那么就当作-，这样以来，采用递归遍历，将其遍历序列生成一个字符串，当作其键，然后value值设置成0，1，0代表没出现这个序列，1代表出现了这个序列。遍历到当前结点，只需要判断其遍历序列是否存在即可。"><a href="#解题思路：本题刚开始没有解出来，我刚开始的思路就是把键设计成一个数组，然后将其先序序列插入进去，最后再查找键，如果有和它一样的键的话，那么就是相同的子树，但是发现，这么想其实是错的，因为你先序的话，如果采用的不是递归的话，那么默认是不将空指针当作元素的，所以其实序列如果一样，但是树的结构可能是不一样的！然后借鉴了别人的想法，采用string的方法，将遍历序列转换成一个字符串，如果遇到空指针，那么就当作-，这样以来，采用递归遍历，将其遍历序列生成一个字符串，当作其键，然后value值设置成0，1，0代表没出现这个序列，1代表出现了这个序列。遍历到当前结点，只需要判断其遍历序列是否存在即可。" class="headerlink" title="解题思路：本题刚开始没有解出来，我刚开始的思路就是把键设计成一个数组，然后将其先序序列插入进去，最后再查找键，如果有和它一样的键的话，那么就是相同的子树，但是发现，这么想其实是错的，因为你先序的话，如果采用的不是递归的话，那么默认是不将空指针当作元素的，所以其实序列如果一样，但是树的结构可能是不一样的！然后借鉴了别人的想法，采用string的方法，将遍历序列转换成一个字符串，如果遇到空指针，那么就当作#，这样以来，采用递归遍历，将其遍历序列生成一个字符串，当作其键，然后value值设置成0，1，0代表没出现这个序列，1代表出现了这个序列。遍历到当前结点，只需要判断其遍历序列是否存在即可。"></a>解题思路：本题刚开始没有解出来，我刚开始的思路就是把键设计成一个数组，然后将其先序序列插入进去，最后再查找键，如果有和它一样的键的话，那么就是相同的子树，但是发现，这么想其实是错的，因为你先序的话，如果采用的不是递归的话，那么默认是不将空指针当作元素的，所以其实序列如果一样，但是树的结构可能是不一样的！然后借鉴了别人的想法，采用string的方法，将遍历序列转换成一个字符串，如果遇到空指针，那么就当作#，这样以来，采用递归遍历，将其遍历序列生成一个字符串，当作其键，然后value值设置成0，1，0代表没出现这个序列，1代表出现了这个序列。遍历到当前结点，只需要判断其遍历序列是否存在即可。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"> * struct TreeNode &#123;</span></span><br><span class="line"><span class="comment"> *     int val;</span></span><br><span class="line"><span class="comment"> *     TreeNode *left;</span></span><br><span class="line"><span class="comment"> *     TreeNode *right;</span></span><br><span class="line"><span class="comment"> *     TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;</span></span><br><span class="line"><span class="comment"> * &#125;;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;TreeNode*&gt; findDuplicateSubtrees(TreeNode* root) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;TreeNode*&gt;result;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>,<span class="keyword">int</span>&gt;node_seq;</span><br><span class="line">        node_sequences(root,result,node_seq);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">node_sequences</span><span class="params">(TreeNode* root,<span class="built_in">vector</span>&lt;TreeNode*&gt;&amp;result,<span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>,<span class="keyword">int</span>&gt;&amp;node_seq)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(!root)<span class="keyword">return</span> <span class="string">"#"</span>;</span><br><span class="line">        <span class="built_in">string</span> str=to_string(root-&gt;val)+node_sequences(root-&gt;left,result,node_seq)+node_sequences(root-&gt;right,result,node_seq);</span><br><span class="line">        <span class="keyword">if</span>(node_seq[str]==<span class="number">1</span>)result.push_back(root);</span><br><span class="line">        node_seq[str]++;</span><br><span class="line">        <span class="keyword">return</span> str;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>理解LSTM</title>
    <url>/ck3bmvcyb003rbsg42ned09q3.html</url>
    <content><![CDATA[<p>SimpleRNN并不是keras中唯一的循环层，另外还有两个：LSTM和GRU。SimpleRNN最大的问题就是：在t时刻，理论上讲它能记住许多时间步之前见过的信息，但是实际上他是不可能学到这种长期依赖的。其原因就在于梯度消失问题，这一问题有点类似于在层数比较多的非循环网络（前馈网络）中观察到的效应：随着层数的增加，网络最终无法训练（具体原因没有深究，以后一定要看看论文），而LSTM和GRU就是为了解决SimpleRNN梯度消失这个问题设计的。<a id="more"></a></p>
<h4 id="LSTM-长短期记忆-算法的提出，是研究梯度消失的重要成果，-解决长依赖问题-。"><a href="#LSTM-长短期记忆-算法的提出，是研究梯度消失的重要成果，-解决长依赖问题-。" class="headerlink" title="LSTM(长短期记忆)算法的提出，是研究梯度消失的重要成果， 解决长依赖问题 。"></a>LSTM(长短期记忆)算法的提出，是研究梯度消失的重要成果， 解决长依赖问题 。</h4><p>所有循环神经网络都具有神经网络的重复模块链的形式。 在标准的RNN中，该重复模块将具有非常简单的结构，例如单个tanh层。标准的RNN网络如下图所示：</p>
<img src="/ck3bmvcyb003rbsg42ned09q3/1.png" class="">

<p> LSTM也具有这种链式结构，但是它的重复单元不同于标准RNN网络里的单元只有一个网络层，它的内部有四个网络层。LSTMs的结构如下图所示：</p>
<img src="/ck3bmvcyb003rbsg42ned09q3/2.png" class="">

<p> 在解释LSTMs的详细结构时先定义一下图中各个符号的含义，符号包括下面几种 </p>
<img src="/ck3bmvcyb003rbsg42ned09q3/3.jpg" class="">

<p>图中黄色类似于CNN里的激活函数操作，粉色圆圈表示点操作，单箭头表示数据流向，箭头合并表示向量的合并（concat）操作，箭头分叉表示向量的拷贝操作 </p>
<h4 id="LSTM核心思想"><a href="#LSTM核心思想" class="headerlink" title="LSTM核心思想"></a>LSTM核心思想</h4><p>LSTM的关键在于细胞的状态(整个绿色的图表示的是一个cell)，和穿过细胞的那条水平线。</p>
<p>细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。</p>
<img src="/ck3bmvcyb003rbsg42ned09q3/4.png" class="">

<p>若只有上面的那条水平线是没办法实现添加或者删除信息的。而是通过一种叫做 门（gates） 的结构来实现的。</p>
<p>门 可以实现选择性地让信息通过，主要是通过一个 sigmoid 的神经层 和一个逐点相乘的操作来实现的。</p>
<img src="/ck3bmvcyb003rbsg42ned09q3/5.png" class="">

<p>sigmoid 层输出（是一个向量）的每个元素都是一个在 0 和 1 之间的实数，表示让对应信息通过的权重（或者占比）。比如， 0 表示“不让任何信息通过”， 1 表示“让所有信息通过”。</p>
<p>LSTM通过三个这样的结构来实现信息的保护和控制。这三个门分别输入门、遗忘门和输出门。</p>
<h4 id="逐步理解LSTM"><a href="#逐步理解LSTM" class="headerlink" title="逐步理解LSTM"></a>逐步理解LSTM</h4><p>接下来通过三个门来逐步理解LSTM的原理</p>
<h5 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h5><p>在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取 h(t-1)和x(t)， 输出一个在 0到 1之间的数值即每个在细胞状态C(t-1)中的数字， 1 表示“完全保留”，0 表示“完全舍弃” 。</p>
<p>举个例子，在语言中我们基于已经看到的词来预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。 </p>
<img src="/ck3bmvcyb003rbsg42ned09q3/6.png" class="">

<p> 其中h(t-1)表示的是上一个cell的输出，x(t)表示的是当前细胞的输入。σ表示sigmod函数。 </p>
<h5 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h5><p>下一步是决定给细胞状态添加哪些新的信息。这一步又分为两个步骤，首先，利用h(t-1)和x(t)通过一个称为输入门的操作来决定更新哪些信息。然后利用h(t-1)和x(t)通过一个tanh层得到新的候选细胞信息~Ct，这些信息可能会被更新到细胞信息中。这两步描述如下图所示：</p>
<img src="/ck3bmvcyb003rbsg42ned09q3/7.png" class="">

<p> 下面将更新旧的细胞信息C(t-1)，变为新的细胞信息Ct。更新的规则就是通过忘记门选择忘记旧细胞信息的一部分，通过输入门选择添加候选细胞信息~Ct的一部分得到新的细胞信息Ct。更新操作如下图所示:</p>
<img src="/ck3bmvcyb003rbsg42ned09q3/8.png" class="">

<h5 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h5><p>最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p>
<p> 还是拿语言模型来举例说明，在预测动词形式的时候，我们需要通过输入的主语是单数还是复数来推断输出门输出的预测动词是单数形式还是复数形式。 </p>
<img src="/ck3bmvcyb003rbsg42ned09q3/9.png" class="">

<h4 id="LSTM变体"><a href="#LSTM变体" class="headerlink" title="LSTM变体"></a>LSTM变体</h4><p>下面主要讲一下其中比较著名的变种 GRU（Gated Recurrent Unit ），这是由 Cho, et al. (2014) 提出。在 GRU 中，如下图所示，只有两个门：重置门（reset gate）和更新门（update gate）。同时在这个结构中，把细胞状态和隐藏状态进行了合并。最后模型比标准的 LSTM 结构要简单，而且这个结构后来也非常流行。</p>
<img src="/ck3bmvcyb003rbsg42ned09q3/10.png" class="">

<p> 其中，<strong>r</strong>表示重置门，<strong>t</strong>表示更新门。重置门决定是否将之前的状态忘记。(作用相当于合并了 LSTM 中的遗忘门和传入门）当<strong>r</strong>趋于0的时候，前一个时刻的状态信息 h(t-1)会被忘掉，隐藏状态^ht 会被重置为当前输入的信息。更新门决定是否要将隐藏状态更新为新的状态 ht(作用相当于 LSTM 中的输出门)</p>
<p>其中重置门为上图中前面那个门，决定了如何将新的输入信息与前面的记忆相结合。更新门为上图中后面那个门，定义了前面记忆保存到当前时间步的量。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>3无重复字符的最长子串</title>
    <url>/ck3bmvcux0014bsg416debbnf.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>给定一个字符串，请你找出其中不含有重复字符的 <strong>最长子串</strong> 的长度。</p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: <span class="string">"abcabcbb"</span></span><br><span class="line">输出: <span class="number">3</span> </span><br><span class="line">解释: 因为无重复字符的最长子串是 <span class="string">"abc"</span>，所以其长度为 <span class="number">3</span>。</span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: <span class="string">"bbbbb"</span></span><br><span class="line">输出: <span class="number">1</span></span><br><span class="line">解释: 因为无重复字符的最长子串是 <span class="string">"b"</span>，所以其长度为 <span class="number">1</span>。</span><br></pre></td></tr></table></figure>

<p><strong>示例 3:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: <span class="string">"pwwkew"</span></span><br><span class="line">输出: <span class="number">3</span></span><br><span class="line">解释: 因为无重复字符的最长子串是 <span class="string">"wke"</span>，所以其长度为 <span class="number">3</span>。</span><br><span class="line">     请注意，你的答案必须是 子串 的长度，<span class="string">"pwke"</span> 是一个子序列，不是子串。</span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="解题思路：本题一开始的思路就是循环当前元素，（但是没考虑到双循环，慢慢做，才发现需要-，因为我们只能确定以当前为首字母的字串的长度），然后就是使用哈希set，从当前元素向后循环，只要没出现在set中的，就插入到set中，并且使用count计数，如果见到了，就代表有重复，令count和max比较，确定是否更新，count置0，以此类推即可，但是要注意的是如果都没有见到重复元素，也要更新max值（这点忽视了，导致数据点没全部通过-）"><a href="#解题思路：本题一开始的思路就是循环当前元素，（但是没考虑到双循环，慢慢做，才发现需要-，因为我们只能确定以当前为首字母的字串的长度），然后就是使用哈希set，从当前元素向后循环，只要没出现在set中的，就插入到set中，并且使用count计数，如果见到了，就代表有重复，令count和max比较，确定是否更新，count置0，以此类推即可，但是要注意的是如果都没有见到重复元素，也要更新max值（这点忽视了，导致数据点没全部通过-）" class="headerlink" title="解题思路：本题一开始的思路就是循环当前元素，（但是没考虑到双循环，慢慢做，才发现需要= =，因为我们只能确定以当前为首字母的字串的长度），然后就是使用哈希set，从当前元素向后循环，只要没出现在set中的，就插入到set中，并且使用count计数，如果见到了，就代表有重复，令count和max比较，确定是否更新，count置0，以此类推即可，但是要注意的是如果都没有见到重复元素，也要更新max值（这点忽视了，导致数据点没全部通过~）"></a>解题思路：本题一开始的思路就是循环当前元素，（但是没考虑到双循环，慢慢做，才发现需要= =，因为我们只能确定以当前为首字母的字串的长度），然后就是使用哈希set，从当前元素向后循环，只要没出现在set中的，就插入到set中，并且使用count计数，如果见到了，就代表有重复，令count和max比较，确定是否更新，count置0，以此类推即可，但是要注意的是如果都没有见到重复元素，也要更新max值（这点忽视了，导致数据点没全部通过~）</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(s.size()==<span class="number">0</span>)<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> max=<span class="number">1</span>;</span><br><span class="line">        <span class="built_in">unordered_set</span>&lt;<span class="keyword">char</span>&gt;S_set;</span><br><span class="line">        <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> j;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;s.size();i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(j=i;j&lt;s.size();j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(S_set.count(s[j])==<span class="number">0</span>)count++; </span><br><span class="line">                <span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="keyword">if</span>(max&lt;count)max=count;</span><br><span class="line">                    count=<span class="number">0</span>;</span><br><span class="line">                    S_set.clear();</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                S_set.insert(s[j]);</span><br><span class="line">                <span class="keyword">if</span>(j==s.size()<span class="number">-1</span>)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(max&lt;count)max=count;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>771宝石与石头</title>
    <url>/ck3bmvcwa0021bsg49jgu447f.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>给定字符串<code>J</code> 代表石头中宝石的类型，和字符串 <code>S</code>代表你拥有的石头。 <code>S</code> 中每个字符代表了一种你拥有的石头的类型，你想知道你拥有的石头中有多少是宝石。</p>
<p><code>J</code> 中的字母不重复，<code>J</code> 和 <code>S</code>中的所有字符都是字母。字母区分大小写，因此<code>&quot;a&quot;</code>和<code>&quot;A&quot;</code>是不同类型的石头。</p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: J = <span class="string">"aA"</span>, S = <span class="string">"aAAbbbb"</span></span><br><span class="line">输出: <span class="number">3</span></span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: J = <span class="string">"z"</span>, S = <span class="string">"ZZ"</span></span><br><span class="line">输出: <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p><strong>注意:</strong></p>
<ul>
<li><code>S</code> 和 <code>J</code> 最多含有50个字母。</li>
<li><code>J</code> 中的字符不重复。</li>
</ul>
</blockquote>
<h6 id="解题思路：本题是最基础的哈希集合的题目，一眼就可以看出来要使用哈希set，将J的插入到set中，然后循环S，再判断当前元素是否在J中即可，比较水。"><a href="#解题思路：本题是最基础的哈希集合的题目，一眼就可以看出来要使用哈希set，将J的插入到set中，然后循环S，再判断当前元素是否在J中即可，比较水。" class="headerlink" title="解题思路：本题是最基础的哈希集合的题目，一眼就可以看出来要使用哈希set，将J的插入到set中，然后循环S，再判断当前元素是否在J中即可，比较水。"></a>解题思路：本题是最基础的哈希集合的题目，一眼就可以看出来要使用哈希set，将J的插入到set中，然后循环S，再判断当前元素是否在J中即可，比较水。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">numJewelsInStones</span><span class="params">(<span class="built_in">string</span> J, <span class="built_in">string</span> S)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_set</span>&lt;<span class="keyword">char</span>&gt;J_set;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;J.size();i++)&#123;</span><br><span class="line">            J_set.insert(J[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> count=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;S.size();i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(J_set.count(S[i])&gt;<span class="number">0</span>)count++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> count;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>36有效的数独</title>
    <url>/ck3bmvcum000wbsg41swi2cz2.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>判断一个 9x9 的数独是否有效。只需要<strong>根据以下规则</strong>，验证已经填入的数字是否有效即可。</p>
<a id="more"></a>

<ol>
<li>数字 <code>1-9</code> 在每一行只能出现一次。</li>
<li>数字 <code>1-9</code> 在每一列只能出现一次。</li>
<li>数字 <code>1-9</code> 在每一个以粗实线分隔的 <code>3x3</code> 宫内只能出现一次。</li>
</ol>
<img src="/ck3bmvcum000wbsg41swi2cz2/1.jpg" class="">

<p>上图是一个部分填充的有效的数独。</p>
<p>数独部分空格内已填入了数字，空白格用 <code>&#39;.&#39;</code> 表示。</p>
<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入:</span><br><span class="line">[</span><br><span class="line">  [<span class="string">"5"</span>,<span class="string">"3"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"7"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"6"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"1"</span>,<span class="string">"9"</span>,<span class="string">"5"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"9"</span>,<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"6"</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"6"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"3"</span>],</span><br><span class="line">  [<span class="string">"4"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"3"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"1"</span>],</span><br><span class="line">  [<span class="string">"7"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"2"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"6"</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"6"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"2"</span>,<span class="string">"8"</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"4"</span>,<span class="string">"1"</span>,<span class="string">"9"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"5"</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"7"</span>,<span class="string">"9"</span>]</span><br><span class="line">]</span><br><span class="line">输出: <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入:</span><br><span class="line">[</span><br><span class="line">  [<span class="string">"8"</span>,<span class="string">"3"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"7"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"6"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"1"</span>,<span class="string">"9"</span>,<span class="string">"5"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"9"</span>,<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"6"</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"6"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"3"</span>],</span><br><span class="line">  [<span class="string">"4"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"3"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"1"</span>],</span><br><span class="line">  [<span class="string">"7"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"2"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"6"</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"6"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"2"</span>,<span class="string">"8"</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"4"</span>,<span class="string">"1"</span>,<span class="string">"9"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"5"</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"7"</span>,<span class="string">"9"</span>]</span><br><span class="line">]</span><br><span class="line">输出: <span class="literal">false</span></span><br><span class="line">解释: 除了第一行的第一个数字从 <span class="number">5</span> 改为 <span class="number">8</span> 以外，空格内其他数字均与 示例<span class="number">1</span> 相同。</span><br><span class="line">     但由于位于左上角的 <span class="number">3</span>x3 宫内有两个 <span class="number">8</span> 存在, 因此这个数独是无效的。</span><br></pre></td></tr></table></figure>

<p><strong>说明:</strong></p>
<ul>
<li>一个有效的数独（部分已被填充）不一定是可解的</li>
<li>只需要根据以上规则，验证已经填入的数字是否有效即可</li>
<li>给定数独序列只包含数字 <code>1-9</code> 和字符 <code>&#39;.&#39;</code> </li>
<li>给定数独永远是 <code>9x9</code> 形式的</li>
</ul>
</blockquote>
<h6 id="解题思路：本题刚开始，在思考怎么设计哈希映射中的键时，还是没有想好，但是总的思想就是判断三个，行，列，3x3的矩阵中不能有重复元素，将行号，列号，块号-通过行列求得，这个需要再思考-设计为键，只需要判断当前元素是否在判断的相应区域出现过，出现过就设置为1，如果判断其他的元素时，value为1，那么就代表当前区域出现过当前元素，不符合题目要求。"><a href="#解题思路：本题刚开始，在思考怎么设计哈希映射中的键时，还是没有想好，但是总的思想就是判断三个，行，列，3x3的矩阵中不能有重复元素，将行号，列号，块号-通过行列求得，这个需要再思考-设计为键，只需要判断当前元素是否在判断的相应区域出现过，出现过就设置为1，如果判断其他的元素时，value为1，那么就代表当前区域出现过当前元素，不符合题目要求。" class="headerlink" title="解题思路：本题刚开始，在思考怎么设计哈希映射中的键时，还是没有想好，但是总的思想就是判断三个，行，列，3x3的矩阵中不能有重复元素，将行号，列号，块号(通过行列求得，这个需要再思考= =)设计为键，只需要判断当前元素是否在判断的相应区域出现过，出现过就设置为1，如果判断其他的元素时，value为1，那么就代表当前区域出现过当前元素，不符合题目要求。"></a>解题思路：本题刚开始，在思考怎么设计哈希映射中的键时，还是没有想好，但是总的思想就是判断三个，行，列，3x3的矩阵中不能有重复元素，将行号，列号，块号(通过行列求得，这个需要再思考= =)设计为键，只需要判断当前元素是否在判断的相应区域出现过，出现过就设置为1，如果判断其他的元素时，value为1，那么就代表当前区域出现过当前元素，不符合题目要求。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">isValidSudoku</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">char</span>&gt;&gt;&amp; board)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">unordered_map</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt;&gt;row(<span class="number">9</span>),col(<span class="number">9</span>),block(<span class="number">9</span>);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">9</span>;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">9</span>;j++)&#123;</span><br><span class="line">                <span class="keyword">char</span> cur=board[i][j];</span><br><span class="line">                <span class="keyword">if</span>(cur==<span class="string">'.'</span>)<span class="keyword">continue</span>;</span><br><span class="line">                <span class="keyword">int</span> block_index=(i/<span class="number">3</span>)*<span class="number">3</span>+(j/<span class="number">3</span>);</span><br><span class="line">                <span class="keyword">if</span>(row[i][cur]||col[j][cur]||block[block_index][cur])<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                row[i][cur]=<span class="number">1</span>;</span><br><span class="line">                col[j][cur]=<span class="number">1</span>;</span><br><span class="line">                block[block_index][cur]=<span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>49字母异位词分组</title>
    <url>/ck3bmvcvg001hbsg433q4ehu3.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p> 给定一个字符串数组，将字母异位词组合在一起。字母异位词指字母相同，但排列不同的字符串。 </p>
<a id="more"></a>

<blockquote>
<p><strong>示例:</strong></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">输入: [<span class="string">"eat"</span>, <span class="string">"tea"</span>, <span class="string">"tan"</span>, <span class="string">"ate"</span>, <span class="string">"nat"</span>, <span class="string">"bat"</span>],</span><br><span class="line">输出:</span><br><span class="line">[</span><br><span class="line">  [<span class="string">"ate"</span>,<span class="string">"eat"</span>,<span class="string">"tea"</span>],</span><br><span class="line">  [<span class="string">"nat"</span>,<span class="string">"tan"</span>],</span><br><span class="line">  [<span class="string">"bat"</span>]</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p><strong>说明：</strong></p>
<ul>
<li>所有输入均为小写字母。</li>
<li>不考虑答案输出的顺序。</li>
</ul>
</blockquote>
<h6 id="解题思路：本题得益于之前的设计键的思想（将不同字符串的映射转换成按照字母大小排序后的字符串，这样符合题目要求的字符串就会被分到同一组），得以有思路，第一次做的思路是，循环字符串，然后存到哈希映射中，同步存到哈希集合set中，这样便于循环将他们分组，刚开始看错了，以为是一个完整的字符串，还考虑用逗号分成一个个字符串，后来发现已经分好了-，第二个思路比较好，借鉴别人的，就是key是排序好的，value是一个字符数组，然后将整个字符数组直接插入到二维数组中即可，不再需要第一次的循环插入。"><a href="#解题思路：本题得益于之前的设计键的思想（将不同字符串的映射转换成按照字母大小排序后的字符串，这样符合题目要求的字符串就会被分到同一组），得以有思路，第一次做的思路是，循环字符串，然后存到哈希映射中，同步存到哈希集合set中，这样便于循环将他们分组，刚开始看错了，以为是一个完整的字符串，还考虑用逗号分成一个个字符串，后来发现已经分好了-，第二个思路比较好，借鉴别人的，就是key是排序好的，value是一个字符数组，然后将整个字符数组直接插入到二维数组中即可，不再需要第一次的循环插入。" class="headerlink" title="解题思路：本题得益于之前的设计键的思想（将不同字符串的映射转换成按照字母大小排序后的字符串，这样符合题目要求的字符串就会被分到同一组），得以有思路，第一次做的思路是，循环字符串，然后存到哈希映射中，同步存到哈希集合set中，这样便于循环将他们分组，刚开始看错了，以为是一个完整的字符串，还考虑用逗号分成一个个字符串，后来发现已经分好了= =，第二个思路比较好，借鉴别人的，就是key是排序好的，value是一个字符数组，然后将整个字符数组直接插入到二维数组中即可，不再需要第一次的循环插入。"></a>解题思路：本题得益于之前的设计键的思想（将不同字符串的映射转换成按照字母大小排序后的字符串，这样符合题目要求的字符串就会被分到同一组），得以有思路，第一次做的思路是，循环字符串，然后存到哈希映射中，同步存到哈希集合set中，这样便于循环将他们分组，刚开始看错了，以为是一个完整的字符串，还考虑用逗号分成一个个字符串，后来发现已经分好了= =，第二个思路比较好，借鉴别人的，就是key是排序好的，value是一个字符数组，然后将整个字符数组直接插入到二维数组中即可，不再需要第一次的循环插入。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">class Solution &#123;</span></span><br><span class="line"><span class="comment">public:</span></span><br><span class="line"><span class="comment">    vector&lt;vector&lt;string&gt;&gt; groupAnagrams(vector&lt;string&gt;&amp; strs) &#123;</span></span><br><span class="line"><span class="comment">        unordered_set&lt;string&gt;uset;</span></span><br><span class="line"><span class="comment">        unordered_multimap&lt;string,string&gt;umap;</span></span><br><span class="line"><span class="comment">        vector&lt;vector&lt;string&gt;&gt;result;</span></span><br><span class="line"><span class="comment">        vector&lt;string&gt;cur_str;</span></span><br><span class="line"><span class="comment">        for(auto str:strs)&#123;</span></span><br><span class="line"><span class="comment">            string temp=str;</span></span><br><span class="line"><span class="comment">            sort(temp.begin(),temp.end());</span></span><br><span class="line"><span class="comment">            umap.insert(&#123;str,temp&#125;);</span></span><br><span class="line"><span class="comment">            uset.insert(temp);</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        for(auto it=uset.begin();it!=uset.end();it++)&#123;</span></span><br><span class="line"><span class="comment">            for(auto it_map=umap.begin();it_map!=umap.end();it_map++)&#123;</span></span><br><span class="line"><span class="comment">                if(*it==it_map-&gt;second)cur_str.push_back(it_map-&gt;first);</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">            result.push_back(cur_str);</span></span><br><span class="line"><span class="comment">            cur_str.clear();</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        return result;</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">&#125;;*/</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt; groupAnagrams(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; strs) &#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>,<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt;umap;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&gt;result;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> str:strs)&#123;</span><br><span class="line">            <span class="built_in">string</span> temp=str;</span><br><span class="line">            sort(temp.begin(),temp.end());</span><br><span class="line">            umap[temp].push_back(str);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> um:umap)&#123;</span><br><span class="line">            result.push_back(um.second);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN循环神经网络</title>
    <url>/ck3bmvcxv003bbsg4cyi733at.html</url>
    <content><![CDATA[<h5 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h5><ul>
<li>之前见过的神经网络都有一个特点，就是没有记忆，只能单独处理每个输入，在输入与输入之间没有保存任何状态，对于这种网络，要想处理数据点的序列或时间序列，需要向网络同时展示整个序列，即将序列转换成单个数据点，然后一次性处理，这种网络叫做前馈网络。</li>
</ul>
<a id="more"></a>

<ul>
<li>细想BP算法,CNN(卷积神经网络)我们会发现, 他们的输出都是只考虑前一个输入的影响而不考虑其它时刻输入的影响, 比如简单的猫,狗,手写数字等单个物体的识别具有较好的效果. 但是, 对于一些与时间先后有关的, 比如视频的下一时刻的预测,文档前后文内容的预测等, 这些算法的表现就不尽如人意了。因此, RNN就应运而生了。</li>
</ul>
<h5 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h5><ul>
<li><p>RNN是一种特殊的神经网络结构, 它是根据”人的认知是基于过往的经验和记忆”这一观点提出的. 它与DNN,CNN不同的是: 它不仅考虑前一时刻的输入,而且赋予了网络对前面的内容的一种’记忆’功能.</p>
</li>
<li><p>RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p>
</li>
<li><img src="/ck3bmvcxv003bbsg4cyi733at/1.png" class="">

</li>
</ul>
<h5 id="RNN前向传播"><a href="#RNN前向传播" class="headerlink" title="RNN前向传播"></a>RNN前向传播</h5><img src="/ck3bmvcxv003bbsg4cyi733at/2.png" class="">

<p>我们可以看到RNN层级结构较之于CNN来说比较简单, 它主要有<strong>输入层</strong>,<strong>Hidden Layer</strong>, <strong>输出层</strong>组成。</p>
<p>并且会发现在<strong>Hidden Layer</strong> 有一个箭头表示数据的循环更新, 这个就是实现时间记忆功能的方法。</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/3.png" class="">

<p>所示为Hidden Layer的层级展开图. t-1, t, t+1表示时间序列. X表示输入的样本. St表示样本在时间t处的的记忆,<strong>St = f(W*St-1 +U*Xt)</strong>. W表示输入的权重, U表示此刻输入的样本的权重, V表示输出的样本权重。</p>
<p> 在t =1时刻, 一般初始化输入S0=0, 随机初始化W,U,V, 进行下面的公式计算:</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/4.png" class="">

<p>其中,f和g均为激活函数. 其中f可以是tanh,relu,sigmoid等激活函数，g通常是softmax也可以是其他。</p>
<p>时间就向前推进，此时的状态s1作为时刻1的记忆状态将参与下一个时刻的预测活动，也就是:</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/5.png" class="">

<p>以此类推, 可以得到最终的输出值为:</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/6.png" class="">

<p><strong>注意</strong>: </p>
<ul>
<li><p>这里的<strong>W,U,V</strong>在每个时刻都是相等的(<strong>权重共享</strong>).</p>
</li>
<li><p>隐藏状态可以理解为: S=f(现有的输入+过去记忆总结) </p>
</li>
</ul>
<h5 id="RNN反向传播"><a href="#RNN反向传播" class="headerlink" title="RNN反向传播"></a>RNN反向传播</h5><p>我们介绍了RNN的前向传播的方式, 那么RNN的权重参数W,U,V都是怎么更新的呢?</p>
<p>每一次的输出值Ot都会产生一个误差值Et, 则总的误差可以表示为:<img src="/ck3bmvcxv003bbsg4cyi733at/7.png" class=""></p>
<p>则损失函数可以使用交叉熵损失函数也可以使用平方误差损失函数。</p>
<p>由于每一步的输出不仅仅依赖当前步的网络，并且还需要前若干步网络的状态，那么这种BP改版的算法叫做Backpropagation Through Time(BPTT) , 也就是将输出端的误差值反向传递,运用梯度下降法进行更新。</p>
<p>也就是要求参数的梯度：</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/8.png" class="">

<p>首先我们求解<strong>W的更新方法</strong>, 由前面的W的更新可以看出它是每个时刻的偏差的偏导数之和. </p>
<p> 在这里我们以 t = 3时刻为例, 根据链式求导法则可以得到t = 3时刻的偏导数为:</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/9.png" class="">

<p>此时, 根据公式<img src="/ck3bmvcxv003bbsg4cyi733at/10.png" class="">我们会发现, S3除了和W有关之外, 还和前一时刻S2有关。</p>
<p>对于S3直接展开得到下面的式子:</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/11.png" class="">

<p>对于S2直接展开得到下面的式子:</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/12.png" class="">

<p>对于S1直接展开得到下面的式子:</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/13.png" class="">

<p>将上述三个式子合并得到:</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/14.png" class="">

<p>这样就得到了公式:</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/15.png" class="">

<p>这里要说明的是:<img src="/ck3bmvcxv003bbsg4cyi733at/18.png" class="">表示的是S3对W直接求导, 不考虑S2的影响.(也就是例如y = f(x)*g(x)对x求导一样)</p>
<p>其次是对<strong>U的更新方法</strong>. 由于参数U求解和W求解类似,这里就不在赘述了,最终得到的具体的公式如下:</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/16.png" class="">

<p>最后,给出<strong>V的更新公式</strong>(V只和输出O有关):</p>
<img src="/ck3bmvcxv003bbsg4cyi733at/17.png" class="">


]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习应用于文本和序列</title>
    <url>/ck3bmvcy6003mbsg48p9h8jzm.html</url>
    <content><![CDATA[<p>文本可以理解成单词序列、字符序列，序列又有时间序列和其他的一般序列。<a id="more"></a></p>
<p>处理序列的两种基本的深度学习模型</p>
<ul>
<li>循环神经网络（RNN）</li>
<li>一维卷积神经网络</li>
</ul>
<p>模型的输入不能是原始文本，他只能处理数值张量，所以要先将文本向量化</p>
<ul>
<li>文本向量化：将文本转换为数值张量的过程</li>
</ul>
<p>文本向量化的方法：</p>
<ul>
<li>将文本分割成单词，将每个单词转换为一个向量</li>
<li>将文本分割成字符，将每个字符转换为一个向量</li>
<li>提取单词或字符的n-gram，并将每个n-gram转换为一个向量。n-gram是多个连续单词或字符的集合（n-gram之间可重叠）</li>
</ul>
<p>将文本分解而成的单元（单词、字符、n-gram）叫做标记</p>
<p>将文本分解成标记的过程叫做分词</p>
 <img src="/ck3bmvcy6003mbsg48p9h8jzm/fenci.png" class="" title="分词的过程">

<p>将向量与标记关联的方法：</p>
<ul>
<li>对标记做one-hot编码</li>
<li>标记嵌入（token embedding），该方法通常只用于单词，叫做词嵌入（word embedding）<ul>
<li>one-hot得到的向量是二进制的、稀疏的（绝大部分元素都是0）、维数很高的（维数等于词表中单词个数）</li>
<li>词嵌入是低维的浮点数向量（密集向量），并且是从数据中学习得到的，常见的维度是256、512、1024（处理非常大的词表），而one-hot编码维度很高（20000或更高）</li>
<li><img src="/ck3bmvcy6003mbsg48p9h8jzm/bianma.png" class="" title="不同方法编码对比">

</li>
</ul>
</li>
</ul>
<p>获取词嵌入的方法：</p>
<ul>
<li>在完成主任务（比如情感预测、文档分类）的同时学习词嵌入，在这种情况下，一开始是随机的词向量，然后对这些词向量进行学习，其学习方式和学习神经网络权重相同</li>
<li>在不同与待解决的机器学习任务上预计算好词嵌入，然后将其加载到模型中，这些词嵌入叫做预训练词嵌入</li>
</ul>
<p>利用embedding层学习词嵌入（对应方法一）</p>
<ul>
<li><p>词嵌入的作用就是将人类语言映射到几何空间，词向量之间的几何关系表示这些词之间的语义关系。但由于语言是特定文化和特定环境的反射，所以要根据实际情况，不同问题不同分析。所以一个合理的做法就是对每个新任务都学习一个新的词嵌入空间，在keras框架中，我么只需要学习一个层的权重即可，即embedding层。</p>
<ul>
<li>embedding层：可以理解成一个字典，将整数序列（二维整数张量）（表示特定单词）映射为密集向量</li>
</ul>
<img src="/ck3bmvcy6003mbsg48p9h8jzm/embedding.png" class="" title="embedding层作用过程">

<ul>
<li>接收整数序列作为输入，然后在字典中查找整数，最后返回相关联的向量（返回的是一个三维浮点数张量）</li>
<li>将一个embedding层实例化时，他的权重（即标记向量的内部字典）初始时是随机的，在训练过程中利用反向传播来逐渐调节这些词向量，改变空间结构以便下游模型可以使用。</li>
</ul>
</li>
</ul>
<p>利用预训练的词嵌入</p>
<ul>
<li>背景：可用的训练数据很少，无法学习适合特定任务的词嵌入</li>
<li>从预计算的嵌入空间中加载嵌入张量，其原理与在图像分类中使用预训练的卷积神经网络是一样的<ul>
<li>原理：没有足够的数据来自己学习真正的特征，但是你需要的特征是非常通用的，很常见，那么就可以重复使用在其他问题上学到的特征</li>
</ul>
</li>
</ul>
<p>预计算的词嵌入数据库：</p>
<ul>
<li>word2vec</li>
<li>GloVe</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>219存在重复元素II</title>
    <url>/ck3bmvcti000dbsg4b5pb9urd.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p> 给定一个整数数组和一个整数 <em>k</em>，判断数组中是否存在两个不同的索引 <em>i</em> 和 <em>j</em>，使得 <strong>nums [i] = nums [j]</strong>，并且 <em>i</em> 和 <em>j</em> 的差的绝对值最大为 <em>k</em>。 </p>
<a id="more"></a>

<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: nums = [1,2,3,1], k = 3</span><br><span class="line">输出: true</span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: nums = [1,0,1,1], k = 1</span><br><span class="line">输出: true</span><br></pre></td></tr></table></figure>

<p><strong>示例 3:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: nums = [1,2,3,1,2,3], k = 2</span><br><span class="line">输出: false</span><br></pre></td></tr></table></figure>
</blockquote>
<h6 id="解题思路：本题思路比较清楚，就是使用哈希映射，当前元素值-gt-数组索引，循环当前数组，如果在哈希表中找到与当前元素相同的数，那么就计算他们之间的索引距离，如果小于等于给定K即可。"><a href="#解题思路：本题思路比较清楚，就是使用哈希映射，当前元素值-gt-数组索引，循环当前数组，如果在哈希表中找到与当前元素相同的数，那么就计算他们之间的索引距离，如果小于等于给定K即可。" class="headerlink" title="解题思路：本题思路比较清楚，就是使用哈希映射，当前元素值-&gt;数组索引，循环当前数组，如果在哈希表中找到与当前元素相同的数，那么就计算他们之间的索引距离，如果小于等于给定K即可。"></a>解题思路：本题思路比较清楚，就是使用哈希映射，当前元素值-&gt;数组索引，循环当前数组，如果在哈希表中找到与当前元素相同的数，那么就计算他们之间的索引距离，如果小于等于给定K即可。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">containsNearbyDuplicate</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_multimap</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt;umap;</span><br><span class="line">        <span class="built_in">unordered_multimap</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt;::iterator it;</span><br><span class="line">        <span class="keyword">int</span> cur_distance;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.size();i++)&#123;</span><br><span class="line">            it=umap.find(nums[i]);</span><br><span class="line">            <span class="keyword">if</span>(it!=umap.end())&#123;</span><br><span class="line">                cur_distance=i-(it-&gt;second);</span><br><span class="line">                <span class="keyword">if</span>(cur_distance&lt;=k)<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            umap.insert(&#123;nums[i],i&#125;);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
  </entry>
  <entry>
    <title>350两个数组的交集II</title>
    <url>/ck3bmvcuj000ubsg4gh381u2c.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p> 给定两个数组，编写一个函数来计算它们的交集。<a id="more"></a> </p>
<blockquote>
<p><strong>示例 1:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: nums1 = [1,2,2,1], nums2 = [2,2]</span><br><span class="line">输出: [2,2]</span><br></pre></td></tr></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: nums1 = [4,9,5], nums2 = [9,4,9,8,4]</span><br><span class="line">输出: [4,9]</span><br></pre></td></tr></table></figure>

<p><strong>说明：</strong></p>
<ul>
<li>输出结果中每个元素出现的次数，应与元素在两个数组中出现的次数一致。</li>
<li>我们可以不考虑输出结果的顺序。</li>
</ul>
<p><strong><em>\</em>进阶:**</strong></p>
<ul>
<li>如果给定的数组已经排好序呢？你将如何优化你的算法？</li>
<li>如果 <em>nums1</em> 的大小比 <em>nums2</em> 小很多，哪种方法更优？</li>
<li>如果 <em>nums2</em> 的元素存储在磁盘上，磁盘内存是有限的，并且你不能一次加载所有的元素到内存中，你该怎么办？</li>
</ul>
</blockquote>
<h6 id="解题思路：本题和之前的求两个数组的交集不同，不同之处就在于，示例1，交集元素如果出现多次，那么结果中应该也是多次，处理操作就是使用unordered-multiset（允许重复元素出现）-那么就是如果当前元素在另一个哈希集合中找到，那么就将其插入到数组中，但是还要有一个操作，就是将另一个集合-中的当前元素删掉，因为有可能后边还会出现，造成结果错误。"><a href="#解题思路：本题和之前的求两个数组的交集不同，不同之处就在于，示例1，交集元素如果出现多次，那么结果中应该也是多次，处理操作就是使用unordered-multiset（允许重复元素出现）-那么就是如果当前元素在另一个哈希集合中找到，那么就将其插入到数组中，但是还要有一个操作，就是将另一个集合-中的当前元素删掉，因为有可能后边还会出现，造成结果错误。" class="headerlink" title="解题思路：本题和之前的求两个数组的交集不同，不同之处就在于，示例1，交集元素如果出现多次，那么结果中应该也是多次，处理操作就是使用unordered_multiset（允许重复元素出现）,那么就是如果当前元素在另一个哈希集合中找到，那么就将其插入到数组中，但是还要有一个操作，就是将另一个集合 中的当前元素删掉，因为有可能后边还会出现，造成结果错误。"></a>解题思路：本题和之前的求两个数组的交集不同，不同之处就在于，示例1，交集元素如果出现多次，那么结果中应该也是多次，处理操作就是使用unordered_multiset（允许重复元素出现）,那么就是如果当前元素在另一个哈希集合中找到，那么就将其插入到数组中，但是还要有一个操作，就是将另一个集合 中的当前元素删掉，因为有可能后边还会出现，造成结果错误。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; intersect(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums1, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums2) &#123;</span><br><span class="line">        <span class="built_in">unordered_multiset</span>&lt;<span class="keyword">int</span>&gt;unums1;</span><br><span class="line">        <span class="built_in">unordered_multiset</span>&lt;<span class="keyword">int</span>&gt;unums2;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;result;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums1.size();i++)&#123;</span><br><span class="line">            unums1.insert(nums1[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums2.size();i++)&#123;</span><br><span class="line">            unums2.insert(nums2[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">unordered_multiset</span>&lt;<span class="keyword">int</span>&gt;::iterator it;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums1.size();i++)&#123;</span><br><span class="line">            it=unums2.find(nums1[i]);</span><br><span class="line">            <span class="keyword">if</span>(it!=unums2.end())&#123;</span><br><span class="line">                result.push_back(nums1[i]);</span><br><span class="line">                unums2.erase(it);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
  </entry>
  <entry>
    <title>387字符串中的第一个唯一字符</title>
    <url>/ck3bmvcut0011bsg4dl3dgo4p.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p> 给定一个字符串，找到它的第一个不重复的字符，并返回它的索引。如果不存在，则返回 -1。 <a id="more"></a></p>
<blockquote>
<p>案例:</p>
<p>s = “leetcode”<br>返回 0.</p>
<p>s = “loveleetcode”,<br>返回 2.</p>
<p>注意事项：您可以假定该字符串只包含小写字母。</p>
</blockquote>
<h6 id="解题思路：本题思路比较清楚，能想到使用哈希映射即可，字符-gt-出现次数，第一次循环，如果在哈希表中存在，次数就-，否则则置为0，最后再依次循环，找到第一个出现次数为0的字符即可。"><a href="#解题思路：本题思路比较清楚，能想到使用哈希映射即可，字符-gt-出现次数，第一次循环，如果在哈希表中存在，次数就-，否则则置为0，最后再依次循环，找到第一个出现次数为0的字符即可。" class="headerlink" title="解题思路：本题思路比较清楚，能想到使用哈希映射即可，字符-&gt;出现次数，第一次循环，如果在哈希表中存在，次数就++，否则则置为0，最后再依次循环，找到第一个出现次数为0的字符即可。"></a>解题思路：本题思路比较清楚，能想到使用哈希映射即可，字符-&gt;出现次数，第一次循环，如果在哈希表中存在，次数就++，否则则置为0，最后再依次循环，找到第一个出现次数为0的字符即可。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">firstUniqChar</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>,<span class="keyword">int</span>&gt;umap;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="keyword">char</span>,<span class="keyword">int</span>&gt;::iterator it;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;s.size();i++)&#123;</span><br><span class="line">            it=umap.find(s[i]);</span><br><span class="line">            <span class="keyword">if</span>(it!=umap.end())it-&gt;second++;</span><br><span class="line">            <span class="keyword">else</span> umap[s[i]]=<span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;s.size();i++)&#123;</span><br><span class="line">            it=umap.find(s[i]);</span><br><span class="line">            <span class="keyword">if</span>(it-&gt;second==<span class="number">0</span>)<span class="keyword">return</span> i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
  </entry>
  <entry>
    <title>599两个列表的最小索引总和</title>
    <url>/ck3bmvcvu001obsg4a3av0rpe.html</url>
    <content><![CDATA[<h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>假设Andy和Doris想在晚餐时选择一家餐厅，并且他们都有一个表示最喜爱餐厅的列表，每个餐厅的名字用字符串表示。<a id="more"></a></p>
<p>你需要帮助他们用<strong>最少的索引和</strong>找出他们<strong>共同喜爱的餐厅</strong>。 如果答案不止一个，则输出所有答案并且不考虑顺序。 你可以假设总是存在一个答案。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">示例 <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">输入:</span><br><span class="line">[<span class="string">"Shogun"</span>, <span class="string">"Tapioca Express"</span>, <span class="string">"Burger King"</span>, <span class="string">"KFC"</span>]</span><br><span class="line">[<span class="string">"Piatti"</span>, <span class="string">"The Grill at Torrey Pines"</span>, <span class="string">"Hungry Hunter Steakhouse"</span>, <span class="string">"Shogun"</span>]</span><br><span class="line">输出: [<span class="string">"Shogun"</span>]</span><br><span class="line">解释: 他们唯一共同喜爱的餐厅是“Shogun”。</span><br><span class="line">示例 <span class="number">2</span>:</span><br><span class="line"></span><br><span class="line">输入:</span><br><span class="line">[<span class="string">"Shogun"</span>, <span class="string">"Tapioca Express"</span>, <span class="string">"Burger King"</span>, <span class="string">"KFC"</span>]</span><br><span class="line">[<span class="string">"KFC"</span>, <span class="string">"Shogun"</span>, <span class="string">"Burger King"</span>]</span><br><span class="line">输出: [<span class="string">"Shogun"</span>]</span><br><span class="line">解释: 他们共同喜爱且具有最小索引和的餐厅是“Shogun”，它有最小的索引和<span class="number">1</span>(<span class="number">0</span>+<span class="number">1</span>)。</span><br><span class="line">提示:</span><br><span class="line"></span><br><span class="line">两个列表的长度范围都在 [<span class="number">1</span>, <span class="number">1000</span>]内。</span><br><span class="line">两个列表中的字符串的长度将在[<span class="number">1</span>，<span class="number">30</span>]的范围内。</span><br><span class="line">下标从<span class="number">0</span>开始，到列表的长度减<span class="number">1</span>。</span><br><span class="line">两个列表都没有重复的元素。</span><br></pre></td></tr></table></figure>

<h6 id="解题思路：本题思路比较清晰，创建两个哈希映射，餐厅名称-gt-索引，然后再循环，如果再另一个哈希表找到当前餐厅，那么就将其两个索引加起来，和min比较，如果比min小，value值就设置成min，反之就设置成2000-（因为两个索引最多为1998），最后再次循环，找到value值为min的加入到数组中即可。"><a href="#解题思路：本题思路比较清晰，创建两个哈希映射，餐厅名称-gt-索引，然后再循环，如果再另一个哈希表找到当前餐厅，那么就将其两个索引加起来，和min比较，如果比min小，value值就设置成min，反之就设置成2000-（因为两个索引最多为1998），最后再次循环，找到value值为min的加入到数组中即可。" class="headerlink" title="解题思路：本题思路比较清晰，创建两个哈希映射，餐厅名称-&gt;索引，然后再循环，如果再另一个哈希表找到当前餐厅，那么就将其两个索引加起来，和min比较，如果比min小，value值就设置成min，反之就设置成2000+（因为两个索引最多为1998），最后再次循环，找到value值为min的加入到数组中即可。"></a>解题思路：本题思路比较清晰，创建两个哈希映射，餐厅名称-&gt;索引，然后再循环，如果再另一个哈希表找到当前餐厅，那么就将其两个索引加起来，和min比较，如果比min小，value值就设置成min，反之就设置成2000+（因为两个索引最多为1998），最后再次循环，找到value值为min的加入到数组中即可。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; findRestaurant(<span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; list1, <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;&amp; list2) &#123;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>,<span class="keyword">int</span>&gt;map_l1;</span><br><span class="line">        <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>,<span class="keyword">int</span>&gt;map_l2;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt;result;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;list1.size();i++)&#123;</span><br><span class="line">            map_l1[list1[i]]=i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;list2.size();i++)&#123;</span><br><span class="line">            map_l2[list2[i]]=i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> min=<span class="number">2000</span>;</span><br><span class="line">        <span class="keyword">int</span> cur_value;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> it=map_l1.begin();it!=map_l1.end();it++)&#123;</span><br><span class="line">            <span class="built_in">unordered_map</span>&lt;<span class="built_in">string</span>,<span class="keyword">int</span>&gt;::iterator l2_iter;</span><br><span class="line">            l2_iter=map_l2.find(it-&gt;first);</span><br><span class="line">            <span class="keyword">if</span>(l2_iter!=map_l2.end())&#123;</span><br><span class="line">                cur_value=it-&gt;second+l2_iter-&gt;second;</span><br><span class="line">                <span class="keyword">if</span>(cur_value&lt;=min)&#123;</span><br><span class="line">                    min=cur_value;</span><br><span class="line">                    it-&gt;second=min;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> it-&gt;second=cur_value;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> it-&gt;second+=<span class="number">2000</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> it=map_l1.begin();it!=map_l1.end();it++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(it-&gt;second==min)result.push_back(it-&gt;first);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
  </entry>
  <entry>
    <title>349两个数组的交集</title>
    <url>/ck3bmvcuc000qbsg45msaat3r.html</url>
    <content><![CDATA[<h2 id="两个数组的交集"><a href="#两个数组的交集" class="headerlink" title="两个数组的交集"></a>两个数组的交集</h2><h6 id="题目描述：给定两个数组，编写一个函数来计算它们的交集"><a href="#题目描述：给定两个数组，编写一个函数来计算它们的交集" class="headerlink" title="题目描述：给定两个数组，编写一个函数来计算它们的交集"></a>题目描述：给定两个数组，编写一个函数来计算它们的交集<a id="more"></a></h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">示例 <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">输入: nums1 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], nums2 = [<span class="number">2</span>,<span class="number">2</span>]</span><br><span class="line">输出: [<span class="number">2</span>]</span><br><span class="line">示例 <span class="number">2</span>:</span><br><span class="line"></span><br><span class="line">输入: nums1 = [<span class="number">4</span>,<span class="number">9</span>,<span class="number">5</span>], nums2 = [<span class="number">9</span>,<span class="number">4</span>,<span class="number">9</span>,<span class="number">8</span>,<span class="number">4</span>]</span><br><span class="line">输出: [<span class="number">9</span>,<span class="number">4</span>]</span><br><span class="line">说明:</span><br><span class="line"></span><br><span class="line">输出结果中的每个元素一定是唯一的。</span><br><span class="line">我们可以不考虑输出结果的顺序。</span><br></pre></td></tr></table></figure>

<h6 id="解题思路：本题比较容易想到使用哈希集合，因为哈希集合-unordered-set-寻找速度快，是o-1-级别（底层使用哈希表，而set底层使用红黑树，所以查找速度为O-logn-故使用前者），将其两个数组全部插入到哈希集合，然后再以一个哈希集合循环，能在另一个找到同元素，即为交集。"><a href="#解题思路：本题比较容易想到使用哈希集合，因为哈希集合-unordered-set-寻找速度快，是o-1-级别（底层使用哈希表，而set底层使用红黑树，所以查找速度为O-logn-故使用前者），将其两个数组全部插入到哈希集合，然后再以一个哈希集合循环，能在另一个找到同元素，即为交集。" class="headerlink" title="解题思路：本题比较容易想到使用哈希集合，因为哈希集合(unordered_set)寻找速度快，是o(1)级别（底层使用哈希表，而set底层使用红黑树，所以查找速度为O(logn),故使用前者），将其两个数组全部插入到哈希集合，然后再以一个哈希集合循环，能在另一个找到同元素，即为交集。"></a>解题思路：本题比较容易想到使用哈希集合，因为哈希集合(unordered_set)寻找速度快，是o(1)级别（底层使用哈希表，而set底层使用红黑树，所以查找速度为O(logn),故使用前者），将其两个数组全部插入到哈希集合，然后再以一个哈希集合循环，能在另一个找到同元素，即为交集。</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; intersection(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums1, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums2) &#123;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;result;</span><br><span class="line">        <span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt;s1;</span><br><span class="line">        <span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt;s2;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums1.size();i++)&#123;</span><br><span class="line">            s1.insert(nums1[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums2.size();i++)&#123;</span><br><span class="line">            s2.insert(nums2[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> it=s2.begin();it!=s2.end();it++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(s1.count(*it)&gt;<span class="number">0</span>)&#123;</span><br><span class="line">                result.push_back(*it);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>136只出现一次的数字</title>
    <url>/ck3bmvcrs0000bsg47javgfsw.html</url>
    <content><![CDATA[<h2 id="只出现一次的数字"><a href="#只出现一次的数字" class="headerlink" title="只出现一次的数字"></a>只出现一次的数字</h2><h6 id="题目描述："><a href="#题目描述：" class="headerlink" title="题目描述："></a>题目描述：</h6><p>给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。</p>
<p>说明：</p>
<p>你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">示例 <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">输入: [<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>]</span><br><span class="line">输出: <span class="number">1</span></span><br><span class="line">示例 <span class="number">2</span>:</span><br><span class="line"></span><br><span class="line">输入: [<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">输出: <span class="number">4</span></span><br></pre></td></tr></table></figure>

<h6 id="解题思路：本题抛开限制条件，非常简单，就是设置一个multiset（允许重复），然后全部插入之后，再次循环找到个数为1的数字。但是借鉴了别人的思路，太巧妙了，使用异或运算（a-0-a-a-a-0-并且最关键的就是疑惑还支持交换律-）"><a href="#解题思路：本题抛开限制条件，非常简单，就是设置一个multiset（允许重复），然后全部插入之后，再次循环找到个数为1的数字。但是借鉴了别人的思路，太巧妙了，使用异或运算（a-0-a-a-a-0-并且最关键的就是疑惑还支持交换律-）" class="headerlink" title="解题思路：本题抛开限制条件，非常简单，就是设置一个multiset（允许重复），然后全部插入之后，再次循环找到个数为1的数字。但是借鉴了别人的思路，太巧妙了，使用异或运算（a^0=a,a^a=0,并且最关键的就是疑惑还支持交换律- -）"></a>解题思路：本题抛开限制条件，非常简单，就是设置一个multiset（允许重复），然后全部插入之后，再次循环找到个数为1的数字。但是借鉴了别人的思路，太巧妙了，使用异或运算（a^0=a,a^a=0,并且最关键的就是疑惑还支持交换律- -）</h6><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">containsDuplicate</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">        <span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt;uset;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.size();i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(uset.count(nums[i])&gt;<span class="number">0</span>)<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            <span class="keyword">else</span> uset.insert(nums[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        unordered_set&lt;int&gt;uset;</span></span><br><span class="line"><span class="comment">        for(int i=0;i&lt;nums.size();i++)&#123;</span></span><br><span class="line"><span class="comment">             uset.insert(nums[i]);</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        if(uset.size()==nums.size())return false;</span></span><br><span class="line"><span class="comment">        return true;*/</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
</search>
