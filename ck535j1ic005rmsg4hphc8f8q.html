<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.0.0">

<meta name="baidu-site-verification" content="rIJY9xV50M" />
<meta name="google-site-verification" content="cXp9HEmUPyy8tG__BT8wVBzHOk3ZPwyOatKqezH1Ovs" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <link rel="alternate" href="/atom.xml" title="lemon" type="application/atom+xml">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.lug.ustc.edu.cn/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.5.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: true,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":true,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="看完官方的入门文档之后，又在github上找了一个教程，这是一个韩国人写的教程，感觉还不错。  这个资源为深度学习研究人员提供了学习PyTorch的教程 代码大多数模型都使用少于30行代码实现">
<meta name="keywords" content="PyTorch">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch-Day4">
<meta property="og:url" content="https:&#x2F;&#x2F;cblog.club&#x2F;ck535j1ic005rmsg4hphc8f8q.html">
<meta property="og:site_name" content="lemon">
<meta property="og:description" content="看完官方的入门文档之后，又在github上找了一个教程，这是一个韩国人写的教程，感觉还不错。  这个资源为深度学习研究人员提供了学习PyTorch的教程 代码大多数模型都使用少于30行代码实现">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https:&#x2F;&#x2F;cblog.club&#x2F;ck535j1ic005rmsg4hphc8f8q&#x2F;1.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;cblog.club&#x2F;ck535j1ic005rmsg4hphc8f8q&#x2F;2.jpg">
<meta property="og:updated_time" content="2019-11-22T13:37:52.716Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;cblog.club&#x2F;ck535j1ic005rmsg4hphc8f8q&#x2F;1.jpg">

<link rel="canonical" href="https://cblog.club/ck535j1ic005rmsg4hphc8f8q.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>PyTorch-Day4 | lemon</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
	<a href="https://github.com/ustblc" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">lemon</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">一直在路上</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://cblog.club/ck535j1ic005rmsg4hphc8f8q.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/head.jpg">
      <meta itemprop="name" content="lemon">
      <meta itemprop="description" content="研一在读">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lemon">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          PyTorch-Day4
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2019-11-22 21:02:15 / 修改时间：21:37:52" itemprop="dateCreated datePublished" datetime="2019-11-22T21:02:15+08:00">2019-11-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PyTorch/" itemprop="url" rel="index">
                    <span itemprop="name">PyTorch</span>
                  </a>
                </span>
            </span>

          
            <span id="/ck535j1ic005rmsg4hphc8f8q.html" class="post-meta-item leancloud_visitors" data-flag-title="PyTorch-Day4" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/ck535j1ic005rmsg4hphc8f8q.html#comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/ck535j1ic005rmsg4hphc8f8q.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>13k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>11 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>看完官方的入门文档之后，又在github上找了一个教程，这是一个韩国人写的教程，感觉还不错。</p>
<ul>
<li>这个资源为深度学习研究人员提供了学习PyTorch的教程</li>
<li>代码大多数模型都使用少于30行代码实现</li>
</ul>
<a id="more"></a>

<h3 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h3><ul>
<li>基本的自动求导 例子1</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建张量tensors(这里举得的例子是张量)</span></span><br><span class="line">x = torch.tensor(<span class="number">1.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">w = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个可以计算的公式 y=2x+3</span></span><br><span class="line">y = w * x + b</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br><span class="line">print(w.grad)</span><br><span class="line">print(b.grad)</span><br></pre></td></tr></table></figure>

<pre><code>tensor(2.)
tensor(1.)
tensor(1.)</code></pre><h4 id="来解释一样2，1，1这三个数是怎么来的，首先y-2x-3，那么对于x求梯度（也就是求导）结果为2，以此类推。"><a href="#来解释一样2，1，1这三个数是怎么来的，首先y-2x-3，那么对于x求梯度（也就是求导）结果为2，以此类推。" class="headerlink" title="来解释一样2，1，1这三个数是怎么来的，首先y=2x+3，那么对于x求梯度（也就是求导）结果为2，以此类推。"></a>来解释一样2，1，1这三个数是怎么来的，首先y=2x+3，那么对于x求梯度（也就是求导）结果为2，以此类推。</h4><ul>
<li>基本的自动求导 例子2</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建一个 shape(10,3)和(10,2)的张量tensors</span></span><br><span class="line">x=torch.randn(<span class="number">10</span>,<span class="number">3</span>)</span><br><span class="line">y=torch.randn(<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个全连接层</span></span><br><span class="line">linear=nn.Linear(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">print(<span class="string">'w:'</span>,linear.weight)</span><br><span class="line">print(<span class="string">'b:'</span>,linear.bias)</span><br></pre></td></tr></table></figure>

<pre><code>w: Parameter containing:
tensor([[-0.1955,  0.2712,  0.5710],
        [-0.3143, -0.5540,  0.3058]], requires_grad=True)
b: Parameter containing:
tensor([-0.2155, -0.1076], requires_grad=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立一个损失函数和优化器</span></span><br><span class="line">criterion=nn.MSELoss()</span><br><span class="line">optimizer=torch.optim.SGD(linear.parameters(),lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">pred=linear(x)</span><br><span class="line"><span class="comment">#计算损失</span></span><br><span class="line">loss=criterion(pre,y)</span><br><span class="line"><span class="comment">#加上item()，就可以转换python中的float型数值</span></span><br><span class="line">print(<span class="string">'loss:'</span>,loss.item())</span><br></pre></td></tr></table></figure>

<pre><code>loss: 0.9739418625831604</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"><span class="comment">#打印输出梯度</span></span><br><span class="line">print(<span class="string">'dl/dw:'</span>,linear.weight.grad)</span><br><span class="line">print(<span class="string">'dl/db:'</span>,linear.bias.grad)</span><br></pre></td></tr></table></figure>

<pre><code>dl/dw: tensor([[ 0.0576,  0.1626,  0.4199],
        [-0.1823, -0.3512,  0.5736]])
dl/db: tensor([0.1189, 0.1488])</code></pre><ul>
<li>optimizer.step()这个方法会更新所有的参数。一旦梯度被如backward()之类的函数计算好后，我们就可以调用这个函数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#接着进行梯度下降</span></span><br><span class="line">optimizer.step()</span><br><span class="line"><span class="comment">#打印输出在一次梯度下降优化后的损失</span></span><br><span class="line">pred=linear(x)</span><br><span class="line">loss=criterion(pred,y)</span><br><span class="line">print(<span class="string">'loss after 1 step optimization:'</span>,loss.item())</span><br></pre></td></tr></table></figure>

<pre><code>loss after 1 step optimization: 0.9595106840133667</code></pre><h4 id="从numpy中加载数据"><a href="#从numpy中加载数据" class="headerlink" title="从numpy中加载数据"></a>从numpy中加载数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先创建一个numpy数组</span></span><br><span class="line">x=np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#将numpy数组转换成torch tensor（torch中的张量）</span></span><br><span class="line">y=torch.from_numpy(x)</span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#再将torch tensor转换成numpy数组</span></span><br><span class="line">z=y.numpy()</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[1, 2],
        [3, 4]], dtype=torch.int32)
[[1 2]
 [3 4]]</code></pre><h4 id="定义数据输入流水线（类似于keras中加载数据集的意味）"><a href="#定义数据输入流水线（类似于keras中加载数据集的意味）" class="headerlink" title="定义数据输入流水线（类似于keras中加载数据集的意味）"></a>定义数据输入流水线（类似于keras中加载数据集的意味）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下载和创建CIFAR10数据集</span></span><br><span class="line">train_dataset=torchvision.datasets.CIFAR10(root=<span class="string">'./data/'</span>,train=<span class="literal">True</span>,transform=transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取一个数据对(图像和标签)（从磁盘中读取数据）</span></span><br><span class="line">image,label=train_dataset[<span class="number">0</span>]</span><br><span class="line">print(image.size())</span><br><span class="line">print(label)</span><br></pre></td></tr></table></figure>

<pre><code>Files already downloaded and verified
torch.Size([3, 32, 32])
6</code></pre><img src="/ck535j1ic005rmsg4hphc8f8q/1.jpg" class="">

<p> 我们可以看出，图片是彩色图像（3通道），尺寸大小是32x32，标签的索引为6，我们知道cifar10：它有如下10个类别:(’airplane’,’automobile’,’bird’,’cat’,’deer’,’dog’,’frog’,’horse’,’ship’,’truck’)，猜测这个6就应该是frog青蛙。嘿，果然他就是！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个显示图片的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span><span class="params">(img)</span>:</span></span><br><span class="line">    img=img/<span class="number">2</span>+<span class="number">0.5</span></span><br><span class="line">    npimg=img.numpy()</span><br><span class="line">    <span class="comment">#transpose对换数组的维度</span></span><br><span class="line">    plt.imshow(np.transpose(npimg,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line">    print(np.transpose(npimg,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)).shape)</span><br><span class="line">    plt.show();</span><br><span class="line"></span><br><span class="line">imshow(image)</span><br></pre></td></tr></table></figure>

<pre><code>(32, 32, 3)</code></pre><p>这个<code>img=img/2+0.5</code>就是对图像灰度做了点修正，然后transpose对换数组的维度，我们看啊，从数据集中读取的image，它的格式是(3,32,32)，而plt中要显示的图片，它的格式是(32,32,3)所以，要使用这个函数，将图片转换成我们能识别的格式，终于找到原因了= =！~</p>
<h4 id="数据加载到数据加载器中（它以非常简单的方式提供了队列和线程）"><a href="#数据加载到数据加载器中（它以非常简单的方式提供了队列和线程）" class="headerlink" title="数据加载到数据加载器中（它以非常简单的方式提供了队列和线程）"></a>数据加载到数据加载器中（它以非常简单的方式提供了队列和线程）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#当迭代器开始的时候，队列和线程就会从文件中加载数据</span></span><br><span class="line">data_iter=iter(train_loader)</span><br><span class="line"></span><br><span class="line"><span class="comment">#mini-batch的images和labels</span></span><br><span class="line">images,labels=data_iter.next()</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据加载器的实际使用方法如下</span></span><br><span class="line"><span class="keyword">for</span> images,labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="comment">#训练代码写在这里</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h4 id="为自定义的数据集设计数据输入流水线"><a href="#为自定义的数据集设计数据输入流水线" class="headerlink" title="为自定义的数据集设计数据输入流水线"></a>为自定义的数据集设计数据输入流水线</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先创建自定义的数据集</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#To do</span></span><br><span class="line">        <span class="comment">#初始化你的文件路径和文件名列表</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self,index)</span>:</span></span><br><span class="line">        <span class="comment">#To do</span></span><br><span class="line">        <span class="comment">#首先从文件中取一个数据（举个例子：using numpy.fromfile, PIL.Image.open（打开一个图像文件））</span></span><br><span class="line">        <span class="comment">#接着对数据进行预处理（举个例子：torchvision.Transform）</span></span><br><span class="line">        <span class="comment">#PIL：Python Imaging Library，已经是Python平台事实上的图像处理标准库了。</span></span><br><span class="line">        <span class="comment">#torchvision.Transform对PIL.Image进行变换</span></span><br><span class="line">        <span class="comment">#最后返回一个数据对（image和label）</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#应该将数据集的总大小更改为0，意思就是全部训练完成</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#然后你就可以使用提前创建好的数据加载器</span></span><br><span class="line">custom_dataset=CustomDataset()</span><br><span class="line">train_loader=torch.utils.data.DataLoader(dataset=custom_dataset,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下载和加载提前训练好的ResNet-18</span></span><br><span class="line">resnet=torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果你只想微调模型的顶层，可以按照下面设置</span></span><br><span class="line"><span class="keyword">for</span> parm <span class="keyword">in</span> resnet.parameters():</span><br><span class="line">    parm.requires_grad=<span class="literal">False</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#更换顶层进行微调(将原先resnet的全连接层的输入样本大小改为100)</span></span><br><span class="line">resnet.fc=nn.Linear(resnet.fc.in_features,<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#forward省略</span></span><br><span class="line"></span><br><span class="line">images=torch.randn(<span class="number">64</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">outputs=resnet(images)</span><br><span class="line">print(outputs.size())</span><br></pre></td></tr></table></figure>

<pre><code>Downloading: &quot;https://download.pytorch.org/models/resnet18-5c106cde.pth&quot; to C:\Users\user/.cache\torch\checkpoints\resnet18-5c106cde.pth
100%|█████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:29&lt;00:00, 1.59MB/s]


torch.Size([64, 100])</code></pre><h4 id="保存和加载模型"><a href="#保存和加载模型" class="headerlink" title="保存和加载模型"></a>保存和加载模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存和加载完整的模型</span></span><br><span class="line">torch.save(resnet,<span class="string">'model.ckpt'</span>)</span><br><span class="line">model=torch.load(<span class="string">'model.ckpt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#只保存和加载模型的权重参数（推荐！）</span></span><br><span class="line">torch.save(resnet.state_dict(),<span class="string">'params.ckpt'</span>)</span><br><span class="line">resnet.load_state_dict(torch.load(<span class="string">'params.ckpt'</span>))</span><br></pre></td></tr></table></figure>




<pre><code>&lt;All keys matched successfully&gt;</code></pre><ul>
<li>state_dict() 以dict返回optimizer的状态。它包含两项。<ul>
<li>state 一个保存了当前优化状态的dict。optimizer的类别不同，state的内容也会不同。</li>
<li>param_groups  一个包含了全部参数组的dict。</li>
</ul>
</li>
</ul>
<h3 id="线性回归练习"><a href="#线性回归练习" class="headerlink" title="线性回归练习"></a>线性回归练习</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h4 id="设置超参数Hyper-Parameters-就是人工设定的参数，不是从网络中学到的"><a href="#设置超参数Hyper-Parameters-就是人工设定的参数，不是从网络中学到的" class="headerlink" title="设置超参数Hyper-Parameters(就是人工设定的参数，不是从网络中学到的)"></a>设置超参数Hyper-Parameters(就是人工设定的参数，不是从网络中学到的)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input_size=<span class="number">1</span></span><br><span class="line">output_size=<span class="number">1</span></span><br><span class="line">num_epochs=<span class="number">60</span></span><br><span class="line">learning_rate=<span class="number">0.001</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#准备数据集Toy dataset（玩具数据集，小数据集，缺乏实验研究，先这样理解，自己玩的数据集（自定义）= =）</span></span><br><span class="line"><span class="comment"># Toy dataset</span></span><br><span class="line">x_train = np.array([[<span class="number">3.3</span>], [<span class="number">4.4</span>], [<span class="number">5.5</span>], [<span class="number">6.71</span>], [<span class="number">6.93</span>], [<span class="number">4.168</span>], </span><br><span class="line">                    [<span class="number">9.779</span>], [<span class="number">6.182</span>], [<span class="number">7.59</span>], [<span class="number">2.167</span>], [<span class="number">7.042</span>], </span><br><span class="line">                    [<span class="number">10.791</span>], [<span class="number">5.313</span>], [<span class="number">7.997</span>], [<span class="number">3.1</span>]], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">y_train = np.array([[<span class="number">1.7</span>], [<span class="number">2.76</span>], [<span class="number">2.09</span>], [<span class="number">3.19</span>], [<span class="number">1.694</span>], [<span class="number">1.573</span>], </span><br><span class="line">                    [<span class="number">3.366</span>], [<span class="number">2.596</span>], [<span class="number">2.53</span>], [<span class="number">1.221</span>], [<span class="number">2.827</span>], </span><br><span class="line">                    [<span class="number">3.465</span>], [<span class="number">1.65</span>], [<span class="number">2.904</span>], [<span class="number">1.3</span>]], dtype=np.float32)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#线性回归的模型,这里输入输出都是一维的，标量，也就是全连接层</span></span><br><span class="line">model=nn.Linear(input_size,output_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#损失函数和优化器</span></span><br><span class="line">criterion=nn.MSELoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="comment">#将numpy数组转换成torch tensors</span></span><br><span class="line">    inputs=torch.from_numpy(x_train)</span><br><span class="line">    targets=torch.from_numpy(y_train)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#前向传播</span></span><br><span class="line">    outputs=model(inputs)</span><br><span class="line">    loss=criterion(outputs,targets)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#反向传播和优化</span></span><br><span class="line">    <span class="comment">#zero_grad()将module中的所有模型参数的梯度设置为0.</span></span><br><span class="line">    <span class="comment">#将所有参数的梯度缓存清零,然后进行反向传播</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(epoch+<span class="number">1</span>)%<span class="number">5</span>==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Epoch [&#123;&#125;/&#123;&#125;],Loss:&#123;:.4f&#125;'</span>.format(epoch+<span class="number">1</span>,num_epochs,loss.item()))</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制图表</span></span><br><span class="line"><span class="comment">#这里如果不加detach()的话，会报错：</span></span><br><span class="line"><span class="comment">#Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.</span></span><br><span class="line"><span class="comment">#无法在需要grad的Variable上调用numpy()。 请改用var.detach().numpy()</span></span><br><span class="line"><span class="comment">#detach()的作用就是不带梯度，返回一个从当前图中分离下来的新的Variable，返回的Variable的requires_grad=False。</span></span><br><span class="line">predicted=model(torch.from_numpy(x_train)).detach().numpy()</span><br><span class="line">plt.plot(x_train,y_train,<span class="string">'ro'</span>,label=<span class="string">'Original data'</span>)</span><br><span class="line">plt.plot(x_train,predicted,label=<span class="string">'Fitted line'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#保存模型的权重（推荐，不推荐直接保存网络）</span></span><br><span class="line">torch.save(model.state_dict(),<span class="string">'linear.ckpt'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch [5/60],Loss:10.8415
Epoch [10/60],Loss:4.5517
Epoch [15/60],Loss:2.0035
Epoch [20/60],Loss:0.9710
Epoch [25/60],Loss:0.5526
Epoch [30/60],Loss:0.3829
Epoch [35/60],Loss:0.3140
Epoch [40/60],Loss:0.2860
Epoch [45/60],Loss:0.2745
Epoch [50/60],Loss:0.2696
Epoch [55/60],Loss:0.2675
Epoch [60/60],Loss:0.2665</code></pre><img src="/ck535j1ic005rmsg4hphc8f8q/2.jpg" class="">


<h3 id="逻辑回归练习"><a href="#逻辑回归练习" class="headerlink" title="逻辑回归练习"></a>逻辑回归练习</h3><h4 id="设置超参数Hyper-Parameters-就是人工设定的参数，不是从网络中学到的-1"><a href="#设置超参数Hyper-Parameters-就是人工设定的参数，不是从网络中学到的-1" class="headerlink" title="设置超参数Hyper-Parameters(就是人工设定的参数，不是从网络中学到的)"></a>设置超参数Hyper-Parameters(就是人工设定的参数，不是从网络中学到的)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input_size=<span class="number">784</span></span><br><span class="line">num_classes=<span class="number">10</span></span><br><span class="line">num_epochs=<span class="number">5</span></span><br><span class="line">batch_size=<span class="number">100</span></span><br><span class="line">learning_rate=<span class="number">0.001</span></span><br></pre></td></tr></table></figure>

<h4 id="经典的手写数字识别，MNIST数据集（images，labels）"><a href="#经典的手写数字识别，MNIST数据集（images，labels）" class="headerlink" title="经典的手写数字识别，MNIST数据集（images，labels）"></a>经典的手写数字识别，MNIST数据集（images，labels）</h4><ul>
<li>这里测试集就不需要<code>download=True</code>了，因为经查看文件夹，发现MNIST训练集和测试集在一个包里= =~</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_dateset=torchvision.datasets.MNIST(root=<span class="string">'./data'</span>,train=<span class="literal">True</span>,transform=transforms.ToTensor(),download=<span class="literal">True</span>)</span><br><span class="line">test_dataset=torchvision.datasets.MNIST(root=<span class="string">'./data'</span>,train=<span class="literal">False</span>,transform=transforms.ToTensor())</span><br></pre></td></tr></table></figure>

<h4 id="数据加载-设计数据输入流水线"><a href="#数据加载-设计数据输入流水线" class="headerlink" title="数据加载(设计数据输入流水线)"></a>数据加载(设计数据输入流水线)</h4><ul>
<li>shuffle设置为True时会在每个epoch重新打乱数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_loader=torch.utils.data.DataLoader(dataset=train_dateset,batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="逻辑回归的模型"><a href="#逻辑回归的模型" class="headerlink" title="逻辑回归的模型"></a>逻辑回归的模型</h4><ul>
<li><p>这里有必要再介绍一下MNIST数据集，因为你要搞懂输入大小和输出大小的设定是怎么来的</p>
<ul>
<li><p>MNIST共有7万张图片。其中6万张用于训练神经网络，1万张用于测试神经网络。</p>
</li>
<li><p>每张图片是一个28*28像素点的0~9的手写数字图片。</p>
</li>
<li><p>黑底白字。黑底用0表示，白字用0~1之间的浮点数表示，越接近1，颜色越白。</p>
</li>
<li><p>我们把784个像素点组成一个长度为784的一维数组，这个一维数据就是我们要喂入神经网络的输入特征。MNIST数据集还提供了每张图片对应的标签，以一个长度为10的一维数组给出。</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model=nn.Linear(input_size,num_classes)</span><br></pre></td></tr></table></figure>

<h4 id="定义损失函数和优化器"><a href="#定义损失函数和优化器" class="headerlink" title="定义损失函数和优化器"></a>定义损失函数和优化器</h4><ul>
<li>这里我们使用交叉熵来作为损失函数比较好，因为这是一个分类问题，具体原因需要机器学习基础，可能看过，暂时忘了= =！</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)</span><br></pre></td></tr></table></figure>

<h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#因为我们是按照批量进行训练的，之前设置的batch_size=100,总共有60000张，那么就应该需要600个批次</span></span><br><span class="line">total_step=len(train_loader)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i,(images,labels) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        <span class="comment">#将图片重新设定形状大小，（batch_size,input_size）,多了一维是批量数大小</span></span><br><span class="line">        <span class="comment">#当前的images的大小是28*28*batch_size的，所以再将它们重新变一下形状即可</span></span><br><span class="line">        images=images.reshape(<span class="number">-1</span>,<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#前向传播</span></span><br><span class="line">        outputs=model(images)</span><br><span class="line">        loss=criterion(outputs,labels)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#梯度置0+反向传播+更新参数</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(i+<span class="number">1</span>)%<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch [&#123;&#125;/&#123;&#125;], Step [&#123;&#125;/&#123;&#125;], Loss: &#123;:.4f&#125;'</span>.format(epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, total_step, loss.item()))</span><br></pre></td></tr></table></figure>

<pre><code>Epoch [1/5], Step [100/600], Loss: 2.2148
Epoch [1/5], Step [200/600], Loss: 2.1090
Epoch [1/5], Step [300/600], Loss: 2.0445
Epoch [1/5], Step [400/600], Loss: 1.9044
Epoch [1/5], Step [500/600], Loss: 1.8223
Epoch [1/5], Step [600/600], Loss: 1.7955
Epoch [2/5], Step [100/600], Loss: 1.7728
Epoch [2/5], Step [200/600], Loss: 1.7020
Epoch [2/5], Step [300/600], Loss: 1.6512
Epoch [2/5], Step [400/600], Loss: 1.4894
Epoch [2/5], Step [500/600], Loss: 1.5539
Epoch [2/5], Step [600/600], Loss: 1.4890
Epoch [3/5], Step [100/600], Loss: 1.4254
Epoch [3/5], Step [200/600], Loss: 1.4015
Epoch [3/5], Step [300/600], Loss: 1.4230
Epoch [3/5], Step [400/600], Loss: 1.3121
Epoch [3/5], Step [500/600], Loss: 1.3245
Epoch [3/5], Step [600/600], Loss: 1.3191
Epoch [4/5], Step [100/600], Loss: 1.2653
Epoch [4/5], Step [200/600], Loss: 1.1637
Epoch [4/5], Step [300/600], Loss: 1.2509
Epoch [4/5], Step [400/600], Loss: 1.1195
Epoch [4/5], Step [500/600], Loss: 1.1106
Epoch [4/5], Step [600/600], Loss: 1.1059
Epoch [5/5], Step [100/600], Loss: 1.1150
Epoch [5/5], Step [200/600], Loss: 1.0129
Epoch [5/5], Step [300/600], Loss: 1.0519
Epoch [5/5], Step [400/600], Loss: 1.0661
Epoch [5/5], Step [500/600], Loss: 0.9755
Epoch [5/5], Step [600/600], Loss: 1.0962</code></pre><h4 id="测试模型并保存模型"><a href="#测试模型并保存模型" class="headerlink" title="测试模型并保存模型"></a>测试模型并保存模型</h4><ul>
<li>这里需要注意的是，在测试阶段，我们不需要再次计算梯度了（为了提高内存的效率= =！）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试模型</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    correct=<span class="number">0</span></span><br><span class="line">    total=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images,labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        <span class="comment">#print(labels.size(0)),其值就是100，也就是我们设置的批量大小</span></span><br><span class="line">        images=images.reshape(<span class="number">-1</span>,<span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        outputs=model(images)</span><br><span class="line">        _,predicted=torch.max(outputs.data,<span class="number">1</span>)</span><br><span class="line">        total+=labels.size(<span class="number">0</span>)</span><br><span class="line">        correct+=(predicted==labels).sum()</span><br><span class="line">        </span><br><span class="line">    print(<span class="string">'Accuracy of the model on the 10000 test images: &#123;&#125; %'</span>.format(<span class="number">100</span> * correct / total))</span><br><span class="line">    </span><br><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">torch.save(model.state_dict(),<span class="string">'logistic.ckpt'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy of the model on the 10000 test images: 82 %</code></pre><ul>
<li>没有用CNN，得到的这个效果其实还可以</li>
</ul>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>PyTorch使用的流程：</p>
<ul>
<li>第一步：通常是设置一些超参数（例如输入大小、如果是分类问题：可能有类别数目、训练轮次，批量大小，学习率）</li>
<li>第二步：加载数据集（有的是PyTorch中封装好的，也有是自定义的，具体问题具体分析）<ul>
<li>训练数据集</li>
<li>测试训练集</li>
</ul>
</li>
<li>第三步：定义数据加载器（设计数据输入流水线）</li>
<li>第四步：建立模型，复杂一点的就是创建自定义网络模型</li>
<li>第五步：定义损失函数和优化器<ul>
<li>回归一般用MSE（均方误差）</li>
<li>分类问题一般用CrossEntropyLoss（交叉熵）</li>
</ul>
</li>
<li>第六步：训练模型<ul>
<li>外部循环是训练轮次</li>
<li>内部循环如果有，一般是数据集很大，我们分成了很多批次，每次按照批次大小训练</li>
<li>训练的步骤一般是：如果是批量输入，考虑是否要给数据改变形状-&gt;前向传播-&gt;计算损失-&gt;优化器梯度置0-&gt;反向传播-&gt;更新所有参数</li>
</ul>
</li>
<li>第七步：测试模型（在测试阶段不需要计算梯度，使用<code>with torch.no_grad()</code>）</li>
<li>第八步：保存模型，推荐只保存模型的权重</li>
</ul>
<h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><p>在这个教程里,我们将学习如何使用数据并行(DataParallel)来使用多GPU。</p>
<p>PyTorch非常容易的就可以使用GPU,你可以用如下方式把一个模型放到GPU上:</p>
<p><code>device = torch.device(&quot;cuda:0&quot;)</code></p>
<p><code>model.to(device)</code></p>
<p>然后你可以复制所有的张量到GPU上:</p>
<p><code>mytensor = my_tensor.to(device)</code></p>
<p>请注意,只调用<code>mytensor.gpu()</code>并没有复制张量到GPU上。你需要把它赋值给一个新的张量并在GPU上使用这个张量。</p>
<p>在多GPU上执行前向和反向传播是自然而然的事。然而，<strong>PyTorch默认将只是用一个GPU</strong>。你可以使用DataParallel让模型并行运行来轻易的让你的操作在多个GPU上运行。</p>
<p><code>model = nn.DataParallel(model)</code></p>
<p>这是这篇教程背后的核心，我们接下来将更详细的介绍它</p>
<h4 id="导入PyTorch模块和定义参数"><a href="#导入PyTorch模块和定义参数" class="headerlink" title="导入PyTorch模块和定义参数"></a>导入PyTorch模块和定义参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset,DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义参数</span></span><br><span class="line">input_size=<span class="number">5</span></span><br><span class="line">output_size=<span class="number">2</span></span><br><span class="line"></span><br><span class="line">batch_size=<span class="number">30</span></span><br><span class="line">data_size=<span class="number">100</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设备</span></span><br><span class="line">device=torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">print(device)</span><br><span class="line">print(torch.cuda.device_count())</span><br><span class="line">print(torch.cuda.current_device())</span><br><span class="line">print(torch.cuda.get_device_name(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<pre><code>cuda:0
1
0
GeForce GTX 950M</code></pre><h4 id="虚拟数据集"><a href="#虚拟数据集" class="headerlink" title="虚拟数据集"></a>虚拟数据集</h4><p>制作一个虚拟（随机）数据集，你只需实现<strong>getitem</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,size,length)</span>:</span></span><br><span class="line">        self.len=length</span><br><span class="line">        <span class="comment">#randn返回一个张量，从标准正态分布（均值为0，方差为1）中抽取的一组随机数。</span></span><br><span class="line">        <span class="comment">#length--整数序列，定义了输出张量的形状</span></span><br><span class="line">        <span class="comment">#size--输出张量的形状</span></span><br><span class="line">        self.data=torch.randn(length,size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self,index)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.len</span><br><span class="line">    </span><br><span class="line">rand_loader=DataLoader(dataset=RandomDataset(input_size,data_size),batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="简单的模型"><a href="#简单的模型" class="headerlink" title="简单的模型"></a>简单的模型</h4><p>作为演示，我们的模型只接受一个输入，执行一个线性操作，然后得到结果。然而，你能在任何模型（CNN，RNN，Capsule Net等）上使用DataParallel。</p>
<p>我们在模型内部放置了一条打印语句来检测输入和输出向量的大小。请注意批等级为0时打印的内容。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size,output_size)</span>:</span></span><br><span class="line">        <span class="comment">#这句话官方就是这么写，规定好了,继承Module的初始化函数</span></span><br><span class="line">        super(Model,self).__init__()</span><br><span class="line">        self.fc=nn.Linear(input_size,output_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input)</span>:</span></span><br><span class="line">        output=self.fc(input)</span><br><span class="line">        print(<span class="string">"\tIn Model: input size"</span>, input.size(),<span class="string">"output size"</span>, output.size())</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<h4 id="创建一个模型和数据并行"><a href="#创建一个模型和数据并行" class="headerlink" title="创建一个模型和数据并行"></a>创建一个模型和数据并行</h4><p>这是本教程的核心部分。首先，我们需要创建一个模型实例和检测我们是否有多个GPU。如果我们有多个GPU，我们使用nn.DataParallel来包装我们的模型。然后通过model.to(device)把模型放到GPU上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model=Model(input_size,output_size)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count()&gt;<span class="number">1</span>:</span><br><span class="line">    print(<span class="string">"let's use"</span>,torch.cuda.device_count(),<span class="string">"GPUs!"</span>)</span><br><span class="line">    </span><br><span class="line">    model=nn.DataParallel(model)</span><br><span class="line">    </span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>




<pre><code>Model(
  (fc): Linear(in_features=5, out_features=2, bias=True)
)</code></pre><h4 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h4><p>现在我们可以看看输入和输出张量的大小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> rand_loader:</span><br><span class="line">    input=data.to(device)</span><br><span class="line">    output=model(input)</span><br><span class="line">    print(<span class="string">"outside: input size"</span>,input_size,<span class="string">"output size"</span>,output_size)</span><br></pre></td></tr></table></figure>

<pre><code>    In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])
outside: input size 5 output size 2
    In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])
outside: input size 5 output size 2
    In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])
outside: input size 5 output size 2
    In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])
outside: input size 5 output size 2</code></pre><h4 id="但是，我只有一个GPU-！"><a href="#但是，我只有一个GPU-！" class="headerlink" title="但是，我只有一个GPU = =！"></a>但是，我只有一个GPU = =！</h4>
    </div>

    
    
    

	<div>
	  
		<div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
	  
	</div>
        <div class="reward-container">
  <div></div>
  <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="lemon 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="lemon 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>lemon
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://cblog.club/ck535j1ic005rmsg4hphc8f8q.html" title="PyTorch-Day4">https://cblog.club/ck535j1ic005rmsg4hphc8f8q.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"> <i class="fa fa-tag"></i> PyTorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
                <a href="/ck535j1j60066msg47f9a3clg.html" rel="next" title="Python之进程和线程_2">
                  <i class="fa fa-chevron-left"></i> Python之进程和线程_2
                </a>
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
                <a href="/ck535j1hb0057msg4fxoh7mef.html" rel="prev" title="DFS深优先度搜索">
                  DFS深优先度搜索 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="comments"></div>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="lemon"
      src="/images/head.jpg">
  <p class="site-author-name" itemprop="name">lemon</p>
  <div class="site-description" itemprop="description">研一在读</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">114</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="feed-link motion-element">
    <a href="/atom.xml" rel="alternate">
      <i class="fa fa-rss"></i>RSS
    </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ustblc" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ustblc" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://cblog.club/about/" title="E-Mail → https:&#x2F;&#x2F;cblog.club&#x2F;about&#x2F;"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/2724172991" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;2724172991" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/dreamLC1998" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;dreamLC1998" rel="noopener" target="_blank"><i class="fa fa-fw fa-heartbeat"></i>CSDN</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
	
		<script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
		<script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
		<div class="widget-wrap">
			<div id="myCanvasContainer" class="widget tagcloud">
				<canvas width="250" height="150" id="resCanvas" style="width=100%">
					<ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/BFS/" rel="tag">BFS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DFS/" rel="tag">DFS</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Django/" rel="tag">Django</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OS/" rel="tag">OS</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyTorch/" rel="tag">PyTorch</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/" rel="tag">leetcode</a><span class="tag-list-count">50</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" rel="tag">二叉树</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%89%91%E6%8C%87offer/" rel="tag">剑指offer</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" rel="tag">动态规划</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8F%8C%E6%8C%87%E9%92%88/" rel="tag">双指针</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E6%BA%AF/" rel="tag">回溯</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BE%E5%83%8F%E9%A2%86%E5%9F%9F/" rel="tag">图像领域</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B9%BF%E5%BA%A6%E6%90%9C%E7%B4%A2/" rel="tag">广度搜索</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/" rel="tag">拓扑排序</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A0%88/" rel="tag">栈</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E6%90%9C%E7%B4%A2/" rel="tag">深度搜索</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" rel="tag">环境配置</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A8%8B%E5%BA%8F%E6%89%93%E5%8C%85/" rel="tag">程序打包</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/" rel="tag">自动化测试</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a><span class="tag-list-count">1</span></li></ul>
				</canvas>
			</div>
		</div>
	
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
<script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">by lemon</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:49</span>
</div>
	<div class="powered-by">
		<i class="fa fa-user-md"></i>
		<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
</span>
	</div>
<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">博客全站共108.1k字</span>
</div>
        












        
      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/pisces.js"></script>
<script src="/js/next-boot.js"></script>



  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  <script src="/js/local-search.js"></script>













  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: '2gTMxod3jhz1PoDX6d4NKqQx-gzGzoHsz',
    appKey: '0aoGT3GaT7ei6OAop6VlIrYV',
    placeholder: "快来评论吧^ ^!",
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: 'zh-cn' || 'zh-cn',
    path: location.pathname,
    recordIP: true,
    serverURLs: ''
  });
}, window.Valine);
</script>

</body>
</html>
